{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport numpy as np"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Load Concrete date"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  Strength  \n0            1040.0           676.0   28     79.99  \n1            1055.0           676.0   28     61.89  \n2             932.0           594.0  270     40.27  \n3             932.0           594.0  365     41.05  \n4             978.4           825.5  360     44.30  "
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data = pd.read_csv('https://cocl.us/concrete_data/concrete_data.csv')\nconcrete_data.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Check the size of the data"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(1030, 9)"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data.shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Check columns and contents"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<bound method NDFrame.describe of       Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0      540.0                 0.0      0.0  162.0               2.5   \n1      540.0                 0.0      0.0  162.0               2.5   \n2      332.5               142.5      0.0  228.0               0.0   \n3      332.5               142.5      0.0  228.0               0.0   \n4      198.6               132.4      0.0  192.0               0.0   \n5      266.0               114.0      0.0  228.0               0.0   \n6      380.0                95.0      0.0  228.0               0.0   \n7      380.0                95.0      0.0  228.0               0.0   \n8      266.0               114.0      0.0  228.0               0.0   \n9      475.0                 0.0      0.0  228.0               0.0   \n10     198.6               132.4      0.0  192.0               0.0   \n11     198.6               132.4      0.0  192.0               0.0   \n12     427.5                47.5      0.0  228.0               0.0   \n13     190.0               190.0      0.0  228.0               0.0   \n14     304.0                76.0      0.0  228.0               0.0   \n15     380.0                 0.0      0.0  228.0               0.0   \n16     139.6               209.4      0.0  192.0               0.0   \n17     342.0                38.0      0.0  228.0               0.0   \n18     380.0                95.0      0.0  228.0               0.0   \n19     475.0                 0.0      0.0  228.0               0.0   \n20     427.5                47.5      0.0  228.0               0.0   \n21     139.6               209.4      0.0  192.0               0.0   \n22     139.6               209.4      0.0  192.0               0.0   \n23     139.6               209.4      0.0  192.0               0.0   \n24     380.0                 0.0      0.0  228.0               0.0   \n25     380.0                 0.0      0.0  228.0               0.0   \n26     380.0                95.0      0.0  228.0               0.0   \n27     342.0                38.0      0.0  228.0               0.0   \n28     427.5                47.5      0.0  228.0               0.0   \n29     475.0                 0.0      0.0  228.0               0.0   \n...      ...                 ...      ...    ...               ...   \n1000   141.9               166.6    129.7  173.5              10.9   \n1001   297.8               137.2    106.9  201.3               6.0   \n1002   321.3               164.2      0.0  190.5               4.6   \n1003   366.0               187.0      0.0  191.3               6.6   \n1004   279.8               128.9    100.4  172.4               9.5   \n1005   252.1                97.1     75.6  193.8               8.3   \n1006   164.6                 0.0    150.4  181.6              11.7   \n1007   155.6               243.5      0.0  180.3              10.7   \n1008   160.2               188.0    146.4  203.2              11.3   \n1009   298.1                 0.0    107.0  186.4               6.1   \n1010   317.9                 0.0    126.5  209.7               5.7   \n1011   287.3               120.5     93.9  187.6               9.2   \n1012   325.6               166.4      0.0  174.0               8.9   \n1013   355.9                 0.0    141.6  193.3              11.0   \n1014   132.0               206.5    160.9  178.9               5.5   \n1015   322.5               148.6      0.0  185.8               8.5   \n1016   164.2                 0.0    200.1  181.2              12.6   \n1017   313.8                 0.0    112.6  169.9              10.1   \n1018   321.4                 0.0    127.9  182.5              11.5   \n1019   139.7               163.9    127.7  236.7               5.8   \n1020   288.4               121.0      0.0  177.4               7.0   \n1021   298.2                 0.0    107.0  209.7              11.1   \n1022   264.5               111.0     86.5  195.5               5.9   \n1023   159.8               250.0      0.0  168.4              12.2   \n1024   166.0               259.7      0.0  183.2              12.7   \n1025   276.4               116.0     90.3  179.6               8.9   \n1026   322.2                 0.0    115.6  196.0              10.4   \n1027   148.5               139.4    108.6  192.7               6.1   \n1028   159.1               186.7      0.0  175.6              11.3   \n1029   260.9               100.5     78.3  200.6               8.6   \n\n      Coarse Aggregate  Fine Aggregate  Age  Strength  \n0               1040.0           676.0   28     79.99  \n1               1055.0           676.0   28     61.89  \n2                932.0           594.0  270     40.27  \n3                932.0           594.0  365     41.05  \n4                978.4           825.5  360     44.30  \n5                932.0           670.0   90     47.03  \n6                932.0           594.0  365     43.70  \n7                932.0           594.0   28     36.45  \n8                932.0           670.0   28     45.85  \n9                932.0           594.0   28     39.29  \n10               978.4           825.5   90     38.07  \n11               978.4           825.5   28     28.02  \n12               932.0           594.0  270     43.01  \n13               932.0           670.0   90     42.33  \n14               932.0           670.0   28     47.81  \n15               932.0           670.0   90     52.91  \n16              1047.0           806.9   90     39.36  \n17               932.0           670.0  365     56.14  \n18               932.0           594.0   90     40.56  \n19               932.0           594.0  180     42.62  \n20               932.0           594.0  180     41.84  \n21              1047.0           806.9   28     28.24  \n22              1047.0           806.9    3      8.06  \n23              1047.0           806.9  180     44.21  \n24               932.0           670.0  365     52.52  \n25               932.0           670.0  270     53.30  \n26               932.0           594.0  270     41.15  \n27               932.0           670.0  180     52.12  \n28               932.0           594.0   28     37.43  \n29               932.0           594.0    7     38.60  \n...                ...             ...  ...       ...  \n1000             882.6           785.3   28     44.61  \n1001             878.4           655.3   28     53.52  \n1002             870.0           774.0   28     57.22  \n1003             824.3           756.9   28     65.91  \n1004             825.1           804.9   28     52.83  \n1005             835.5           821.4   28     33.40  \n1006            1023.3           728.9   28     18.03  \n1007            1022.0           697.7   28     37.36  \n1008             828.7           709.7   28     35.31  \n1009             879.0           815.2   28     42.64  \n1010             860.5           736.6   28     40.06  \n1011             904.4           695.9   28     43.80  \n1012             881.6           790.0   28     61.24  \n1013             801.4           778.4   28     40.87  \n1014             866.9           735.6   28     33.31  \n1015             951.0           709.5   28     52.43  \n1016             849.3           846.0   28     15.09  \n1017             925.3           782.9   28     38.46  \n1018             870.1           779.7   28     37.27  \n1019             868.6           655.6   28     35.23  \n1020             907.9           829.5   28     42.14  \n1021             879.6           744.2   28     31.88  \n1022             832.6           790.4   28     41.54  \n1023            1049.3           688.2   28     39.46  \n1024             858.8           826.8   28     37.92  \n1025             870.1           768.3   28     44.28  \n1026             817.9           813.4   28     31.18  \n1027             892.4           780.0   28     23.70  \n1028             989.6           788.9   28     32.77  \n1029             864.5           761.5   28     32.40  \n\n[1030 rows x 9 columns]>"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data.describe"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's check if we are missing values"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "Cement                0\nBlast Furnace Slag    0\nFly Ash               0\nWater                 0\nSuperplasticizer      0\nCoarse Aggregate      0\nFine Aggregate        0\nAge                   0\nStrength              0\ndtype: int64"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data.isnull().sum()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Predictors are all columns exceft for Strength which is the target"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "concrete_data_cols = concrete_data.columns\npredictors = concrete_data[concrete_data_cols[concrete_data_cols != 'Strength']]\ntarget = concrete_data['Strength']"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "n_cols = predictors.shape[1] # number of predictors to be used to build the NN"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's get started with Keras"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "import keras"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "from keras.models import Sequential\nfrom keras.layers import Dense"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's define a regression model, using 1 layer with 10 nodes, the ReLU activation function, adam as the optimizer and the mean squared error as the loss function"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "def regression_model():\n    model = Sequential()\n    model.add(Dense(10,activation='relu',input_shape=(n_cols,)))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Train and test the network"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n['loss']\n"
                }
            ],
            "source": "model = regression_model()\nprint(model.metrics_names)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Train the model using 50 epochs - loop 50 times reporting the mean squared errors"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 5s - loss: 94814.0709 - val_loss: 44288.9915\nEpoch 2/50\n - 0s - loss: 26663.4500 - val_loss: 9336.3042\nEpoch 3/50\n - 0s - loss: 6214.2890 - val_loss: 2930.1056\nEpoch 4/50\n - 0s - loss: 3281.9307 - val_loss: 2674.0747\nEpoch 5/50\n - 0s - loss: 3067.8670 - val_loss: 2678.4248\nEpoch 6/50\n - 0s - loss: 3004.7825 - val_loss: 2646.8428\nEpoch 7/50\n - 0s - loss: 2940.3703 - val_loss: 2623.6394\nEpoch 8/50\n - 0s - loss: 2873.2371 - val_loss: 2607.9648\nEpoch 9/50\n - 0s - loss: 2805.5388 - val_loss: 2581.4045\nEpoch 10/50\n - 0s - loss: 2738.0949 - val_loss: 2554.2923\nEpoch 11/50\n - 0s - loss: 2667.0293 - val_loss: 2536.9920\nEpoch 12/50\n - 1s - loss: 2592.1108 - val_loss: 2547.5710\nEpoch 13/50\n - 0s - loss: 2510.8628 - val_loss: 2531.7000\nEpoch 14/50\n - 0s - loss: 2428.5072 - val_loss: 2528.2811\nEpoch 15/50\n - 0s - loss: 2344.9574 - val_loss: 2576.3287\nEpoch 16/50\n - 0s - loss: 2269.8813 - val_loss: 2626.6732\nEpoch 17/50\n - 1s - loss: 2205.6467 - val_loss: 2616.1500\nEpoch 18/50\n - 1s - loss: 2152.1361 - val_loss: 2661.7075\nEpoch 19/50\n - 0s - loss: 2107.2400 - val_loss: 2646.4229\nEpoch 20/50\n - 7s - loss: 2062.5175 - val_loss: 2642.5132\nEpoch 21/50\n - 0s - loss: 2022.4832 - val_loss: 2611.2321\nEpoch 22/50\n - 0s - loss: 1974.5720 - val_loss: 2621.9163\nEpoch 23/50\n - 3s - loss: 1940.6046 - val_loss: 2596.8062\nEpoch 24/50\n - 0s - loss: 1898.9929 - val_loss: 2583.6871\nEpoch 25/50\n - 1s - loss: 1857.1936 - val_loss: 2541.2265\nEpoch 26/50\n - 0s - loss: 1821.8426 - val_loss: 2521.8167\nEpoch 27/50\n - 0s - loss: 1784.7702 - val_loss: 2508.2118\nEpoch 28/50\n - 0s - loss: 1751.7040 - val_loss: 2474.8753\nEpoch 29/50\n - 0s - loss: 1716.5947 - val_loss: 2443.8309\nEpoch 30/50\n - 0s - loss: 1682.6319 - val_loss: 2428.2451\nEpoch 31/50\n - 0s - loss: 1652.9498 - val_loss: 2398.8212\nEpoch 32/50\n - 0s - loss: 1623.1038 - val_loss: 2365.5229\nEpoch 33/50\n - 0s - loss: 1591.3839 - val_loss: 2351.0648\nEpoch 34/50\n - 0s - loss: 1555.8311 - val_loss: 2310.8076\nEpoch 35/50\n - 1s - loss: 1521.7346 - val_loss: 2315.3095\nEpoch 36/50\n - 0s - loss: 1493.2280 - val_loss: 2259.3665\nEpoch 37/50\n - 6s - loss: 1464.4696 - val_loss: 2259.2401\nEpoch 38/50\n - 3s - loss: 1431.0237 - val_loss: 2224.0856\nEpoch 39/50\n - 0s - loss: 1400.0289 - val_loss: 2174.2615\nEpoch 40/50\n - 0s - loss: 1371.2281 - val_loss: 2164.4032\nEpoch 41/50\n - 0s - loss: 1345.9693 - val_loss: 2133.4658\nEpoch 42/50\n - 0s - loss: 1315.6034 - val_loss: 2104.0981\nEpoch 43/50\n - 0s - loss: 1288.8429 - val_loss: 2054.1855\nEpoch 44/50\n - 0s - loss: 1257.4414 - val_loss: 2083.2580\nEpoch 45/50\n - 0s - loss: 1241.8257 - val_loss: 2001.2257\nEpoch 46/50\n - 0s - loss: 1206.1101 - val_loss: 2025.6457\nEpoch 47/50\n - 0s - loss: 1184.7745 - val_loss: 1987.4617\nEpoch 48/50\n - 0s - loss: 1154.9524 - val_loss: 1950.8595\nEpoch 49/50\n - 0s - loss: 1128.2327 - val_loss: 1910.5401\nEpoch 50/50\n - 0s - loss: 1102.0407 - val_loss: 1887.7655\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 1079.1455 - val_loss: 1848.3747\nEpoch 2/50\n - 0s - loss: 1054.6115 - val_loss: 1860.2380\nEpoch 3/50\n - 0s - loss: 1027.0155 - val_loss: 1803.4319\nEpoch 4/50\n - 0s - loss: 998.1814 - val_loss: 1774.6129\nEpoch 5/50\n - 0s - loss: 969.9967 - val_loss: 1742.0627\nEpoch 6/50\n - 0s - loss: 943.0854 - val_loss: 1715.7221\nEpoch 7/50\n - 0s - loss: 913.3039 - val_loss: 1699.7501\nEpoch 8/50\n - 8s - loss: 887.9733 - val_loss: 1640.1700\nEpoch 9/50\n - 0s - loss: 862.6128 - val_loss: 1641.8192\nEpoch 10/50\n - 0s - loss: 835.0001 - val_loss: 1582.2445\nEpoch 11/50\n - 3s - loss: 819.1042 - val_loss: 1633.1432\nEpoch 12/50\n - 0s - loss: 789.7385 - val_loss: 1518.6401\nEpoch 13/50\n - 0s - loss: 763.8487 - val_loss: 1523.6593\nEpoch 14/50\n - 0s - loss: 741.2232 - val_loss: 1504.4337\nEpoch 15/50\n - 0s - loss: 717.9167 - val_loss: 1421.4810\nEpoch 16/50\n - 0s - loss: 697.8178 - val_loss: 1416.9773\nEpoch 17/50\n - 0s - loss: 680.4690 - val_loss: 1365.3423\nEpoch 18/50\n - 0s - loss: 658.1966 - val_loss: 1410.5266\nEpoch 19/50\n - 0s - loss: 638.5849 - val_loss: 1351.3564\nEpoch 20/50\n - 0s - loss: 616.3354 - val_loss: 1315.5828\nEpoch 21/50\n - 0s - loss: 596.3941 - val_loss: 1248.1860\nEpoch 22/50\n - 0s - loss: 578.8809 - val_loss: 1252.8327\nEpoch 23/50\n - 0s - loss: 565.1976 - val_loss: 1225.4217\nEpoch 24/50\n - 0s - loss: 553.5814 - val_loss: 1150.4149\nEpoch 25/50\n - 0s - loss: 527.9157 - val_loss: 1131.5546\nEpoch 26/50\n - 1s - loss: 510.1378 - val_loss: 1096.7815\nEpoch 27/50\n - 0s - loss: 495.8540 - val_loss: 1059.6330\nEpoch 28/50\n - 0s - loss: 480.3944 - val_loss: 1031.5782\nEpoch 29/50\n - 1s - loss: 466.7641 - val_loss: 987.2697\nEpoch 30/50\n - 1s - loss: 452.1130 - val_loss: 961.3130\nEpoch 31/50\n - 0s - loss: 430.5704 - val_loss: 949.0642\nEpoch 32/50\n - 0s - loss: 415.5397 - val_loss: 914.0923\nEpoch 33/50\n - 6s - loss: 403.4759 - val_loss: 896.9485\nEpoch 34/50\n - 0s - loss: 390.7197 - val_loss: 785.5205\nEpoch 35/50\n - 0s - loss: 371.9252 - val_loss: 737.3438\nEpoch 36/50\n - 3s - loss: 364.3287 - val_loss: 724.7175\nEpoch 37/50\n - 0s - loss: 348.0414 - val_loss: 692.6155\nEpoch 38/50\n - 0s - loss: 335.6982 - val_loss: 674.4160\nEpoch 39/50\n - 0s - loss: 327.8326 - val_loss: 632.2494\nEpoch 40/50\n - 0s - loss: 311.5852 - val_loss: 617.3361\nEpoch 41/50\n - 1s - loss: 301.3220 - val_loss: 531.9272\nEpoch 42/50\n - 0s - loss: 289.6395 - val_loss: 487.5046\nEpoch 43/50\n - 0s - loss: 282.2445 - val_loss: 509.0901\nEpoch 44/50\n - 0s - loss: 271.2148 - val_loss: 468.4464\nEpoch 45/50\n - 0s - loss: 261.8152 - val_loss: 428.0862\nEpoch 46/50\n - 0s - loss: 252.2912 - val_loss: 447.7311\nEpoch 47/50\n - 3s - loss: 244.2100 - val_loss: 424.6056\nEpoch 48/50\n - 1s - loss: 237.4126 - val_loss: 452.2451\nEpoch 49/50\n - 0s - loss: 230.6430 - val_loss: 400.1052\nEpoch 50/50\n - 0s - loss: 222.3606 - val_loss: 398.8667\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 219.9243 - val_loss: 342.8088\nEpoch 2/50\n - 0s - loss: 212.6709 - val_loss: 310.8098\nEpoch 3/50\n - 5s - loss: 207.7982 - val_loss: 308.3172\nEpoch 4/50\n - 0s - loss: 198.0467 - val_loss: 310.3631\nEpoch 5/50\n - 0s - loss: 195.0276 - val_loss: 331.4236\nEpoch 6/50\n - 0s - loss: 191.7888 - val_loss: 313.0610\nEpoch 7/50\n - 4s - loss: 186.2831 - val_loss: 253.6932\nEpoch 8/50\n - 0s - loss: 179.5119 - val_loss: 248.9772\nEpoch 9/50\n - 0s - loss: 177.9395 - val_loss: 221.7371\nEpoch 10/50\n - 0s - loss: 174.3783 - val_loss: 231.3026\nEpoch 11/50\n - 0s - loss: 167.6643 - val_loss: 258.6353\nEpoch 12/50\n - 0s - loss: 167.8167 - val_loss: 266.4232\nEpoch 13/50\n - 0s - loss: 167.5789 - val_loss: 222.7427\nEpoch 14/50\n - 0s - loss: 165.3251 - val_loss: 184.0912\nEpoch 15/50\n - 0s - loss: 157.0345 - val_loss: 198.8387\nEpoch 16/50\n - 0s - loss: 154.7498 - val_loss: 173.1223\nEpoch 17/50\n - 1s - loss: 158.2433 - val_loss: 182.9054\nEpoch 18/50\n - 0s - loss: 150.5584 - val_loss: 181.1257\nEpoch 19/50\n - 0s - loss: 149.9564 - val_loss: 192.7402\nEpoch 20/50\n - 0s - loss: 147.3286 - val_loss: 174.6162\nEpoch 21/50\n - 0s - loss: 145.8401 - val_loss: 170.1002\nEpoch 22/50\n - 0s - loss: 143.4180 - val_loss: 150.6335\nEpoch 23/50\n - 0s - loss: 146.0824 - val_loss: 187.2226\nEpoch 24/50\n - 0s - loss: 143.2668 - val_loss: 139.5901\nEpoch 25/50\n - 0s - loss: 143.1009 - val_loss: 137.0963\nEpoch 26/50\n - 0s - loss: 139.8391 - val_loss: 170.6882\nEpoch 27/50\n - 0s - loss: 139.7611 - val_loss: 139.2688\nEpoch 28/50\n - 1s - loss: 138.4553 - val_loss: 142.0833\nEpoch 29/50\n - 6s - loss: 138.1328 - val_loss: 138.1847\nEpoch 30/50\n - 0s - loss: 137.8454 - val_loss: 138.6922\nEpoch 31/50\n - 3s - loss: 136.0869 - val_loss: 145.6610\nEpoch 32/50\n - 0s - loss: 135.0805 - val_loss: 168.3772\nEpoch 33/50\n - 0s - loss: 135.1612 - val_loss: 138.6848\nEpoch 34/50\n - 0s - loss: 133.7782 - val_loss: 134.2077\nEpoch 35/50\n - 0s - loss: 133.4905 - val_loss: 123.7726\nEpoch 36/50\n - 0s - loss: 132.8943 - val_loss: 136.3005\nEpoch 37/50\n - 0s - loss: 137.5054 - val_loss: 133.4965\nEpoch 38/50\n - 0s - loss: 135.5319 - val_loss: 118.8541\nEpoch 39/50\n - 0s - loss: 139.1277 - val_loss: 162.3337\nEpoch 40/50\n - 0s - loss: 138.9129 - val_loss: 112.8344\nEpoch 41/50\n - 0s - loss: 131.6574 - val_loss: 123.9263\nEpoch 42/50\n - 1s - loss: 133.3831 - val_loss: 113.1286\nEpoch 43/50\n - 0s - loss: 136.4665 - val_loss: 141.6176\nEpoch 44/50\n - 0s - loss: 129.2628 - val_loss: 120.8784\nEpoch 45/50\n - 0s - loss: 131.5641 - val_loss: 119.7911\nEpoch 46/50\n - 0s - loss: 132.0689 - val_loss: 109.1425\nEpoch 47/50\n - 0s - loss: 139.5861 - val_loss: 157.7787\nEpoch 48/50\n - 0s - loss: 134.5345 - val_loss: 112.2323\nEpoch 49/50\n - 0s - loss: 130.5144 - val_loss: 107.2774\nEpoch 50/50\n - 9s - loss: 131.1339 - val_loss: 112.4861\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 135.7874 - val_loss: 103.2028\nEpoch 2/50\n - 0s - loss: 135.2632 - val_loss: 133.9721\nEpoch 3/50\n - 0s - loss: 133.8998 - val_loss: 105.5983\nEpoch 4/50\n - 1s - loss: 132.2620 - val_loss: 117.1735\nEpoch 5/50\n - 0s - loss: 127.9235 - val_loss: 107.7796\nEpoch 6/50\n - 1s - loss: 127.9472 - val_loss: 116.3580\nEpoch 7/50\n - 0s - loss: 128.9702 - val_loss: 121.1948\nEpoch 8/50\n - 0s - loss: 127.9570 - val_loss: 103.9424\nEpoch 9/50\n - 0s - loss: 131.8454 - val_loss: 103.0876\nEpoch 10/50\n - 0s - loss: 130.0415 - val_loss: 113.5750\nEpoch 11/50\n - 0s - loss: 130.7007 - val_loss: 103.4627\nEpoch 12/50\n - 0s - loss: 131.4131 - val_loss: 127.7637\nEpoch 13/50\n - 0s - loss: 133.0001 - val_loss: 97.5736\nEpoch 14/50\n - 0s - loss: 130.5290 - val_loss: 129.9754\nEpoch 15/50\n - 0s - loss: 129.4357 - val_loss: 104.2347\nEpoch 16/50\n - 1s - loss: 133.4874 - val_loss: 122.6103\nEpoch 17/50\n - 0s - loss: 130.1368 - val_loss: 104.1182\nEpoch 18/50\n - 0s - loss: 128.9024 - val_loss: 99.2378\nEpoch 19/50\n - 0s - loss: 133.4029 - val_loss: 115.2116\nEpoch 20/50\n - 0s - loss: 140.5644 - val_loss: 97.6437\nEpoch 21/50\n - 0s - loss: 130.7559 - val_loss: 114.6243\nEpoch 22/50\n - 0s - loss: 125.8318 - val_loss: 108.1945\nEpoch 23/50\n - 6s - loss: 135.4000 - val_loss: 140.0259\nEpoch 24/50\n - 3s - loss: 129.9092 - val_loss: 97.9917\nEpoch 25/50\n - 0s - loss: 127.0151 - val_loss: 95.9619\nEpoch 26/50\n - 0s - loss: 128.3550 - val_loss: 112.7504\nEpoch 27/50\n - 0s - loss: 125.8758 - val_loss: 115.2741\nEpoch 28/50\n - 0s - loss: 131.8609 - val_loss: 98.9699\nEpoch 29/50\n - 0s - loss: 125.7480 - val_loss: 95.9381\nEpoch 30/50\n - 0s - loss: 126.5456 - val_loss: 94.3934\nEpoch 31/50\n - 0s - loss: 127.0142 - val_loss: 93.2350\nEpoch 32/50\n - 0s - loss: 131.9587 - val_loss: 105.3499\nEpoch 33/50\n - 0s - loss: 128.4833 - val_loss: 108.3691\nEpoch 34/50\n - 0s - loss: 128.6763 - val_loss: 100.2434\nEpoch 35/50\n - 0s - loss: 125.5058 - val_loss: 92.4708\nEpoch 36/50\n - 0s - loss: 132.9789 - val_loss: 105.6968\nEpoch 37/50\n - 0s - loss: 125.6100 - val_loss: 100.7766\nEpoch 38/50\n - 0s - loss: 126.2815 - val_loss: 92.3133\nEpoch 39/50\n - 0s - loss: 125.9836 - val_loss: 113.5310\nEpoch 40/50\n - 1s - loss: 133.0760 - val_loss: 91.5689\nEpoch 41/50\n - 0s - loss: 124.9105 - val_loss: 91.1714\nEpoch 42/50\n - 0s - loss: 131.3705 - val_loss: 92.1416\nEpoch 43/50\n - 0s - loss: 127.8121 - val_loss: 109.0872\nEpoch 44/50\n - 0s - loss: 125.0870 - val_loss: 95.1341\nEpoch 45/50\n - 3s - loss: 131.3756 - val_loss: 117.9920\nEpoch 46/50\n - 4s - loss: 126.7706 - val_loss: 94.2552\nEpoch 47/50\n - 0s - loss: 124.4275 - val_loss: 95.3068\nEpoch 48/50\n - 0s - loss: 124.8566 - val_loss: 96.5940\nEpoch 49/50\n - 3s - loss: 126.5762 - val_loss: 88.8190\nEpoch 50/50\n - 0s - loss: 126.8608 - val_loss: 95.7107\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 124.9922 - val_loss: 89.7541\nEpoch 2/50\n - 0s - loss: 125.3458 - val_loss: 105.6098\nEpoch 3/50\n - 0s - loss: 134.8032 - val_loss: 107.5942\nEpoch 4/50\n - 0s - loss: 127.3571 - val_loss: 90.6484\nEpoch 5/50\n - 0s - loss: 129.4684 - val_loss: 93.0277\nEpoch 6/50\n - 0s - loss: 125.9912 - val_loss: 107.5192\nEpoch 7/50\n - 0s - loss: 129.1684 - val_loss: 89.6052\nEpoch 8/50\n - 3s - loss: 127.4575 - val_loss: 91.1456\nEpoch 9/50\n - 0s - loss: 125.3037 - val_loss: 94.1484\nEpoch 10/50\n - 0s - loss: 124.3911 - val_loss: 95.5627\nEpoch 11/50\n - 0s - loss: 127.4241 - val_loss: 90.4262\nEpoch 12/50\n - 0s - loss: 127.7340 - val_loss: 95.3109\nEpoch 13/50\n - 0s - loss: 128.9296 - val_loss: 112.5551\nEpoch 14/50\n - 0s - loss: 127.7761 - val_loss: 90.9833\nEpoch 15/50\n - 0s - loss: 126.2170 - val_loss: 93.8919\nEpoch 16/50\n - 0s - loss: 125.9506 - val_loss: 97.3458\nEpoch 17/50\n - 0s - loss: 126.5313 - val_loss: 83.9569\nEpoch 18/50\n - 0s - loss: 124.7774 - val_loss: 91.7768\nEpoch 19/50\n - 6s - loss: 130.7277 - val_loss: 88.3178\nEpoch 20/50\n - 0s - loss: 134.5120 - val_loss: 96.0566\nEpoch 21/50\n - 4s - loss: 127.4298 - val_loss: 126.3504\nEpoch 22/50\n - 0s - loss: 131.3320 - val_loss: 90.6649\nEpoch 23/50\n - 0s - loss: 125.5131 - val_loss: 85.6771\nEpoch 24/50\n - 1s - loss: 124.1734 - val_loss: 96.6569\nEpoch 25/50\n - 0s - loss: 124.2326 - val_loss: 90.1379\nEpoch 26/50\n - 0s - loss: 125.9150 - val_loss: 122.0587\nEpoch 27/50\n - 0s - loss: 130.2758 - val_loss: 92.2567\nEpoch 28/50\n - 0s - loss: 123.9564 - val_loss: 98.8491\nEpoch 29/50\n - 0s - loss: 128.3575 - val_loss: 85.1037\nEpoch 30/50\n - 0s - loss: 131.4657 - val_loss: 84.0090\nEpoch 31/50\n - 0s - loss: 134.5559 - val_loss: 122.4302\nEpoch 32/50\n - 1s - loss: 127.8359 - val_loss: 136.2832\nEpoch 33/50\n - 3s - loss: 134.2526 - val_loss: 89.9551\nEpoch 34/50\n - 0s - loss: 127.4074 - val_loss: 89.8367\nEpoch 35/50\n - 0s - loss: 126.9440 - val_loss: 85.2571\nEpoch 36/50\n - 0s - loss: 128.6484 - val_loss: 105.3534\nEpoch 37/50\n - 0s - loss: 123.2751 - val_loss: 83.0550\nEpoch 38/50\n - 0s - loss: 127.0847 - val_loss: 86.2202\nEpoch 39/50\n - 6s - loss: 124.1372 - val_loss: 97.9578\nEpoch 40/50\n - 3s - loss: 129.7497 - val_loss: 81.8886\nEpoch 41/50\n - 0s - loss: 126.9550 - val_loss: 82.6183\nEpoch 42/50\n - 0s - loss: 123.2771 - val_loss: 83.2861\nEpoch 43/50\n - 0s - loss: 126.2140 - val_loss: 82.9820\nEpoch 44/50\n - 0s - loss: 123.4881 - val_loss: 110.2578\nEpoch 45/50\n - 0s - loss: 124.8950 - val_loss: 83.2716\nEpoch 46/50\n - 0s - loss: 123.7372 - val_loss: 87.1498\nEpoch 47/50\n - 0s - loss: 128.1175 - val_loss: 84.0744\nEpoch 48/50\n - 0s - loss: 123.1394 - val_loss: 86.0618\nEpoch 49/50\n - 0s - loss: 124.5405 - val_loss: 78.9624\nEpoch 50/50\n - 0s - loss: 122.2852 - val_loss: 86.7067\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 124.6330 - val_loss: 91.3290\nEpoch 2/50\n - 0s - loss: 121.9843 - val_loss: 95.2427\nEpoch 3/50\n - 0s - loss: 126.2711 - val_loss: 96.2312\nEpoch 4/50\n - 0s - loss: 124.5912 - val_loss: 81.6815\nEpoch 5/50\n - 0s - loss: 122.4527 - val_loss: 111.7802\nEpoch 6/50\n - 0s - loss: 126.4936 - val_loss: 80.4450\nEpoch 7/50\n - 0s - loss: 124.9743 - val_loss: 82.1203\nEpoch 8/50\n - 0s - loss: 122.8438 - val_loss: 85.6565\nEpoch 9/50\n - 0s - loss: 122.0861 - val_loss: 79.5407\nEpoch 10/50\n - 8s - loss: 126.6891 - val_loss: 80.7256\nEpoch 11/50\n - 0s - loss: 128.3152 - val_loss: 88.7752\nEpoch 12/50\n - 0s - loss: 131.5469 - val_loss: 83.6296\nEpoch 13/50\n - 3s - loss: 123.3215 - val_loss: 83.1606\nEpoch 14/50\n - 0s - loss: 128.2447 - val_loss: 80.9559\nEpoch 15/50\n - 0s - loss: 136.5901 - val_loss: 79.3875\nEpoch 16/50\n - 0s - loss: 125.3835 - val_loss: 79.0312\nEpoch 17/50\n - 3s - loss: 122.4023 - val_loss: 81.3562\nEpoch 18/50\n - 0s - loss: 127.0167 - val_loss: 80.9845\nEpoch 19/50\n - 0s - loss: 123.5253 - val_loss: 95.1301\nEpoch 20/50\n - 1s - loss: 125.6788 - val_loss: 94.7021\nEpoch 21/50\n - 1s - loss: 125.2935 - val_loss: 80.4504\nEpoch 22/50\n - 0s - loss: 124.3544 - val_loss: 99.1373\nEpoch 23/50\n - 0s - loss: 131.5728 - val_loss: 117.7184\nEpoch 24/50\n - 0s - loss: 137.7790 - val_loss: 100.4061\nEpoch 25/50\n - 0s - loss: 129.2302 - val_loss: 84.0821\nEpoch 26/50\n - 0s - loss: 122.0366 - val_loss: 79.8612\nEpoch 27/50\n - 0s - loss: 121.8257 - val_loss: 81.7717\nEpoch 28/50\n - 0s - loss: 122.8312 - val_loss: 87.6993\nEpoch 29/50\n - 0s - loss: 122.6714 - val_loss: 86.9254\nEpoch 30/50\n - 5s - loss: 122.4437 - val_loss: 80.1579\nEpoch 31/50\n - 0s - loss: 123.1175 - val_loss: 97.8347\nEpoch 32/50\n - 0s - loss: 129.1172 - val_loss: 78.1956\nEpoch 33/50\n - 3s - loss: 125.9785 - val_loss: 81.3946\nEpoch 34/50\n - 0s - loss: 121.7828 - val_loss: 78.2945\nEpoch 35/50\n - 0s - loss: 122.7264 - val_loss: 85.7325\nEpoch 36/50\n - 0s - loss: 121.8003 - val_loss: 80.6049\nEpoch 37/50\n - 0s - loss: 125.8425 - val_loss: 85.1835\nEpoch 38/50\n - 0s - loss: 128.7930 - val_loss: 79.2591\nEpoch 39/50\n - 0s - loss: 122.3123 - val_loss: 79.3887\nEpoch 40/50\n - 0s - loss: 122.9074 - val_loss: 96.4591\nEpoch 41/50\n - 0s - loss: 128.0847 - val_loss: 79.3471\nEpoch 42/50\n - 0s - loss: 129.8133 - val_loss: 87.2310\nEpoch 43/50\n - 0s - loss: 123.5279 - val_loss: 80.1887\nEpoch 44/50\n - 3s - loss: 121.9862 - val_loss: 80.7838\nEpoch 45/50\n - 0s - loss: 120.7885 - val_loss: 87.7302\nEpoch 46/50\n - 0s - loss: 121.4920 - val_loss: 78.2298\nEpoch 47/50\n - 0s - loss: 123.8698 - val_loss: 76.7922\nEpoch 48/50\n - 0s - loss: 126.0472 - val_loss: 84.7698\nEpoch 49/50\n - 0s - loss: 127.4251 - val_loss: 84.6051\nEpoch 50/50\n - 1s - loss: 122.8650 - val_loss: 77.9682\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 122.5919 - val_loss: 89.6819\nEpoch 2/50\n - 5s - loss: 121.8099 - val_loss: 95.6757\nEpoch 3/50\n - 0s - loss: 122.2045 - val_loss: 80.5958\nEpoch 4/50\n - 0s - loss: 122.3228 - val_loss: 77.5033\nEpoch 5/50\n - 0s - loss: 122.7385 - val_loss: 85.5604\nEpoch 6/50\n - 3s - loss: 122.5939 - val_loss: 90.9523\nEpoch 7/50\n - 0s - loss: 120.3081 - val_loss: 75.5773\nEpoch 8/50\n - 1s - loss: 124.0005 - val_loss: 89.7193\nEpoch 9/50\n - 1s - loss: 121.4590 - val_loss: 85.3785\nEpoch 10/50\n - 3s - loss: 123.9553 - val_loss: 91.7387\nEpoch 11/50\n - 0s - loss: 133.0597 - val_loss: 148.3684\nEpoch 12/50\n - 0s - loss: 132.2331 - val_loss: 88.4986\nEpoch 13/50\n - 0s - loss: 123.6160 - val_loss: 94.7296\nEpoch 14/50\n - 0s - loss: 124.5638 - val_loss: 81.9978\nEpoch 15/50\n - 0s - loss: 121.4068 - val_loss: 75.1612\nEpoch 16/50\n - 1s - loss: 129.7869 - val_loss: 82.4481\nEpoch 17/50\n - 6s - loss: 130.0239 - val_loss: 77.8360\nEpoch 18/50\n - 1s - loss: 129.9813 - val_loss: 76.0358\nEpoch 19/50\n - 5s - loss: 127.6300 - val_loss: 80.5272\nEpoch 20/50\n - 0s - loss: 126.1998 - val_loss: 78.6808\nEpoch 21/50\n - 0s - loss: 124.6834 - val_loss: 76.8943\nEpoch 22/50\n - 0s - loss: 121.4761 - val_loss: 76.2575\nEpoch 23/50\n - 0s - loss: 119.6250 - val_loss: 81.0404\nEpoch 24/50\n - 0s - loss: 126.1156 - val_loss: 75.2697\nEpoch 25/50\n - 0s - loss: 131.0419 - val_loss: 76.1441\nEpoch 26/50\n - 0s - loss: 118.7808 - val_loss: 82.9685\nEpoch 27/50\n - 0s - loss: 123.6081 - val_loss: 75.6167\nEpoch 28/50\n - 0s - loss: 126.0203 - val_loss: 81.6643\nEpoch 29/50\n - 0s - loss: 122.4051 - val_loss: 100.8274\nEpoch 30/50\n - 0s - loss: 126.4572 - val_loss: 80.8787\nEpoch 31/50\n - 0s - loss: 119.3927 - val_loss: 74.0797\nEpoch 32/50\n - 0s - loss: 125.2637 - val_loss: 97.0513\nEpoch 33/50\n - 5s - loss: 131.7219 - val_loss: 155.5200\nEpoch 34/50\n - 0s - loss: 134.1518 - val_loss: 81.9714\nEpoch 35/50\n - 0s - loss: 122.3263 - val_loss: 79.7623\nEpoch 36/50\n - 0s - loss: 121.2478 - val_loss: 80.5249\nEpoch 37/50\n - 4s - loss: 124.0773 - val_loss: 80.2916\nEpoch 38/50\n - 0s - loss: 120.4076 - val_loss: 78.8341\nEpoch 39/50\n - 0s - loss: 122.4363 - val_loss: 85.5536\nEpoch 40/50\n - 0s - loss: 128.6199 - val_loss: 83.2690\nEpoch 41/50\n - 0s - loss: 123.7559 - val_loss: 90.3789\nEpoch 42/50\n - 0s - loss: 121.7900 - val_loss: 87.1912\nEpoch 43/50\n - 0s - loss: 131.0050 - val_loss: 91.3574\nEpoch 44/50\n - 0s - loss: 124.3380 - val_loss: 75.0601\nEpoch 45/50\n - 0s - loss: 122.9622 - val_loss: 78.5431\nEpoch 46/50\n - 0s - loss: 126.3240 - val_loss: 89.0663\nEpoch 47/50\n - 0s - loss: 137.6446 - val_loss: 95.0491\nEpoch 48/50\n - 0s - loss: 126.5050 - val_loss: 83.5908\nEpoch 49/50\n - 0s - loss: 121.0932 - val_loss: 74.7176\nEpoch 50/50\n - 0s - loss: 125.0782 - val_loss: 76.7334\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 128.1916 - val_loss: 82.3351\nEpoch 2/50\n - 0s - loss: 128.1271 - val_loss: 79.7865\nEpoch 3/50\n - 0s - loss: 123.7547 - val_loss: 77.5010\nEpoch 4/50\n - 0s - loss: 121.5760 - val_loss: 76.3834\nEpoch 5/50\n - 0s - loss: 121.0829 - val_loss: 107.7656\nEpoch 6/50\n - 0s - loss: 128.1932 - val_loss: 114.9908\nEpoch 7/50\n - 0s - loss: 121.5223 - val_loss: 76.7453\nEpoch 8/50\n - 0s - loss: 119.4006 - val_loss: 96.5571\nEpoch 9/50\n - 0s - loss: 121.9816 - val_loss: 83.8084\nEpoch 10/50\n - 0s - loss: 124.4663 - val_loss: 83.3021\nEpoch 11/50\n - 0s - loss: 124.7075 - val_loss: 77.6221\nEpoch 12/50\n - 11s - loss: 122.6246 - val_loss: 77.0929\nEpoch 13/50\n - 0s - loss: 123.4387 - val_loss: 78.6245\nEpoch 14/50\n - 0s - loss: 119.2473 - val_loss: 81.8588\nEpoch 15/50\n - 0s - loss: 121.6313 - val_loss: 76.5597\nEpoch 16/50\n - 0s - loss: 123.0058 - val_loss: 99.7365\nEpoch 17/50\n - 0s - loss: 132.1634 - val_loss: 101.0005\nEpoch 18/50\n - 0s - loss: 122.7902 - val_loss: 94.0514\nEpoch 19/50\n - 0s - loss: 124.1713 - val_loss: 82.3151\nEpoch 20/50\n - 0s - loss: 119.2146 - val_loss: 95.0806\nEpoch 21/50\n - 0s - loss: 124.1529 - val_loss: 77.6643\nEpoch 22/50\n - 0s - loss: 121.5510 - val_loss: 75.7695\nEpoch 23/50\n - 0s - loss: 124.9404 - val_loss: 79.8458\nEpoch 24/50\n - 0s - loss: 124.9955 - val_loss: 74.4402\nEpoch 25/50\n - 0s - loss: 121.5154 - val_loss: 87.5139\nEpoch 26/50\n - 0s - loss: 122.7773 - val_loss: 75.5260\nEpoch 27/50\n - 0s - loss: 121.3934 - val_loss: 75.4907\nEpoch 28/50\n - 0s - loss: 122.4360 - val_loss: 83.3239\nEpoch 29/50\n - 0s - loss: 125.4581 - val_loss: 78.2541\nEpoch 30/50\n - 0s - loss: 122.4565 - val_loss: 81.9351\nEpoch 31/50\n - 0s - loss: 119.1589 - val_loss: 74.0945\nEpoch 32/50\n - 0s - loss: 119.0876 - val_loss: 75.7739\nEpoch 33/50\n - 0s - loss: 119.8292 - val_loss: 90.4803\nEpoch 34/50\n - 0s - loss: 124.7117 - val_loss: 104.8703\nEpoch 35/50\n - 0s - loss: 124.4102 - val_loss: 78.2795\nEpoch 36/50\n - 0s - loss: 126.1429 - val_loss: 76.7715\nEpoch 37/50\n - 0s - loss: 121.7241 - val_loss: 74.5719\nEpoch 38/50\n - 0s - loss: 119.8250 - val_loss: 76.1060\nEpoch 39/50\n - 10s - loss: 123.5603 - val_loss: 76.2462\nEpoch 40/50\n - 0s - loss: 125.4553 - val_loss: 76.5413\nEpoch 41/50\n - 0s - loss: 130.3979 - val_loss: 74.7587\nEpoch 42/50\n - 0s - loss: 120.9614 - val_loss: 92.8068\nEpoch 43/50\n - 0s - loss: 124.8847 - val_loss: 76.2586\nEpoch 44/50\n - 0s - loss: 120.4525 - val_loss: 74.5912\nEpoch 45/50\n - 0s - loss: 126.5257 - val_loss: 75.0580\nEpoch 46/50\n - 0s - loss: 126.6997 - val_loss: 76.6188\nEpoch 47/50\n - 0s - loss: 120.1201 - val_loss: 77.2625\nEpoch 48/50\n - 0s - loss: 122.3549 - val_loss: 73.3676\nEpoch 49/50\n - 0s - loss: 119.8808 - val_loss: 77.0604\nEpoch 50/50\n - 0s - loss: 121.5523 - val_loss: 74.4575\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 124.4972 - val_loss: 74.5076\nEpoch 2/50\n - 0s - loss: 120.1839 - val_loss: 74.6688\nEpoch 3/50\n - 0s - loss: 118.3885 - val_loss: 75.9383\nEpoch 4/50\n - 0s - loss: 125.1055 - val_loss: 80.9783\nEpoch 5/50\n - 0s - loss: 127.7790 - val_loss: 77.6950\nEpoch 6/50\n - 0s - loss: 118.2515 - val_loss: 73.2113\nEpoch 7/50\n - 0s - loss: 119.6838 - val_loss: 78.2081\nEpoch 8/50\n - 0s - loss: 120.9342 - val_loss: 79.5820\nEpoch 9/50\n - 0s - loss: 119.9407 - val_loss: 82.8520\nEpoch 10/50\n - 0s - loss: 124.7179 - val_loss: 81.3486\nEpoch 11/50\n - 5s - loss: 120.8292 - val_loss: 71.8082\nEpoch 12/50\n - 0s - loss: 119.5985 - val_loss: 79.2633\nEpoch 13/50\n - 0s - loss: 122.9946 - val_loss: 78.5934\nEpoch 14/50\n - 4s - loss: 124.4406 - val_loss: 82.8332\nEpoch 15/50\n - 0s - loss: 138.2903 - val_loss: 73.8240\nEpoch 16/50\n - 0s - loss: 125.1845 - val_loss: 73.2052\nEpoch 17/50\n - 0s - loss: 119.3861 - val_loss: 74.5054\nEpoch 18/50\n - 0s - loss: 119.4672 - val_loss: 81.4347\nEpoch 19/50\n - 0s - loss: 123.3125 - val_loss: 76.0984\nEpoch 20/50\n - 0s - loss: 121.2251 - val_loss: 73.3260\nEpoch 21/50\n - 0s - loss: 127.5012 - val_loss: 90.8216\nEpoch 22/50\n - 0s - loss: 125.8310 - val_loss: 99.1589\nEpoch 23/50\n - 0s - loss: 122.6629 - val_loss: 83.8388\nEpoch 24/50\n - 0s - loss: 121.9748 - val_loss: 99.2153\nEpoch 25/50\n - 0s - loss: 122.4387 - val_loss: 75.4729\nEpoch 26/50\n - 0s - loss: 120.7804 - val_loss: 73.3378\nEpoch 27/50\n - 0s - loss: 120.5385 - val_loss: 73.8290\nEpoch 28/50\n - 0s - loss: 120.2638 - val_loss: 75.4436\nEpoch 29/50\n - 0s - loss: 118.7002 - val_loss: 76.0776\nEpoch 30/50\n - 0s - loss: 121.2841 - val_loss: 82.5408\nEpoch 31/50\n - 0s - loss: 123.2619 - val_loss: 103.7883\nEpoch 32/50\n - 0s - loss: 128.1211 - val_loss: 98.7252\nEpoch 33/50\n - 0s - loss: 128.7863 - val_loss: 83.5552\nEpoch 34/50\n - 0s - loss: 119.5480 - val_loss: 99.2155\nEpoch 35/50\n - 0s - loss: 123.5125 - val_loss: 94.6917\nEpoch 36/50\n - 0s - loss: 121.2955 - val_loss: 76.0761\nEpoch 37/50\n - 0s - loss: 121.8140 - val_loss: 75.3839\nEpoch 38/50\n - 5s - loss: 120.9398 - val_loss: 73.6466\nEpoch 39/50\n - 0s - loss: 119.4774 - val_loss: 74.0140\nEpoch 40/50\n - 0s - loss: 126.8581 - val_loss: 81.8498\nEpoch 41/50\n - 0s - loss: 123.8895 - val_loss: 74.6678\nEpoch 42/50\n - 3s - loss: 118.8901 - val_loss: 73.2269\nEpoch 43/50\n - 0s - loss: 125.0916 - val_loss: 75.9785\nEpoch 44/50\n - 0s - loss: 121.9019 - val_loss: 73.2539\nEpoch 45/50\n - 0s - loss: 127.8029 - val_loss: 79.8275\nEpoch 46/50\n - 3s - loss: 128.8380 - val_loss: 89.0636\nEpoch 47/50\n - 0s - loss: 132.1324 - val_loss: 73.8238\nEpoch 48/50\n - 0s - loss: 121.1387 - val_loss: 76.1891\nEpoch 49/50\n - 0s - loss: 117.8412 - val_loss: 81.6518\nEpoch 50/50\n - 0s - loss: 118.4006 - val_loss: 85.2254\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 118.9822 - val_loss: 92.4473\nEpoch 2/50\n - 0s - loss: 118.6776 - val_loss: 76.5210\nEpoch 3/50\n - 0s - loss: 123.5787 - val_loss: 74.6306\nEpoch 4/50\n - 0s - loss: 120.9650 - val_loss: 72.8643\nEpoch 5/50\n - 1s - loss: 117.8614 - val_loss: 77.5745\nEpoch 6/50\n - 0s - loss: 129.0036 - val_loss: 75.3893\nEpoch 7/50\n - 0s - loss: 120.4955 - val_loss: 81.3961\nEpoch 8/50\n - 0s - loss: 122.9420 - val_loss: 80.4872\nEpoch 9/50\n - 6s - loss: 121.3192 - val_loss: 89.4666\nEpoch 10/50\n - 0s - loss: 124.9591 - val_loss: 72.5322\nEpoch 11/50\n - 5s - loss: 118.9788 - val_loss: 79.0091\nEpoch 12/50\n - 1s - loss: 117.4136 - val_loss: 73.4025\nEpoch 13/50\n - 0s - loss: 120.5491 - val_loss: 77.7120\nEpoch 14/50\n - 0s - loss: 118.6318 - val_loss: 81.4678\nEpoch 15/50\n - 0s - loss: 122.3194 - val_loss: 74.6820\nEpoch 16/50\n - 0s - loss: 125.1704 - val_loss: 74.9662\nEpoch 17/50\n - 0s - loss: 127.0117 - val_loss: 89.6449\nEpoch 18/50\n - 0s - loss: 121.9274 - val_loss: 78.8897\nEpoch 19/50\n - 1s - loss: 117.9444 - val_loss: 76.9798\nEpoch 20/50\n - 0s - loss: 123.5810 - val_loss: 82.9346\nEpoch 21/50\n - 0s - loss: 120.7541 - val_loss: 106.7343\nEpoch 22/50\n - 0s - loss: 122.0106 - val_loss: 82.2829\nEpoch 23/50\n - 0s - loss: 127.5410 - val_loss: 73.8295\nEpoch 24/50\n - 0s - loss: 119.8224 - val_loss: 72.7574\nEpoch 25/50\n - 0s - loss: 120.3274 - val_loss: 102.4024\nEpoch 26/50\n - 0s - loss: 121.3146 - val_loss: 89.7909\nEpoch 27/50\n - 0s - loss: 120.8587 - val_loss: 76.1004\nEpoch 28/50\n - 0s - loss: 122.6851 - val_loss: 73.5534\nEpoch 29/50\n - 0s - loss: 118.7379 - val_loss: 79.6658\nEpoch 30/50\n - 0s - loss: 120.5039 - val_loss: 76.6083\nEpoch 31/50\n - 0s - loss: 118.8058 - val_loss: 93.6488\nEpoch 32/50\n - 5s - loss: 125.8229 - val_loss: 79.0260\nEpoch 33/50\n - 6s - loss: 128.6669 - val_loss: 116.5195\nEpoch 34/50\n - 0s - loss: 127.1782 - val_loss: 80.1361\nEpoch 35/50\n - 0s - loss: 119.0096 - val_loss: 75.5343\nEpoch 36/50\n - 0s - loss: 122.3607 - val_loss: 74.0394\nEpoch 37/50\n - 0s - loss: 118.9158 - val_loss: 74.1034\nEpoch 38/50\n - 0s - loss: 122.3973 - val_loss: 77.9879\nEpoch 39/50\n - 0s - loss: 118.7543 - val_loss: 73.8368\nEpoch 40/50\n - 0s - loss: 117.6563 - val_loss: 77.2890\nEpoch 41/50\n - 0s - loss: 119.5200 - val_loss: 74.2177\nEpoch 42/50\n - 0s - loss: 118.1791 - val_loss: 80.2788\nEpoch 43/50\n - 0s - loss: 120.0824 - val_loss: 72.6022\nEpoch 44/50\n - 0s - loss: 120.8727 - val_loss: 75.0659\nEpoch 45/50\n - 1s - loss: 130.7587 - val_loss: 77.5237\nEpoch 46/50\n - 0s - loss: 124.7204 - val_loss: 74.0340\nEpoch 47/50\n - 0s - loss: 117.8996 - val_loss: 79.5546\nEpoch 48/50\n - 0s - loss: 123.7584 - val_loss: 79.8595\nEpoch 49/50\n - 0s - loss: 118.1096 - val_loss: 79.9877\nEpoch 50/50\n - 0s - loss: 121.9915 - val_loss: 94.0980\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 5s - loss: 126.2165 - val_loss: 84.8620\nEpoch 2/50\n - 0s - loss: 119.3957 - val_loss: 72.6241\nEpoch 3/50\n - 0s - loss: 120.0780 - val_loss: 79.5811\nEpoch 4/50\n - 0s - loss: 126.0158 - val_loss: 74.8566\nEpoch 5/50\n - 5s - loss: 121.6097 - val_loss: 84.2508\nEpoch 6/50\n - 0s - loss: 120.6700 - val_loss: 82.8918\nEpoch 7/50\n - 1s - loss: 119.9941 - val_loss: 74.4489\nEpoch 8/50\n - 0s - loss: 118.9848 - val_loss: 76.6610\nEpoch 9/50\n - 0s - loss: 121.9293 - val_loss: 81.3874\nEpoch 10/50\n - 0s - loss: 119.7170 - val_loss: 75.0480\nEpoch 11/50\n - 0s - loss: 134.4143 - val_loss: 71.6480\nEpoch 12/50\n - 0s - loss: 125.2603 - val_loss: 73.4607\nEpoch 13/50\n - 0s - loss: 120.8562 - val_loss: 72.5064\nEpoch 14/50\n - 0s - loss: 118.0207 - val_loss: 76.3207\nEpoch 15/50\n - 1s - loss: 120.2844 - val_loss: 74.5969\nEpoch 16/50\n - 0s - loss: 117.0197 - val_loss: 79.2501\nEpoch 17/50\n - 1s - loss: 117.0672 - val_loss: 71.6541\nEpoch 18/50\n - 0s - loss: 117.4585 - val_loss: 72.7505\nEpoch 19/50\n - 0s - loss: 117.9801 - val_loss: 77.2250\nEpoch 20/50\n - 0s - loss: 122.5298 - val_loss: 94.6696\nEpoch 21/50\n - 0s - loss: 122.1350 - val_loss: 88.3821\nEpoch 22/50\n - 5s - loss: 129.7826 - val_loss: 87.7121\nEpoch 23/50\n - 0s - loss: 121.6878 - val_loss: 79.6607\nEpoch 24/50\n - 0s - loss: 120.0850 - val_loss: 78.0567\nEpoch 25/50\n - 0s - loss: 119.5100 - val_loss: 76.5001\nEpoch 26/50\n - 3s - loss: 122.8185 - val_loss: 90.4147\nEpoch 27/50\n - 0s - loss: 122.1094 - val_loss: 78.5637\nEpoch 28/50\n - 0s - loss: 123.1220 - val_loss: 82.5664\nEpoch 29/50\n - 0s - loss: 118.5605 - val_loss: 73.4997\nEpoch 30/50\n - 0s - loss: 123.9242 - val_loss: 104.3405\nEpoch 31/50\n - 0s - loss: 121.3580 - val_loss: 86.7759\nEpoch 32/50\n - 0s - loss: 128.8717 - val_loss: 83.2655\nEpoch 33/50\n - 0s - loss: 127.7179 - val_loss: 72.6218\nEpoch 34/50\n - 1s - loss: 119.8236 - val_loss: 76.7821\nEpoch 35/50\n - 0s - loss: 129.6087 - val_loss: 86.4300\nEpoch 36/50\n - 0s - loss: 128.8657 - val_loss: 72.0097\nEpoch 37/50\n - 0s - loss: 118.1638 - val_loss: 73.6425\nEpoch 38/50\n - 0s - loss: 119.4677 - val_loss: 73.0018\nEpoch 39/50\n - 0s - loss: 118.4300 - val_loss: 75.0485\nEpoch 40/50\n - 0s - loss: 119.5888 - val_loss: 73.1128\nEpoch 41/50\n - 0s - loss: 118.1461 - val_loss: 80.6564\nEpoch 42/50\n - 0s - loss: 118.9759 - val_loss: 72.9041\nEpoch 43/50\n - 0s - loss: 117.4250 - val_loss: 79.8418\nEpoch 44/50\n - 0s - loss: 123.7543 - val_loss: 73.0365\nEpoch 45/50\n - 0s - loss: 117.6474 - val_loss: 73.5993\nEpoch 46/50\n - 5s - loss: 119.6343 - val_loss: 76.0767\nEpoch 47/50\n - 0s - loss: 124.3548 - val_loss: 73.4649\nEpoch 48/50\n - 0s - loss: 117.8351 - val_loss: 75.5843\nEpoch 49/50\n - 6s - loss: 118.8207 - val_loss: 73.6717\nEpoch 50/50\n - 0s - loss: 118.6508 - val_loss: 88.2619\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 123.7534 - val_loss: 82.7180\nEpoch 2/50\n - 0s - loss: 118.7658 - val_loss: 74.3594\nEpoch 3/50\n - 0s - loss: 130.0603 - val_loss: 75.2391\nEpoch 4/50\n - 0s - loss: 118.8895 - val_loss: 72.9713\nEpoch 5/50\n - 0s - loss: 118.6582 - val_loss: 73.6259\nEpoch 6/50\n - 0s - loss: 116.3275 - val_loss: 83.8032\nEpoch 7/50\n - 0s - loss: 117.5971 - val_loss: 75.6190\nEpoch 8/50\n - 0s - loss: 122.7720 - val_loss: 73.4289\nEpoch 9/50\n - 0s - loss: 117.9038 - val_loss: 75.4882\nEpoch 10/50\n - 0s - loss: 121.0105 - val_loss: 106.8300\nEpoch 11/50\n - 0s - loss: 127.4937 - val_loss: 73.4400\nEpoch 12/50\n - 0s - loss: 117.1583 - val_loss: 88.1494\nEpoch 13/50\n - 5s - loss: 124.2160 - val_loss: 99.0784\nEpoch 14/50\n - 0s - loss: 121.4450 - val_loss: 86.5758\nEpoch 15/50\n - 0s - loss: 127.8697 - val_loss: 99.1204\nEpoch 16/50\n - 0s - loss: 124.3215 - val_loss: 95.8337\nEpoch 17/50\n - 4s - loss: 121.7567 - val_loss: 87.5036\nEpoch 18/50\n - 0s - loss: 119.0144 - val_loss: 110.8061\nEpoch 19/50\n - 0s - loss: 124.5766 - val_loss: 77.9642\nEpoch 20/50\n - 0s - loss: 122.3035 - val_loss: 72.4300\nEpoch 21/50\n - 0s - loss: 119.1417 - val_loss: 94.5977\nEpoch 22/50\n - 0s - loss: 127.7405 - val_loss: 81.8768\nEpoch 23/50\n - 0s - loss: 119.4213 - val_loss: 86.4263\nEpoch 24/50\n - 0s - loss: 116.9782 - val_loss: 83.7698\nEpoch 25/50\n - 0s - loss: 118.3891 - val_loss: 88.7971\nEpoch 26/50\n - 0s - loss: 118.2730 - val_loss: 73.9335\nEpoch 27/50\n - 0s - loss: 121.3657 - val_loss: 74.7958\nEpoch 28/50\n - 0s - loss: 120.7287 - val_loss: 72.6241\nEpoch 29/50\n - 3s - loss: 118.4281 - val_loss: 82.7350\nEpoch 30/50\n - 0s - loss: 116.6651 - val_loss: 76.8608\nEpoch 31/50\n - 5s - loss: 117.4361 - val_loss: 76.0912\nEpoch 32/50\n - 0s - loss: 119.2658 - val_loss: 76.2962\nEpoch 33/50\n - 1s - loss: 132.0329 - val_loss: 81.9037\nEpoch 34/50\n - 3s - loss: 117.5290 - val_loss: 74.2562\nEpoch 35/50\n - 0s - loss: 119.0270 - val_loss: 75.5021\nEpoch 36/50\n - 0s - loss: 119.9041 - val_loss: 82.9078\nEpoch 37/50\n - 0s - loss: 119.0169 - val_loss: 85.8832\nEpoch 38/50\n - 0s - loss: 116.4139 - val_loss: 75.4957\nEpoch 39/50\n - 0s - loss: 118.9946 - val_loss: 81.9190\nEpoch 40/50\n - 0s - loss: 116.4292 - val_loss: 89.0168\nEpoch 41/50\n - 0s - loss: 118.4726 - val_loss: 83.4315\nEpoch 42/50\n - 0s - loss: 133.9515 - val_loss: 80.8017\nEpoch 43/50\n - 0s - loss: 129.5633 - val_loss: 78.9787\nEpoch 44/50\n - 0s - loss: 120.0536 - val_loss: 83.3438\nEpoch 45/50\n - 0s - loss: 121.6713 - val_loss: 85.1890\nEpoch 46/50\n - 0s - loss: 121.0012 - val_loss: 80.3400\nEpoch 47/50\n - 0s - loss: 116.0018 - val_loss: 78.8635\nEpoch 48/50\n - 0s - loss: 117.4102 - val_loss: 72.7083\nEpoch 49/50\n - 0s - loss: 119.9730 - val_loss: 78.4515\nEpoch 50/50\n - 0s - loss: 119.9475 - val_loss: 90.2625\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 3s - loss: 123.7837 - val_loss: 71.0529\nEpoch 2/50\n - 8s - loss: 116.5722 - val_loss: 77.0669\nEpoch 3/50\n - 0s - loss: 118.5789 - val_loss: 72.3681\nEpoch 4/50\n - 0s - loss: 118.9058 - val_loss: 73.6753\nEpoch 5/50\n - 0s - loss: 118.7061 - val_loss: 76.6069\nEpoch 6/50\n - 0s - loss: 116.3605 - val_loss: 84.5053\nEpoch 7/50\n - 0s - loss: 119.2330 - val_loss: 99.4992\nEpoch 8/50\n - 3s - loss: 122.3380 - val_loss: 79.2181\nEpoch 9/50\n - 1s - loss: 119.5238 - val_loss: 75.7505\nEpoch 10/50\n - 0s - loss: 117.7867 - val_loss: 73.1612\nEpoch 11/50\n - 1s - loss: 124.4811 - val_loss: 73.7493\nEpoch 12/50\n - 0s - loss: 119.6633 - val_loss: 78.9001\nEpoch 13/50\n - 0s - loss: 115.8178 - val_loss: 79.3662\nEpoch 14/50\n - 0s - loss: 122.6283 - val_loss: 98.1969\nEpoch 15/50\n - 0s - loss: 130.9118 - val_loss: 73.2674\nEpoch 16/50\n - 0s - loss: 122.8074 - val_loss: 81.6906\nEpoch 17/50\n - 0s - loss: 116.3084 - val_loss: 74.0472\nEpoch 18/50\n - 0s - loss: 116.5742 - val_loss: 77.4194\nEpoch 19/50\n - 5s - loss: 122.2849 - val_loss: 79.9960\nEpoch 20/50\n - 4s - loss: 116.8317 - val_loss: 77.3707\nEpoch 21/50\n - 0s - loss: 119.5679 - val_loss: 72.3726\nEpoch 22/50\n - 1s - loss: 119.5143 - val_loss: 76.5029\nEpoch 23/50\n - 1s - loss: 117.7423 - val_loss: 71.7483\nEpoch 24/50\n - 0s - loss: 116.8379 - val_loss: 97.9109\nEpoch 25/50\n - 0s - loss: 118.6744 - val_loss: 86.0548\nEpoch 26/50\n - 0s - loss: 117.5360 - val_loss: 72.0830\nEpoch 27/50\n - 0s - loss: 117.4518 - val_loss: 73.1966\nEpoch 28/50\n - 0s - loss: 116.1173 - val_loss: 80.4788\nEpoch 29/50\n - 0s - loss: 120.0101 - val_loss: 73.3766\nEpoch 30/50\n - 0s - loss: 117.6320 - val_loss: 72.0933\nEpoch 31/50\n - 0s - loss: 126.2112 - val_loss: 96.8246\nEpoch 32/50\n - 0s - loss: 120.1951 - val_loss: 81.3002\nEpoch 33/50\n - 0s - loss: 117.1095 - val_loss: 73.0522\nEpoch 34/50\n - 0s - loss: 115.9664 - val_loss: 76.0092\nEpoch 35/50\n - 0s - loss: 115.7000 - val_loss: 77.5169\nEpoch 36/50\n - 0s - loss: 119.2009 - val_loss: 72.8324\nEpoch 37/50\n - 0s - loss: 116.6383 - val_loss: 73.1882\nEpoch 38/50\n - 1s - loss: 118.8215 - val_loss: 73.9592\nEpoch 39/50\n - 0s - loss: 115.8365 - val_loss: 76.0841\nEpoch 40/50\n - 0s - loss: 119.2105 - val_loss: 76.8190\nEpoch 41/50\n - 0s - loss: 116.2882 - val_loss: 112.3903\nEpoch 42/50\n - 0s - loss: 122.7819 - val_loss: 73.0098\nEpoch 43/50\n - 0s - loss: 117.7086 - val_loss: 79.0419\nEpoch 44/50\n - 0s - loss: 123.7724 - val_loss: 90.1864\nEpoch 45/50\n - 5s - loss: 120.1082 - val_loss: 79.7133\nEpoch 46/50\n - 0s - loss: 116.8903 - val_loss: 71.7266\nEpoch 47/50\n - 0s - loss: 118.3884 - val_loss: 75.1800\nEpoch 48/50\n - 3s - loss: 117.0303 - val_loss: 86.6888\nEpoch 49/50\n - 0s - loss: 123.0165 - val_loss: 81.8021\nEpoch 50/50\n - 0s - loss: 121.5812 - val_loss: 72.6178\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 119.8669 - val_loss: 75.6507\nEpoch 2/50\n - 1s - loss: 125.3069 - val_loss: 98.8944\nEpoch 3/50\n - 0s - loss: 134.1696 - val_loss: 85.6523\nEpoch 4/50\n - 0s - loss: 121.2619 - val_loss: 76.2259\nEpoch 5/50\n - 1s - loss: 119.7045 - val_loss: 85.3504\nEpoch 6/50\n - 0s - loss: 118.5928 - val_loss: 79.6124\nEpoch 7/50\n - 0s - loss: 118.0674 - val_loss: 82.6770\nEpoch 8/50\n - 0s - loss: 119.4746 - val_loss: 72.6008\nEpoch 9/50\n - 0s - loss: 122.5877 - val_loss: 72.8552\nEpoch 10/50\n - 0s - loss: 115.9797 - val_loss: 78.2752\nEpoch 11/50\n - 0s - loss: 116.1771 - val_loss: 77.6940\nEpoch 12/50\n - 0s - loss: 115.9429 - val_loss: 81.7025\nEpoch 13/50\n - 0s - loss: 117.8986 - val_loss: 73.1865\nEpoch 14/50\n - 0s - loss: 117.6062 - val_loss: 75.0185\nEpoch 15/50\n - 0s - loss: 119.6346 - val_loss: 85.2656\nEpoch 16/50\n - 0s - loss: 120.6531 - val_loss: 83.3892\nEpoch 17/50\n - 0s - loss: 115.9191 - val_loss: 85.8206\nEpoch 18/50\n - 0s - loss: 117.6929 - val_loss: 82.0999\nEpoch 19/50\n - 5s - loss: 118.0946 - val_loss: 73.2916\nEpoch 20/50\n - 0s - loss: 119.2693 - val_loss: 79.0716\nEpoch 21/50\n - 0s - loss: 120.0357 - val_loss: 75.7087\nEpoch 22/50\n - 0s - loss: 116.3331 - val_loss: 71.9765\nEpoch 23/50\n - 3s - loss: 119.7623 - val_loss: 73.0777\nEpoch 24/50\n - 0s - loss: 121.1078 - val_loss: 76.7697\nEpoch 25/50\n - 1s - loss: 118.4625 - val_loss: 88.8380\nEpoch 26/50\n - 0s - loss: 118.2380 - val_loss: 73.8473\nEpoch 27/50\n - 1s - loss: 120.2023 - val_loss: 92.9065\nEpoch 28/50\n - 1s - loss: 129.7628 - val_loss: 77.8957\nEpoch 29/50\n - 0s - loss: 121.7388 - val_loss: 85.6643\nEpoch 30/50\n - 0s - loss: 120.3957 - val_loss: 76.2251\nEpoch 31/50\n - 0s - loss: 121.0862 - val_loss: 75.3649\nEpoch 32/50\n - 0s - loss: 120.0000 - val_loss: 84.6536\nEpoch 33/50\n - 0s - loss: 120.1743 - val_loss: 82.7050\nEpoch 34/50\n - 0s - loss: 122.1878 - val_loss: 91.2715\nEpoch 35/50\n - 0s - loss: 116.4681 - val_loss: 72.8883\nEpoch 36/50\n - 3s - loss: 116.7263 - val_loss: 72.8656\nEpoch 37/50\n - 0s - loss: 119.4339 - val_loss: 74.4561\nEpoch 38/50\n - 0s - loss: 118.3574 - val_loss: 73.6909\nEpoch 39/50\n - 0s - loss: 116.4841 - val_loss: 73.8556\nEpoch 40/50\n - 8s - loss: 117.9255 - val_loss: 78.2946\nEpoch 41/50\n - 0s - loss: 122.1496 - val_loss: 79.4784\nEpoch 42/50\n - 0s - loss: 115.7055 - val_loss: 100.0431\nEpoch 43/50\n - 0s - loss: 116.2023 - val_loss: 82.5726\nEpoch 44/50\n - 0s - loss: 120.6610 - val_loss: 107.3675\nEpoch 45/50\n - 0s - loss: 122.4093 - val_loss: 76.0377\nEpoch 46/50\n - 0s - loss: 123.1278 - val_loss: 72.7658\nEpoch 47/50\n - 1s - loss: 118.1257 - val_loss: 73.3672\nEpoch 48/50\n - 0s - loss: 117.6889 - val_loss: 81.7526\nEpoch 49/50\n - 1s - loss: 116.1910 - val_loss: 85.1856\nEpoch 50/50\n - 1s - loss: 116.7245 - val_loss: 72.0865\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 133.3433 - val_loss: 131.4077\nEpoch 2/50\n - 0s - loss: 148.1376 - val_loss: 85.8795\nEpoch 3/50\n - 0s - loss: 118.2134 - val_loss: 84.3494\nEpoch 4/50\n - 0s - loss: 115.7912 - val_loss: 73.5093\nEpoch 5/50\n - 0s - loss: 119.0427 - val_loss: 92.8563\nEpoch 6/50\n - 0s - loss: 120.9135 - val_loss: 119.1536\nEpoch 7/50\n - 7s - loss: 140.7925 - val_loss: 120.9917\nEpoch 8/50\n - 1s - loss: 131.5657 - val_loss: 97.7660\nEpoch 9/50\n - 3s - loss: 120.2086 - val_loss: 75.6006\nEpoch 10/50\n - 0s - loss: 115.8982 - val_loss: 72.9931\nEpoch 11/50\n - 1s - loss: 117.7421 - val_loss: 84.5546\nEpoch 12/50\n - 0s - loss: 119.4622 - val_loss: 88.3009\nEpoch 13/50\n - 0s - loss: 117.5307 - val_loss: 76.2359\nEpoch 14/50\n - 0s - loss: 118.5861 - val_loss: 74.0025\nEpoch 15/50\n - 0s - loss: 117.2499 - val_loss: 78.1939\nEpoch 16/50\n - 0s - loss: 116.5870 - val_loss: 73.1211\nEpoch 17/50\n - 0s - loss: 125.8131 - val_loss: 74.1106\nEpoch 18/50\n - 0s - loss: 119.8684 - val_loss: 85.6079\nEpoch 19/50\n - 0s - loss: 115.9471 - val_loss: 74.9932\nEpoch 20/50\n - 0s - loss: 120.4470 - val_loss: 76.0281\nEpoch 21/50\n - 0s - loss: 123.0691 - val_loss: 73.7506\nEpoch 22/50\n - 0s - loss: 125.6376 - val_loss: 79.3400\nEpoch 23/50\n - 0s - loss: 117.3581 - val_loss: 74.6487\nEpoch 24/50\n - 0s - loss: 118.5882 - val_loss: 74.6853\nEpoch 25/50\n - 3s - loss: 118.4986 - val_loss: 78.2762\nEpoch 26/50\n - 0s - loss: 118.4734 - val_loss: 72.4180\nEpoch 27/50\n - 0s - loss: 118.6477 - val_loss: 76.2954\nEpoch 28/50\n - 5s - loss: 116.0992 - val_loss: 90.1950\nEpoch 29/50\n - 0s - loss: 122.5477 - val_loss: 73.9010\nEpoch 30/50\n - 0s - loss: 116.7711 - val_loss: 73.5894\nEpoch 31/50\n - 4s - loss: 123.8528 - val_loss: 75.5721\nEpoch 32/50\n - 0s - loss: 119.6146 - val_loss: 74.1467\nEpoch 33/50\n - 0s - loss: 119.1579 - val_loss: 72.4925\nEpoch 34/50\n - 0s - loss: 118.8760 - val_loss: 72.3213\nEpoch 35/50\n - 0s - loss: 123.3099 - val_loss: 74.7939\nEpoch 36/50\n - 0s - loss: 118.3285 - val_loss: 79.6861\nEpoch 37/50\n - 0s - loss: 117.6569 - val_loss: 79.4893\nEpoch 38/50\n - 0s - loss: 123.7612 - val_loss: 72.7584\nEpoch 39/50\n - 0s - loss: 117.5244 - val_loss: 72.5157\nEpoch 40/50\n - 0s - loss: 116.6283 - val_loss: 81.2134\nEpoch 41/50\n - 3s - loss: 123.9752 - val_loss: 73.5712\nEpoch 42/50\n - 0s - loss: 121.3296 - val_loss: 82.5377\nEpoch 43/50\n - 5s - loss: 125.4261 - val_loss: 79.9224\nEpoch 44/50\n - 0s - loss: 117.7941 - val_loss: 74.5527\nEpoch 45/50\n - 0s - loss: 122.8510 - val_loss: 79.0583\nEpoch 46/50\n - 0s - loss: 133.4505 - val_loss: 76.3906\nEpoch 47/50\n - 3s - loss: 130.4565 - val_loss: 116.7198\nEpoch 48/50\n - 0s - loss: 132.0043 - val_loss: 77.2832\nEpoch 49/50\n - 0s - loss: 117.2022 - val_loss: 75.0008\nEpoch 50/50\n - 3s - loss: 116.4365 - val_loss: 73.8116\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 124.2482 - val_loss: 81.2106\nEpoch 2/50\n - 0s - loss: 122.5983 - val_loss: 74.3945\nEpoch 3/50\n - 0s - loss: 121.8952 - val_loss: 73.9388\nEpoch 4/50\n - 0s - loss: 115.5236 - val_loss: 87.2032\nEpoch 5/50\n - 0s - loss: 121.1034 - val_loss: 73.3258\nEpoch 6/50\n - 0s - loss: 119.6228 - val_loss: 74.8691\nEpoch 7/50\n - 0s - loss: 117.9853 - val_loss: 76.9883\nEpoch 8/50\n - 0s - loss: 119.6471 - val_loss: 74.2799\nEpoch 9/50\n - 0s - loss: 116.9867 - val_loss: 73.1402\nEpoch 10/50\n - 0s - loss: 116.3261 - val_loss: 82.9016\nEpoch 11/50\n - 0s - loss: 116.0113 - val_loss: 89.3549\nEpoch 12/50\n - 0s - loss: 114.2510 - val_loss: 93.7094\nEpoch 13/50\n - 0s - loss: 118.1317 - val_loss: 102.3314\nEpoch 14/50\n - 0s - loss: 116.6623 - val_loss: 73.9720\nEpoch 15/50\n - 0s - loss: 116.7300 - val_loss: 79.2674\nEpoch 16/50\n - 5s - loss: 113.3582 - val_loss: 90.8930\nEpoch 17/50\n - 0s - loss: 119.9734 - val_loss: 102.1784\nEpoch 18/50\n - 1s - loss: 114.5968 - val_loss: 88.3751\nEpoch 19/50\n - 3s - loss: 114.4395 - val_loss: 103.8235\nEpoch 20/50\n - 0s - loss: 121.2131 - val_loss: 82.7488\nEpoch 21/50\n - 0s - loss: 119.1158 - val_loss: 75.3793\nEpoch 22/50\n - 0s - loss: 117.9646 - val_loss: 96.0956\nEpoch 23/50\n - 0s - loss: 117.4203 - val_loss: 73.7190\nEpoch 24/50\n - 0s - loss: 112.7718 - val_loss: 73.1469\nEpoch 25/50\n - 0s - loss: 113.4539 - val_loss: 78.5248\nEpoch 26/50\n - 0s - loss: 113.3162 - val_loss: 75.6973\nEpoch 27/50\n - 0s - loss: 117.2935 - val_loss: 76.8416\nEpoch 28/50\n - 0s - loss: 118.4547 - val_loss: 81.1115\nEpoch 29/50\n - 0s - loss: 122.7466 - val_loss: 75.4553\nEpoch 30/50\n - 0s - loss: 110.9743 - val_loss: 111.7226\nEpoch 31/50\n - 0s - loss: 114.7539 - val_loss: 87.9945\nEpoch 32/50\n - 0s - loss: 110.7835 - val_loss: 75.2230\nEpoch 33/50\n - 0s - loss: 114.0775 - val_loss: 75.2749\nEpoch 34/50\n - 0s - loss: 109.7888 - val_loss: 85.5267\nEpoch 35/50\n - 0s - loss: 114.3625 - val_loss: 117.3768\nEpoch 36/50\n - 0s - loss: 122.6850 - val_loss: 101.4159\nEpoch 37/50\n - 0s - loss: 113.7674 - val_loss: 79.7866\nEpoch 38/50\n - 0s - loss: 110.5378 - val_loss: 82.6177\nEpoch 39/50\n - 0s - loss: 114.3813 - val_loss: 80.4667\nEpoch 40/50\n - 0s - loss: 111.4144 - val_loss: 73.0026\nEpoch 41/50\n - 0s - loss: 110.9999 - val_loss: 117.7816\nEpoch 42/50\n - 0s - loss: 112.8204 - val_loss: 92.6549\nEpoch 43/50\n - 0s - loss: 113.0504 - val_loss: 78.2418\nEpoch 44/50\n - 0s - loss: 113.0479 - val_loss: 109.7749\nEpoch 45/50\n - 0s - loss: 111.5831 - val_loss: 76.0432\nEpoch 46/50\n - 5s - loss: 110.5460 - val_loss: 92.4675\nEpoch 47/50\n - 4s - loss: 114.6655 - val_loss: 84.6427\nEpoch 48/50\n - 1s - loss: 108.9125 - val_loss: 81.1776\nEpoch 49/50\n - 0s - loss: 109.7228 - val_loss: 89.2298\nEpoch 50/50\n - 0s - loss: 111.0020 - val_loss: 109.3162\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 112.8980 - val_loss: 75.4366\nEpoch 2/50\n - 0s - loss: 112.5877 - val_loss: 77.9106\nEpoch 3/50\n - 0s - loss: 109.9640 - val_loss: 78.7793\nEpoch 4/50\n - 0s - loss: 108.1161 - val_loss: 77.1639\nEpoch 5/50\n - 0s - loss: 110.9341 - val_loss: 99.6800\nEpoch 6/50\n - 1s - loss: 121.4289 - val_loss: 169.0766\nEpoch 7/50\n - 0s - loss: 116.6735 - val_loss: 110.4809\nEpoch 8/50\n - 0s - loss: 110.0312 - val_loss: 101.5738\nEpoch 9/50\n - 0s - loss: 117.4083 - val_loss: 127.6231\nEpoch 10/50\n - 0s - loss: 112.8147 - val_loss: 81.2296\nEpoch 11/50\n - 0s - loss: 109.5162 - val_loss: 80.6768\nEpoch 12/50\n - 0s - loss: 107.4003 - val_loss: 91.0999\nEpoch 13/50\n - 0s - loss: 114.4420 - val_loss: 104.2258\nEpoch 14/50\n - 0s - loss: 112.5861 - val_loss: 89.6829\nEpoch 15/50\n - 0s - loss: 110.1960 - val_loss: 97.4209\nEpoch 16/50\n - 7s - loss: 108.4394 - val_loss: 95.8891\nEpoch 17/50\n - 0s - loss: 107.9723 - val_loss: 80.6551\nEpoch 18/50\n - 0s - loss: 107.8637 - val_loss: 79.2332\nEpoch 19/50\n - 0s - loss: 112.6658 - val_loss: 92.3691\nEpoch 20/50\n - 0s - loss: 111.4445 - val_loss: 83.9427\nEpoch 21/50\n - 0s - loss: 108.1660 - val_loss: 77.5895\nEpoch 22/50\n - 0s - loss: 109.1173 - val_loss: 105.3085\nEpoch 23/50\n - 0s - loss: 108.5802 - val_loss: 77.0714\nEpoch 24/50\n - 0s - loss: 117.2079 - val_loss: 77.8869\nEpoch 25/50\n - 0s - loss: 110.6298 - val_loss: 75.2683\nEpoch 26/50\n - 0s - loss: 112.5215 - val_loss: 76.6723\nEpoch 27/50\n - 0s - loss: 113.5129 - val_loss: 76.4788\nEpoch 28/50\n - 0s - loss: 106.6184 - val_loss: 103.4932\nEpoch 29/50\n - 0s - loss: 110.6111 - val_loss: 107.3148\nEpoch 30/50\n - 0s - loss: 107.5280 - val_loss: 79.4411\nEpoch 31/50\n - 1s - loss: 106.5691 - val_loss: 121.7973\nEpoch 32/50\n - 0s - loss: 109.4962 - val_loss: 76.8522\nEpoch 33/50\n - 0s - loss: 106.7819 - val_loss: 83.0453\nEpoch 34/50\n - 0s - loss: 108.8815 - val_loss: 75.7660\nEpoch 35/50\n - 0s - loss: 114.6453 - val_loss: 81.2294\nEpoch 36/50\n - 0s - loss: 109.8866 - val_loss: 79.4847\nEpoch 37/50\n - 0s - loss: 117.5948 - val_loss: 75.2474\nEpoch 38/50\n - 0s - loss: 110.0023 - val_loss: 78.0452\nEpoch 39/50\n - 1s - loss: 108.1068 - val_loss: 83.6295\nEpoch 40/50\n - 7s - loss: 115.0215 - val_loss: 84.0780\nEpoch 41/50\n - 0s - loss: 122.4481 - val_loss: 84.3282\nEpoch 42/50\n - 0s - loss: 106.2305 - val_loss: 117.7467\nEpoch 43/50\n - 0s - loss: 109.2351 - val_loss: 80.1218\nEpoch 44/50\n - 0s - loss: 108.0165 - val_loss: 80.0800\nEpoch 45/50\n - 0s - loss: 108.2923 - val_loss: 78.6822\nEpoch 46/50\n - 0s - loss: 108.0274 - val_loss: 73.1463\nEpoch 47/50\n - 0s - loss: 109.1429 - val_loss: 74.4665\nEpoch 48/50\n - 0s - loss: 109.4240 - val_loss: 81.6808\nEpoch 49/50\n - 0s - loss: 106.6809 - val_loss: 75.9734\nEpoch 50/50\n - 0s - loss: 109.2875 - val_loss: 77.6204\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 106.3291 - val_loss: 74.0417\nEpoch 2/50\n - 0s - loss: 105.1418 - val_loss: 104.5578\nEpoch 3/50\n - 1s - loss: 108.3489 - val_loss: 75.6783\nEpoch 4/50\n - 0s - loss: 108.4198 - val_loss: 79.6636\nEpoch 5/50\n - 0s - loss: 107.1535 - val_loss: 74.6152\nEpoch 6/50\n - 0s - loss: 107.2901 - val_loss: 103.8723\nEpoch 7/50\n - 1s - loss: 110.1861 - val_loss: 96.9464\nEpoch 8/50\n - 0s - loss: 104.4946 - val_loss: 74.8478\nEpoch 9/50\n - 0s - loss: 109.5170 - val_loss: 85.7129\nEpoch 10/50\n - 0s - loss: 105.7165 - val_loss: 82.9498\nEpoch 11/50\n - 0s - loss: 114.0929 - val_loss: 88.0508\nEpoch 12/50\n - 0s - loss: 108.7505 - val_loss: 103.1090\nEpoch 13/50\n - 6s - loss: 102.3212 - val_loss: 109.2416\nEpoch 14/50\n - 0s - loss: 102.7575 - val_loss: 97.0744\nEpoch 15/50\n - 0s - loss: 101.2322 - val_loss: 91.5785\nEpoch 16/50\n - 3s - loss: 100.1400 - val_loss: 93.1444\nEpoch 17/50\n - 0s - loss: 100.1375 - val_loss: 81.6093\nEpoch 18/50\n - 0s - loss: 98.6010 - val_loss: 100.5759\nEpoch 19/50\n - 0s - loss: 98.5884 - val_loss: 84.7665\nEpoch 20/50\n - 0s - loss: 97.5309 - val_loss: 77.4522\nEpoch 21/50\n - 0s - loss: 93.5767 - val_loss: 82.8627\nEpoch 22/50\n - 0s - loss: 95.8973 - val_loss: 116.0103\nEpoch 23/50\n - 1s - loss: 96.4456 - val_loss: 93.5956\nEpoch 24/50\n - 3s - loss: 88.8711 - val_loss: 87.2960\nEpoch 25/50\n - 0s - loss: 92.1965 - val_loss: 119.8883\nEpoch 26/50\n - 0s - loss: 87.9478 - val_loss: 69.7613\nEpoch 27/50\n - 0s - loss: 85.5293 - val_loss: 90.3791\nEpoch 28/50\n - 1s - loss: 85.1212 - val_loss: 66.8878\nEpoch 29/50\n - 0s - loss: 82.2946 - val_loss: 68.7970\nEpoch 30/50\n - 0s - loss: 81.4020 - val_loss: 79.0751\nEpoch 31/50\n - 0s - loss: 78.7644 - val_loss: 73.0292\nEpoch 32/50\n - 5s - loss: 82.1096 - val_loss: 83.1808\nEpoch 33/50\n - 0s - loss: 79.6835 - val_loss: 72.3528\nEpoch 34/50\n - 0s - loss: 77.6605 - val_loss: 65.6490\nEpoch 35/50\n - 0s - loss: 80.6402 - val_loss: 65.6104\nEpoch 36/50\n - 3s - loss: 80.4647 - val_loss: 64.5281\nEpoch 37/50\n - 0s - loss: 76.9142 - val_loss: 66.8823\nEpoch 38/50\n - 0s - loss: 74.6646 - val_loss: 66.9523\nEpoch 39/50\n - 0s - loss: 74.7476 - val_loss: 64.5120\nEpoch 40/50\n - 0s - loss: 74.6543 - val_loss: 67.2023\nEpoch 41/50\n - 0s - loss: 77.3897 - val_loss: 67.9184\nEpoch 42/50\n - 0s - loss: 75.0201 - val_loss: 64.0319\nEpoch 43/50\n - 0s - loss: 74.7319 - val_loss: 64.9538\nEpoch 44/50\n - 0s - loss: 75.2930 - val_loss: 76.2258\nEpoch 45/50\n - 0s - loss: 76.2659 - val_loss: 81.7053\nEpoch 46/50\n - 1s - loss: 74.3403 - val_loss: 71.3384\nEpoch 47/50\n - 0s - loss: 74.3553 - val_loss: 64.5156\nEpoch 48/50\n - 0s - loss: 72.7275 - val_loss: 74.7209\nEpoch 49/50\n - 0s - loss: 74.7369 - val_loss: 66.6016\nEpoch 50/50\n - 0s - loss: 73.5440 - val_loss: 65.4906\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 3s - loss: 74.0750 - val_loss: 65.7872\nEpoch 2/50\n - 0s - loss: 74.5398 - val_loss: 73.8645\nEpoch 3/50\n - 0s - loss: 72.3846 - val_loss: 65.7275\nEpoch 4/50\n - 6s - loss: 72.0813 - val_loss: 66.1468\nEpoch 5/50\n - 0s - loss: 71.7981 - val_loss: 69.9891\nEpoch 6/50\n - 6s - loss: 71.6914 - val_loss: 64.2617\nEpoch 7/50\n - 0s - loss: 71.8295 - val_loss: 69.5310\nEpoch 8/50\n - 0s - loss: 75.1548 - val_loss: 69.6972\nEpoch 9/50\n - 0s - loss: 84.1959 - val_loss: 64.0603\nEpoch 10/50\n - 0s - loss: 74.5067 - val_loss: 64.5115\nEpoch 11/50\n - 1s - loss: 71.5884 - val_loss: 77.8793\nEpoch 12/50\n - 0s - loss: 72.3601 - val_loss: 71.4091\nEpoch 13/50\n - 0s - loss: 77.9268 - val_loss: 76.6203\nEpoch 14/50\n - 0s - loss: 73.9748 - val_loss: 65.5315\nEpoch 15/50\n - 0s - loss: 73.8852 - val_loss: 65.5352\nEpoch 16/50\n - 0s - loss: 73.9538 - val_loss: 70.8254\nEpoch 17/50\n - 0s - loss: 71.6573 - val_loss: 64.6717\nEpoch 18/50\n - 4s - loss: 76.7949 - val_loss: 71.6557\nEpoch 19/50\n - 2s - loss: 80.3850 - val_loss: 65.3949\nEpoch 20/50\n - 0s - loss: 75.0257 - val_loss: 65.4152\nEpoch 21/50\n - 0s - loss: 71.3780 - val_loss: 66.6166\nEpoch 22/50\n - 3s - loss: 70.6271 - val_loss: 64.1524\nEpoch 23/50\n - 0s - loss: 69.8269 - val_loss: 65.6794\nEpoch 24/50\n - 0s - loss: 73.1825 - val_loss: 72.5481\nEpoch 25/50\n - 0s - loss: 75.2701 - val_loss: 66.6035\nEpoch 26/50\n - 0s - loss: 70.2607 - val_loss: 94.0800\nEpoch 27/50\n - 0s - loss: 72.1641 - val_loss: 64.7845\nEpoch 28/50\n - 0s - loss: 73.6620 - val_loss: 67.8238\nEpoch 29/50\n - 0s - loss: 75.5258 - val_loss: 72.1381\nEpoch 30/50\n - 0s - loss: 71.1692 - val_loss: 65.8502\nEpoch 31/50\n - 0s - loss: 70.7324 - val_loss: 64.5962\nEpoch 32/50\n - 0s - loss: 70.6672 - val_loss: 65.9894\nEpoch 33/50\n - 1s - loss: 71.2211 - val_loss: 65.3059\nEpoch 34/50\n - 3s - loss: 70.2521 - val_loss: 69.2816\nEpoch 35/50\n - 0s - loss: 69.3636 - val_loss: 63.7440\nEpoch 36/50\n - 0s - loss: 71.4661 - val_loss: 76.0037\nEpoch 37/50\n - 0s - loss: 69.6797 - val_loss: 69.6391\nEpoch 38/50\n - 0s - loss: 75.5300 - val_loss: 83.8773\nEpoch 39/50\n - 0s - loss: 74.2014 - val_loss: 79.8469\nEpoch 40/50\n - 0s - loss: 69.5561 - val_loss: 68.5590\nEpoch 41/50\n - 5s - loss: 70.6005 - val_loss: 61.7115\nEpoch 42/50\n - 0s - loss: 74.9154 - val_loss: 66.1750\nEpoch 43/50\n - 0s - loss: 70.0155 - val_loss: 62.0909\nEpoch 44/50\n - 0s - loss: 71.9147 - val_loss: 62.2091\nEpoch 45/50\n - 3s - loss: 68.6479 - val_loss: 74.2348\nEpoch 46/50\n - 0s - loss: 72.8841 - val_loss: 95.7880\nEpoch 47/50\n - 0s - loss: 73.6439 - val_loss: 96.0187\nEpoch 48/50\n - 3s - loss: 75.7350 - val_loss: 62.5253\nEpoch 49/50\n - 0s - loss: 72.9873 - val_loss: 66.9340\nEpoch 50/50\n - 0s - loss: 73.2719 - val_loss: 64.1492\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 69.8352 - val_loss: 64.9731\nEpoch 2/50\n - 0s - loss: 70.8320 - val_loss: 62.1326\nEpoch 3/50\n - 0s - loss: 69.9188 - val_loss: 72.4979\nEpoch 4/50\n - 0s - loss: 68.4123 - val_loss: 61.6899\nEpoch 5/50\n - 0s - loss: 68.6299 - val_loss: 64.2550\nEpoch 6/50\n - 0s - loss: 69.8103 - val_loss: 64.5902\nEpoch 7/50\n - 1s - loss: 83.2005 - val_loss: 68.8842\nEpoch 8/50\n - 0s - loss: 75.4139 - val_loss: 68.2855\nEpoch 9/50\n - 1s - loss: 68.3153 - val_loss: 79.3305\nEpoch 10/50\n - 0s - loss: 73.9155 - val_loss: 70.8159\nEpoch 11/50\n - 0s - loss: 67.7102 - val_loss: 62.8814\nEpoch 12/50\n - 5s - loss: 67.9203 - val_loss: 69.2781\nEpoch 13/50\n - 0s - loss: 69.1898 - val_loss: 62.4525\nEpoch 14/50\n - 0s - loss: 69.4062 - val_loss: 61.2228\nEpoch 15/50\n - 0s - loss: 69.8514 - val_loss: 66.0921\nEpoch 16/50\n - 3s - loss: 68.6417 - val_loss: 61.7101\nEpoch 17/50\n - 0s - loss: 68.5696 - val_loss: 66.8073\nEpoch 18/50\n - 0s - loss: 70.4597 - val_loss: 75.5685\nEpoch 19/50\n - 3s - loss: 68.3284 - val_loss: 62.8781\nEpoch 20/50\n - 0s - loss: 75.0014 - val_loss: 62.4059\nEpoch 21/50\n - 0s - loss: 69.7545 - val_loss: 62.4123\nEpoch 22/50\n - 0s - loss: 76.6326 - val_loss: 87.3808\nEpoch 23/50\n - 0s - loss: 72.0375 - val_loss: 73.5104\nEpoch 24/50\n - 0s - loss: 72.0168 - val_loss: 66.7698\nEpoch 25/50\n - 0s - loss: 70.1633 - val_loss: 60.9336\nEpoch 26/50\n - 0s - loss: 70.0208 - val_loss: 63.7498\nEpoch 27/50\n - 0s - loss: 71.5575 - val_loss: 68.2440\nEpoch 28/50\n - 0s - loss: 70.4252 - val_loss: 73.6156\nEpoch 29/50\n - 0s - loss: 69.4604 - val_loss: 75.0345\nEpoch 30/50\n - 0s - loss: 67.0403 - val_loss: 62.4679\nEpoch 31/50\n - 0s - loss: 67.5469 - val_loss: 60.9869\nEpoch 32/50\n - 0s - loss: 69.4577 - val_loss: 67.0911\nEpoch 33/50\n - 0s - loss: 67.7334 - val_loss: 65.7364\nEpoch 34/50\n - 1s - loss: 66.9768 - val_loss: 63.6237\nEpoch 35/50\n - 5s - loss: 68.3055 - val_loss: 75.7314\nEpoch 36/50\n - 0s - loss: 68.4716 - val_loss: 65.0829\nEpoch 37/50\n - 5s - loss: 71.5573 - val_loss: 66.2776\nEpoch 38/50\n - 0s - loss: 73.7868 - val_loss: 61.7813\nEpoch 39/50\n - 0s - loss: 67.3989 - val_loss: 60.0237\nEpoch 40/50\n - 0s - loss: 67.5042 - val_loss: 61.1462\nEpoch 41/50\n - 0s - loss: 67.4810 - val_loss: 60.7120\nEpoch 42/50\n - 0s - loss: 67.5235 - val_loss: 74.6251\nEpoch 43/50\n - 0s - loss: 67.6412 - val_loss: 59.8828\nEpoch 44/50\n - 0s - loss: 67.1935 - val_loss: 74.1576\nEpoch 45/50\n - 0s - loss: 67.0471 - val_loss: 81.4435\nEpoch 46/50\n - 0s - loss: 76.9788 - val_loss: 71.9293\nEpoch 47/50\n - 0s - loss: 73.9648 - val_loss: 59.4217\nEpoch 48/50\n - 1s - loss: 68.0619 - val_loss: 64.3820\nEpoch 49/50\n - 1s - loss: 67.4530 - val_loss: 70.0486\nEpoch 50/50\n - 0s - loss: 67.8279 - val_loss: 61.0155\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 1s - loss: 67.6536 - val_loss: 59.3362\nEpoch 2/50\n - 0s - loss: 66.7177 - val_loss: 76.1009\nEpoch 3/50\n - 0s - loss: 68.1221 - val_loss: 64.0952\nEpoch 4/50\n - 0s - loss: 68.0663 - val_loss: 69.1689\nEpoch 5/50\n - 0s - loss: 67.5986 - val_loss: 66.2309\nEpoch 6/50\n - 0s - loss: 67.7320 - val_loss: 70.1311\nEpoch 7/50\n - 5s - loss: 67.5745 - val_loss: 61.5691\nEpoch 8/50\n - 1s - loss: 66.6263 - val_loss: 64.6975\nEpoch 9/50\n - 0s - loss: 65.5247 - val_loss: 65.1407\nEpoch 10/50\n - 3s - loss: 69.7135 - val_loss: 92.2730\nEpoch 11/50\n - 0s - loss: 68.5235 - val_loss: 64.9738\nEpoch 12/50\n - 3s - loss: 66.0706 - val_loss: 65.3066\nEpoch 13/50\n - 0s - loss: 66.8015 - val_loss: 63.4315\nEpoch 14/50\n - 0s - loss: 67.3515 - val_loss: 62.8027\nEpoch 15/50\n - 0s - loss: 66.4400 - val_loss: 60.6740\nEpoch 16/50\n - 0s - loss: 67.7641 - val_loss: 61.2587\nEpoch 17/50\n - 0s - loss: 73.0553 - val_loss: 64.1728\nEpoch 18/50\n - 0s - loss: 68.3920 - val_loss: 59.8612\nEpoch 19/50\n - 0s - loss: 65.9063 - val_loss: 66.5951\nEpoch 20/50\n - 0s - loss: 67.6872 - val_loss: 80.6803\nEpoch 21/50\n - 0s - loss: 71.4206 - val_loss: 61.5066\nEpoch 22/50\n - 0s - loss: 66.9607 - val_loss: 65.0820\nEpoch 23/50\n - 0s - loss: 67.4898 - val_loss: 68.8713\nEpoch 24/50\n - 0s - loss: 67.2496 - val_loss: 66.4392\nEpoch 25/50\n - 0s - loss: 67.2376 - val_loss: 63.1414\nEpoch 26/50\n - 0s - loss: 66.2454 - val_loss: 62.3942\nEpoch 27/50\n - 0s - loss: 76.1666 - val_loss: 59.0712\nEpoch 28/50\n - 0s - loss: 65.3574 - val_loss: 75.3435\nEpoch 29/50\n - 5s - loss: 71.0526 - val_loss: 66.9814\nEpoch 30/50\n - 0s - loss: 70.6389 - val_loss: 63.1284\nEpoch 31/50\n - 0s - loss: 66.1198 - val_loss: 63.2963\nEpoch 32/50\n - 4s - loss: 71.8361 - val_loss: 60.7535\nEpoch 33/50\n - 0s - loss: 66.7178 - val_loss: 80.0061\nEpoch 34/50\n - 1s - loss: 69.0599 - val_loss: 87.9470\nEpoch 35/50\n - 0s - loss: 73.3896 - val_loss: 89.2461\nEpoch 36/50\n - 0s - loss: 70.8656 - val_loss: 60.9133\nEpoch 37/50\n - 0s - loss: 75.9796 - val_loss: 65.5380\nEpoch 38/50\n - 0s - loss: 77.5444 - val_loss: 62.2374\nEpoch 39/50\n - 0s - loss: 65.9963 - val_loss: 60.6178\nEpoch 40/50\n - 0s - loss: 66.7793 - val_loss: 59.3425\nEpoch 41/50\n - 0s - loss: 65.8465 - val_loss: 60.8429\nEpoch 42/50\n - 0s - loss: 66.7050 - val_loss: 65.6824\nEpoch 43/50\n - 0s - loss: 65.9186 - val_loss: 85.4410\nEpoch 44/50\n - 0s - loss: 70.8757 - val_loss: 60.1511\nEpoch 45/50\n - 0s - loss: 67.4078 - val_loss: 58.8594\nEpoch 46/50\n - 5s - loss: 67.9800 - val_loss: 58.4922\nEpoch 47/50\n - 0s - loss: 66.9617 - val_loss: 60.2129\nEpoch 48/50\n - 0s - loss: 72.5288 - val_loss: 63.5141\nEpoch 49/50\n - 0s - loss: 69.7671 - val_loss: 58.7006\nEpoch 50/50\n - 3s - loss: 69.5326 - val_loss: 59.9192\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 65.5944 - val_loss: 59.9364\nEpoch 2/50\n - 0s - loss: 66.9788 - val_loss: 90.4631\nEpoch 3/50\n - 0s - loss: 75.1525 - val_loss: 77.2364\nEpoch 4/50\n - 0s - loss: 66.9368 - val_loss: 61.7441\nEpoch 5/50\n - 3s - loss: 66.1986 - val_loss: 76.6201\nEpoch 6/50\n - 0s - loss: 66.0292 - val_loss: 60.5793\nEpoch 7/50\n - 0s - loss: 64.6808 - val_loss: 63.6228\nEpoch 8/50\n - 0s - loss: 65.6545 - val_loss: 61.9161\nEpoch 9/50\n - 0s - loss: 69.1866 - val_loss: 62.0138\nEpoch 10/50\n - 0s - loss: 72.8075 - val_loss: 61.7183\nEpoch 11/50\n - 1s - loss: 74.3166 - val_loss: 61.9399\nEpoch 12/50\n - 0s - loss: 69.0901 - val_loss: 94.5308\nEpoch 13/50\n - 5s - loss: 74.5904 - val_loss: 84.6724\nEpoch 14/50\n - 0s - loss: 79.9583 - val_loss: 57.5977\nEpoch 15/50\n - 0s - loss: 67.0026 - val_loss: 64.0406\nEpoch 16/50\n - 0s - loss: 67.0343 - val_loss: 63.0126\nEpoch 17/50\n - 5s - loss: 65.3899 - val_loss: 61.3750\nEpoch 18/50\n - 0s - loss: 65.8544 - val_loss: 61.1583\nEpoch 19/50\n - 0s - loss: 68.9702 - val_loss: 60.2266\nEpoch 20/50\n - 0s - loss: 70.1122 - val_loss: 61.2645\nEpoch 21/50\n - 0s - loss: 66.3106 - val_loss: 61.5472\nEpoch 22/50\n - 0s - loss: 65.3307 - val_loss: 73.9825\nEpoch 23/50\n - 0s - loss: 65.1762 - val_loss: 59.8677\nEpoch 24/50\n - 1s - loss: 67.4293 - val_loss: 60.7159\nEpoch 25/50\n - 0s - loss: 69.7670 - val_loss: 63.2690\nEpoch 26/50\n - 0s - loss: 66.2008 - val_loss: 60.8501\nEpoch 27/50\n - 0s - loss: 65.7432 - val_loss: 60.0226\nEpoch 28/50\n - 0s - loss: 70.1778 - val_loss: 60.1254\nEpoch 29/50\n - 0s - loss: 66.2565 - val_loss: 62.5390\nEpoch 30/50\n - 0s - loss: 65.7495 - val_loss: 59.5044\nEpoch 31/50\n - 1s - loss: 65.5811 - val_loss: 60.9698\nEpoch 32/50\n - 0s - loss: 65.5615 - val_loss: 63.0433\nEpoch 33/50\n - 0s - loss: 65.1965 - val_loss: 65.2948\nEpoch 34/50\n - 0s - loss: 65.5229 - val_loss: 74.1261\nEpoch 35/50\n - 0s - loss: 71.9980 - val_loss: 61.5799\nEpoch 36/50\n - 0s - loss: 75.4218 - val_loss: 61.4451\nEpoch 37/50\n - 0s - loss: 67.6489 - val_loss: 58.2311\nEpoch 38/50\n - 5s - loss: 65.3025 - val_loss: 59.1559\nEpoch 39/50\n - 0s - loss: 64.6697 - val_loss: 61.7650\nEpoch 40/50\n - 5s - loss: 64.9323 - val_loss: 62.5950\nEpoch 41/50\n - 0s - loss: 64.7675 - val_loss: 61.8034\nEpoch 42/50\n - 0s - loss: 66.5516 - val_loss: 59.1366\nEpoch 43/50\n - 0s - loss: 65.7243 - val_loss: 61.6850\nEpoch 44/50\n - 0s - loss: 66.3724 - val_loss: 70.7950\nEpoch 45/50\n - 0s - loss: 70.4078 - val_loss: 74.3050\nEpoch 46/50\n - 0s - loss: 65.2277 - val_loss: 62.8540\nEpoch 47/50\n - 0s - loss: 65.9700 - val_loss: 63.7630\nEpoch 48/50\n - 0s - loss: 67.1378 - val_loss: 60.5263\nEpoch 49/50\n - 0s - loss: 71.5473 - val_loss: 70.4179\nEpoch 50/50\n - 0s - loss: 72.2756 - val_loss: 71.9211\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 66.9278 - val_loss: 67.8911\nEpoch 2/50\n - 0s - loss: 66.3964 - val_loss: 74.0390\nEpoch 3/50\n - 0s - loss: 65.9403 - val_loss: 60.7470\nEpoch 4/50\n - 0s - loss: 65.5254 - val_loss: 73.0756\nEpoch 5/50\n - 0s - loss: 69.7021 - val_loss: 80.9572\nEpoch 6/50\n - 0s - loss: 67.0258 - val_loss: 60.7026\nEpoch 7/50\n - 0s - loss: 64.7956 - val_loss: 59.5145\nEpoch 8/50\n - 5s - loss: 65.5757 - val_loss: 60.1929\nEpoch 9/50\n - 0s - loss: 64.8741 - val_loss: 60.0649\nEpoch 10/50\n - 4s - loss: 66.1720 - val_loss: 64.4523\nEpoch 11/50\n - 0s - loss: 65.4139 - val_loss: 67.0934\nEpoch 12/50\n - 0s - loss: 70.9366 - val_loss: 76.1719\nEpoch 13/50\n - 0s - loss: 70.6957 - val_loss: 63.3716\nEpoch 14/50\n - 0s - loss: 69.3493 - val_loss: 61.6977\nEpoch 15/50\n - 0s - loss: 72.1318 - val_loss: 69.8282\nEpoch 16/50\n - 0s - loss: 76.6229 - val_loss: 65.8247\nEpoch 17/50\n - 0s - loss: 65.7935 - val_loss: 59.4906\nEpoch 18/50\n - 0s - loss: 66.1472 - val_loss: 69.4564\nEpoch 19/50\n - 0s - loss: 65.0009 - val_loss: 64.3989\nEpoch 20/50\n - 0s - loss: 64.1494 - val_loss: 65.1394\nEpoch 21/50\n - 0s - loss: 64.9311 - val_loss: 60.4271\nEpoch 22/50\n - 0s - loss: 67.1624 - val_loss: 62.3015\nEpoch 23/50\n - 0s - loss: 66.9640 - val_loss: 60.5367\nEpoch 24/50\n - 0s - loss: 64.6720 - val_loss: 60.0376\nEpoch 25/50\n - 0s - loss: 73.7977 - val_loss: 61.1563\nEpoch 26/50\n - 0s - loss: 68.2118 - val_loss: 63.3294\nEpoch 27/50\n - 0s - loss: 63.6020 - val_loss: 66.9716\nEpoch 28/50\n - 0s - loss: 68.5340 - val_loss: 73.0147\nEpoch 29/50\n - 0s - loss: 69.1197 - val_loss: 81.1525\nEpoch 30/50\n - 0s - loss: 64.7506 - val_loss: 62.8257\nEpoch 31/50\n - 0s - loss: 64.3279 - val_loss: 65.0627\nEpoch 32/50\n - 0s - loss: 66.0729 - val_loss: 57.6409\nEpoch 33/50\n - 0s - loss: 66.7675 - val_loss: 58.7039\nEpoch 34/50\n - 0s - loss: 64.5010 - val_loss: 59.3151\nEpoch 35/50\n - 0s - loss: 66.3128 - val_loss: 62.0006\nEpoch 36/50\n - 0s - loss: 64.3002 - val_loss: 74.0779\nEpoch 37/50\n - 5s - loss: 66.4753 - val_loss: 79.1461\nEpoch 38/50\n - 0s - loss: 65.1584 - val_loss: 58.5896\nEpoch 39/50\n - 0s - loss: 66.1038 - val_loss: 59.2519\nEpoch 40/50\n - 3s - loss: 64.6724 - val_loss: 61.9387\nEpoch 41/50\n - 0s - loss: 66.7583 - val_loss: 61.1987\nEpoch 42/50\n - 0s - loss: 64.3617 - val_loss: 71.4532\nEpoch 43/50\n - 1s - loss: 64.0258 - val_loss: 66.6815\nEpoch 44/50\n - 0s - loss: 64.7231 - val_loss: 60.5579\nEpoch 45/50\n - 0s - loss: 64.3699 - val_loss: 71.1036\nEpoch 46/50\n - 0s - loss: 64.4443 - val_loss: 80.8076\nEpoch 47/50\n - 0s - loss: 67.7068 - val_loss: 64.3923\nEpoch 48/50\n - 0s - loss: 67.0298 - val_loss: 64.5317\nEpoch 49/50\n - 0s - loss: 64.0498 - val_loss: 59.0412\nEpoch 50/50\n - 0s - loss: 64.9349 - val_loss: 70.5269\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 63.4047 - val_loss: 61.4345\nEpoch 2/50\n - 1s - loss: 67.3387 - val_loss: 60.0886\nEpoch 3/50\n - 0s - loss: 63.2711 - val_loss: 59.8447\nEpoch 4/50\n - 0s - loss: 64.1866 - val_loss: 60.2305\nEpoch 5/50\n - 0s - loss: 63.7840 - val_loss: 63.0504\nEpoch 6/50\n - 0s - loss: 61.3254 - val_loss: 75.3188\nEpoch 7/50\n - 0s - loss: 63.6949 - val_loss: 73.7690\nEpoch 8/50\n - 0s - loss: 66.8984 - val_loss: 60.5688\nEpoch 9/50\n - 0s - loss: 63.2865 - val_loss: 64.5881\nEpoch 10/50\n - 0s - loss: 67.1964 - val_loss: 68.2703\nEpoch 11/50\n - 0s - loss: 61.4264 - val_loss: 70.7866\nEpoch 12/50\n - 10s - loss: 61.3474 - val_loss: 58.0575\nEpoch 13/50\n - 0s - loss: 60.7241 - val_loss: 65.3632\nEpoch 14/50\n - 0s - loss: 59.5711 - val_loss: 64.1169\nEpoch 15/50\n - 0s - loss: 66.1126 - val_loss: 59.1060\nEpoch 16/50\n - 0s - loss: 62.0564 - val_loss: 60.5064\nEpoch 17/50\n - 0s - loss: 62.9134 - val_loss: 64.4153\nEpoch 18/50\n - 0s - loss: 58.1796 - val_loss: 57.7357\nEpoch 19/50\n - 0s - loss: 62.9569 - val_loss: 62.5711\nEpoch 20/50\n - 1s - loss: 61.9917 - val_loss: 66.8600\nEpoch 21/50\n - 0s - loss: 59.5899 - val_loss: 59.7682\nEpoch 22/50\n - 0s - loss: 60.4280 - val_loss: 65.2204\nEpoch 23/50\n - 0s - loss: 67.7089 - val_loss: 63.3740\nEpoch 24/50\n - 0s - loss: 71.5386 - val_loss: 57.9513\nEpoch 25/50\n - 0s - loss: 61.4886 - val_loss: 65.3901\nEpoch 26/50\n - 0s - loss: 59.5889 - val_loss: 68.4196\nEpoch 27/50\n - 0s - loss: 58.2810 - val_loss: 63.7608\nEpoch 28/50\n - 1s - loss: 59.8254 - val_loss: 58.7115\nEpoch 29/50\n - 5s - loss: 58.7564 - val_loss: 57.2004\nEpoch 30/50\n - 0s - loss: 63.2099 - val_loss: 61.7025\nEpoch 31/50\n - 0s - loss: 62.3007 - val_loss: 71.3393\nEpoch 32/50\n - 0s - loss: 60.9610 - val_loss: 68.6669\nEpoch 33/50\n - 4s - loss: 57.0492 - val_loss: 65.7655\nEpoch 34/50\n - 0s - loss: 58.6299 - val_loss: 82.7744\nEpoch 35/50\n - 0s - loss: 60.4436 - val_loss: 98.9945\nEpoch 36/50\n - 0s - loss: 60.0326 - val_loss: 87.2637\nEpoch 37/50\n - 0s - loss: 58.6331 - val_loss: 77.5516\nEpoch 38/50\n - 0s - loss: 56.9208 - val_loss: 68.2810\nEpoch 39/50\n - 0s - loss: 56.5705 - val_loss: 72.3403\nEpoch 40/50\n - 0s - loss: 55.6760 - val_loss: 57.1910\nEpoch 41/50\n - 0s - loss: 57.5827 - val_loss: 74.1026\nEpoch 42/50\n - 0s - loss: 56.9163 - val_loss: 74.9034\nEpoch 43/50\n - 0s - loss: 57.3563 - val_loss: 82.9701\nEpoch 44/50\n - 0s - loss: 56.5177 - val_loss: 63.9156\nEpoch 45/50\n - 0s - loss: 56.3699 - val_loss: 72.2696\nEpoch 46/50\n - 0s - loss: 60.1648 - val_loss: 62.6482\nEpoch 47/50\n - 0s - loss: 57.9367 - val_loss: 60.9123\nEpoch 48/50\n - 0s - loss: 55.1367 - val_loss: 70.7845\nEpoch 49/50\n - 1s - loss: 58.7787 - val_loss: 70.2024\nEpoch 50/50\n - 0s - loss: 56.4150 - val_loss: 75.2706\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 55.3998 - val_loss: 59.8952\nEpoch 2/50\n - 0s - loss: 59.1546 - val_loss: 60.5169\nEpoch 3/50\n - 0s - loss: 57.3721 - val_loss: 68.8216\nEpoch 4/50\n - 0s - loss: 58.6909 - val_loss: 60.3561\nEpoch 5/50\n - 0s - loss: 56.1002 - val_loss: 62.0900\nEpoch 6/50\n - 0s - loss: 54.3161 - val_loss: 62.5256\nEpoch 7/50\n - 7s - loss: 54.6197 - val_loss: 67.9900\nEpoch 8/50\n - 0s - loss: 55.5426 - val_loss: 88.8761\nEpoch 9/50\n - 3s - loss: 58.1133 - val_loss: 59.3366\nEpoch 10/50\n - 0s - loss: 60.2061 - val_loss: 71.4307\nEpoch 11/50\n - 0s - loss: 56.8047 - val_loss: 71.5036\nEpoch 12/50\n - 0s - loss: 60.5057 - val_loss: 59.6002\nEpoch 13/50\n - 0s - loss: 57.0860 - val_loss: 71.8423\nEpoch 14/50\n - 0s - loss: 55.8046 - val_loss: 105.8874\nEpoch 15/50\n - 1s - loss: 58.0595 - val_loss: 63.0607\nEpoch 16/50\n - 0s - loss: 54.2929 - val_loss: 84.4212\nEpoch 17/50\n - 0s - loss: 60.5074 - val_loss: 67.1084\nEpoch 18/50\n - 0s - loss: 54.4578 - val_loss: 63.3520\nEpoch 19/50\n - 0s - loss: 56.7852 - val_loss: 92.7136\nEpoch 20/50\n - 0s - loss: 57.1804 - val_loss: 61.8271\nEpoch 21/50\n - 1s - loss: 57.6753 - val_loss: 66.0118\nEpoch 22/50\n - 0s - loss: 53.3146 - val_loss: 75.2651\nEpoch 23/50\n - 0s - loss: 53.8847 - val_loss: 75.9672\nEpoch 24/50\n - 0s - loss: 54.5558 - val_loss: 56.8728\nEpoch 25/50\n - 0s - loss: 53.3080 - val_loss: 64.5089\nEpoch 26/50\n - 0s - loss: 53.7825 - val_loss: 69.7895\nEpoch 27/50\n - 0s - loss: 58.3180 - val_loss: 55.8527\nEpoch 28/50\n - 0s - loss: 53.0279 - val_loss: 81.9486\nEpoch 29/50\n - 0s - loss: 55.6856 - val_loss: 63.6554\nEpoch 30/50\n - 0s - loss: 57.5158 - val_loss: 59.1879\nEpoch 31/50\n - 0s - loss: 55.6731 - val_loss: 61.2762\nEpoch 32/50\n - 0s - loss: 53.0612 - val_loss: 60.5838\nEpoch 33/50\n - 8s - loss: 54.2914 - val_loss: 72.1870\nEpoch 34/50\n - 0s - loss: 52.3421 - val_loss: 61.7773\nEpoch 35/50\n - 1s - loss: 52.4052 - val_loss: 63.5653\nEpoch 36/50\n - 1s - loss: 53.6086 - val_loss: 96.1832\nEpoch 37/50\n - 0s - loss: 57.3784 - val_loss: 64.4496\nEpoch 38/50\n - 0s - loss: 53.2303 - val_loss: 61.3651\nEpoch 39/50\n - 0s - loss: 52.7151 - val_loss: 58.6305\nEpoch 40/50\n - 0s - loss: 53.3178 - val_loss: 64.5384\nEpoch 41/50\n - 0s - loss: 55.8125 - val_loss: 62.9273\nEpoch 42/50\n - 1s - loss: 51.6745 - val_loss: 69.8461\nEpoch 43/50\n - 1s - loss: 51.6790 - val_loss: 63.3414\nEpoch 44/50\n - 0s - loss: 53.5826 - val_loss: 77.4948\nEpoch 45/50\n - 0s - loss: 52.3075 - val_loss: 58.6570\nEpoch 46/50\n - 0s - loss: 52.1080 - val_loss: 72.2544\nEpoch 47/50\n - 0s - loss: 51.3117 - val_loss: 60.6008\nEpoch 48/50\n - 0s - loss: 57.6150 - val_loss: 104.3642\nEpoch 49/50\n - 0s - loss: 57.7557 - val_loss: 95.0934\nEpoch 50/50\n - 0s - loss: 56.3728 - val_loss: 69.4987\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 53.2267 - val_loss: 72.2235\nEpoch 2/50\n - 1s - loss: 53.2518 - val_loss: 74.0947\nEpoch 3/50\n - 0s - loss: 59.0086 - val_loss: 60.2982\nEpoch 4/50\n - 0s - loss: 59.3696 - val_loss: 60.0546\nEpoch 5/50\n - 0s - loss: 51.1676 - val_loss: 69.4409\nEpoch 6/50\n - 10s - loss: 51.4637 - val_loss: 62.1733\nEpoch 7/50\n - 0s - loss: 53.3037 - val_loss: 65.8610\nEpoch 8/50\n - 0s - loss: 51.4021 - val_loss: 72.4457\nEpoch 9/50\n - 0s - loss: 51.0887 - val_loss: 72.3493\nEpoch 10/50\n - 1s - loss: 51.9018 - val_loss: 73.7385\nEpoch 11/50\n - 0s - loss: 50.5588 - val_loss: 66.6140\nEpoch 12/50\n - 0s - loss: 50.1354 - val_loss: 60.5895\nEpoch 13/50\n - 0s - loss: 55.2019 - val_loss: 55.0754\nEpoch 14/50\n - 0s - loss: 53.3192 - val_loss: 74.8235\nEpoch 15/50\n - 0s - loss: 52.1435 - val_loss: 61.5411\nEpoch 16/50\n - 0s - loss: 50.0174 - val_loss: 67.2666\nEpoch 17/50\n - 0s - loss: 50.7412 - val_loss: 69.9442\nEpoch 18/50\n - 0s - loss: 50.1234 - val_loss: 61.4690\nEpoch 19/50\n - 0s - loss: 52.2086 - val_loss: 75.5125\nEpoch 20/50\n - 0s - loss: 51.6484 - val_loss: 71.2642\nEpoch 21/50\n - 0s - loss: 52.6798 - val_loss: 70.5429\nEpoch 22/50\n - 0s - loss: 52.7167 - val_loss: 74.4339\nEpoch 23/50\n - 0s - loss: 49.9663 - val_loss: 72.3365\nEpoch 24/50\n - 0s - loss: 50.6009 - val_loss: 67.2554\nEpoch 25/50\n - 0s - loss: 51.7255 - val_loss: 97.7668\nEpoch 26/50\n - 0s - loss: 52.4600 - val_loss: 85.6705\nEpoch 27/50\n - 0s - loss: 51.3335 - val_loss: 56.9872\nEpoch 28/50\n - 0s - loss: 52.0896 - val_loss: 77.2621\nEpoch 29/50\n - 0s - loss: 50.1861 - val_loss: 71.8099\nEpoch 30/50\n - 0s - loss: 51.3023 - val_loss: 73.3730\nEpoch 31/50\n - 0s - loss: 57.6054 - val_loss: 95.9593\nEpoch 32/50\n - 0s - loss: 51.6410 - val_loss: 73.0493\nEpoch 33/50\n - 5s - loss: 48.6898 - val_loss: 67.4814\nEpoch 34/50\n - 0s - loss: 49.1843 - val_loss: 69.7523\nEpoch 35/50\n - 0s - loss: 49.8285 - val_loss: 73.2037\nEpoch 36/50\n - 0s - loss: 51.4769 - val_loss: 73.8281\nEpoch 37/50\n - 5s - loss: 51.1924 - val_loss: 65.9748\nEpoch 38/50\n - 0s - loss: 50.1465 - val_loss: 68.9938\nEpoch 39/50\n - 0s - loss: 56.4561 - val_loss: 112.7778\nEpoch 40/50\n - 0s - loss: 51.4955 - val_loss: 74.1881\nEpoch 41/50\n - 0s - loss: 50.7121 - val_loss: 59.5833\nEpoch 42/50\n - 0s - loss: 54.8758 - val_loss: 66.4349\nEpoch 43/50\n - 0s - loss: 50.6565 - val_loss: 61.1507\nEpoch 44/50\n - 0s - loss: 54.7648 - val_loss: 89.4357\nEpoch 45/50\n - 0s - loss: 50.9032 - val_loss: 97.6382\nEpoch 46/50\n - 0s - loss: 50.8200 - val_loss: 75.1024\nEpoch 47/50\n - 0s - loss: 52.6243 - val_loss: 68.5584\nEpoch 48/50\n - 5s - loss: 49.4555 - val_loss: 61.9009\nEpoch 49/50\n - 0s - loss: 49.0952 - val_loss: 74.3530\nEpoch 50/50\n - 0s - loss: 48.7809 - val_loss: 78.8456\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 55.9460 - val_loss: 91.2876\nEpoch 2/50\n - 4s - loss: 49.0577 - val_loss: 67.7655\nEpoch 3/50\n - 0s - loss: 48.8643 - val_loss: 64.1160\nEpoch 4/50\n - 0s - loss: 49.7150 - val_loss: 75.1283\nEpoch 5/50\n - 0s - loss: 51.5327 - val_loss: 64.1196\nEpoch 6/50\n - 1s - loss: 52.1600 - val_loss: 63.9234\nEpoch 7/50\n - 0s - loss: 53.7447 - val_loss: 101.3850\nEpoch 8/50\n - 1s - loss: 49.2274 - val_loss: 93.9858\nEpoch 9/50\n - 0s - loss: 49.6018 - val_loss: 68.2944\nEpoch 10/50\n - 0s - loss: 48.5701 - val_loss: 74.8734\nEpoch 11/50\n - 0s - loss: 49.3976 - val_loss: 80.4568\nEpoch 12/50\n - 0s - loss: 51.3091 - val_loss: 63.9109\nEpoch 13/50\n - 0s - loss: 49.9890 - val_loss: 87.0776\nEpoch 14/50\n - 0s - loss: 48.9640 - val_loss: 65.8129\nEpoch 15/50\n - 0s - loss: 49.9746 - val_loss: 59.6136\nEpoch 16/50\n - 0s - loss: 53.0788 - val_loss: 60.7899\nEpoch 17/50\n - 0s - loss: 53.0487 - val_loss: 88.8416\nEpoch 18/50\n - 0s - loss: 51.8443 - val_loss: 77.0681\nEpoch 19/50\n - 0s - loss: 50.5410 - val_loss: 77.6121\nEpoch 20/50\n - 1s - loss: 51.1302 - val_loss: 69.9562\nEpoch 21/50\n - 0s - loss: 52.5088 - val_loss: 100.9467\nEpoch 22/50\n - 0s - loss: 52.7478 - val_loss: 70.7238\nEpoch 23/50\n - 0s - loss: 52.8013 - val_loss: 71.9979\nEpoch 24/50\n - 10s - loss: 49.6426 - val_loss: 88.2338\nEpoch 25/50\n - 0s - loss: 48.8898 - val_loss: 101.4094\nEpoch 26/50\n - 0s - loss: 53.7760 - val_loss: 62.2437\nEpoch 27/50\n - 0s - loss: 56.8459 - val_loss: 69.8883\nEpoch 28/50\n - 0s - loss: 54.5319 - val_loss: 70.6334\nEpoch 29/50\n - 0s - loss: 49.9056 - val_loss: 88.3162\nEpoch 30/50\n - 0s - loss: 52.3122 - val_loss: 80.5527\nEpoch 31/50\n - 0s - loss: 49.9410 - val_loss: 69.3334\nEpoch 32/50\n - 0s - loss: 49.7707 - val_loss: 84.2319\nEpoch 33/50\n - 0s - loss: 47.7583 - val_loss: 67.1017\nEpoch 34/50\n - 1s - loss: 50.8330 - val_loss: 76.3943\nEpoch 35/50\n - 1s - loss: 55.3731 - val_loss: 114.2740\nEpoch 36/50\n - 0s - loss: 53.9732 - val_loss: 78.8035\nEpoch 37/50\n - 0s - loss: 51.0680 - val_loss: 72.0574\nEpoch 38/50\n - 0s - loss: 48.4233 - val_loss: 95.3061\nEpoch 39/50\n - 0s - loss: 49.7233 - val_loss: 72.0796\nEpoch 40/50\n - 0s - loss: 49.8814 - val_loss: 76.4213\nEpoch 41/50\n - 0s - loss: 47.2822 - val_loss: 96.5285\nEpoch 42/50\n - 0s - loss: 48.1988 - val_loss: 113.3004\nEpoch 43/50\n - 7s - loss: 56.7000 - val_loss: 68.3115\nEpoch 44/50\n - 0s - loss: 53.3101 - val_loss: 67.9432\nEpoch 45/50\n - 0s - loss: 49.4800 - val_loss: 96.7017\nEpoch 46/50\n - 3s - loss: 52.7586 - val_loss: 92.9220\nEpoch 47/50\n - 0s - loss: 49.0874 - val_loss: 89.2568\nEpoch 48/50\n - 0s - loss: 48.9000 - val_loss: 71.4549\nEpoch 49/50\n - 0s - loss: 53.9593 - val_loss: 83.6387\nEpoch 50/50\n - 0s - loss: 47.7876 - val_loss: 88.0123\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 3s - loss: 46.8998 - val_loss: 78.5897\nEpoch 2/50\n - 0s - loss: 48.4120 - val_loss: 92.4605\nEpoch 3/50\n - 0s - loss: 48.3206 - val_loss: 93.1510\nEpoch 4/50\n - 0s - loss: 50.4980 - val_loss: 87.0094\nEpoch 5/50\n - 0s - loss: 48.8684 - val_loss: 81.2309\nEpoch 6/50\n - 0s - loss: 47.1315 - val_loss: 78.4528\nEpoch 7/50\n - 0s - loss: 49.8789 - val_loss: 129.6342\nEpoch 8/50\n - 0s - loss: 53.0604 - val_loss: 92.4312\nEpoch 9/50\n - 0s - loss: 49.5244 - val_loss: 71.5867\nEpoch 10/50\n - 0s - loss: 48.7560 - val_loss: 105.9469\nEpoch 11/50\n - 0s - loss: 49.6815 - val_loss: 94.9315\nEpoch 12/50\n - 0s - loss: 47.2807 - val_loss: 76.8966\nEpoch 13/50\n - 0s - loss: 47.3278 - val_loss: 67.4487\nEpoch 14/50\n - 0s - loss: 52.5203 - val_loss: 77.6414\nEpoch 15/50\n - 0s - loss: 48.3071 - val_loss: 79.4910\nEpoch 16/50\n - 5s - loss: 48.8166 - val_loss: 93.5148\nEpoch 17/50\n - 5s - loss: 47.6494 - val_loss: 78.7242\nEpoch 18/50\n - 1s - loss: 47.6371 - val_loss: 90.7290\nEpoch 19/50\n - 0s - loss: 51.3338 - val_loss: 75.6008\nEpoch 20/50\n - 1s - loss: 48.6242 - val_loss: 67.4693\nEpoch 21/50\n - 0s - loss: 48.1111 - val_loss: 75.0351\nEpoch 22/50\n - 1s - loss: 48.9864 - val_loss: 95.3080\nEpoch 23/50\n - 0s - loss: 49.2507 - val_loss: 88.1535\nEpoch 24/50\n - 0s - loss: 49.9621 - val_loss: 96.0891\nEpoch 25/50\n - 0s - loss: 49.3877 - val_loss: 74.1119\nEpoch 26/50\n - 0s - loss: 49.6263 - val_loss: 81.8191\nEpoch 27/50\n - 1s - loss: 48.8821 - val_loss: 76.1044\nEpoch 28/50\n - 0s - loss: 51.8148 - val_loss: 95.4880\nEpoch 29/50\n - 1s - loss: 50.4004 - val_loss: 66.7359\nEpoch 30/50\n - 0s - loss: 50.5910 - val_loss: 85.2067\nEpoch 31/50\n - 1s - loss: 48.5474 - val_loss: 102.4253\nEpoch 32/50\n - 6s - loss: 53.1619 - val_loss: 74.9267\nEpoch 33/50\n - 0s - loss: 49.8595 - val_loss: 85.9980\nEpoch 34/50\n - 1s - loss: 47.0624 - val_loss: 102.1877\nEpoch 35/50\n - 3s - loss: 48.7911 - val_loss: 69.8497\nEpoch 36/50\n - 0s - loss: 47.9164 - val_loss: 82.3505\nEpoch 37/50\n - 0s - loss: 47.2092 - val_loss: 106.1203\nEpoch 38/50\n - 0s - loss: 50.2656 - val_loss: 69.7972\nEpoch 39/50\n - 0s - loss: 52.8273 - val_loss: 82.1417\nEpoch 40/50\n - 0s - loss: 47.9038 - val_loss: 87.7761\nEpoch 41/50\n - 0s - loss: 47.7005 - val_loss: 93.7750\nEpoch 42/50\n - 0s - loss: 49.7938 - val_loss: 105.8969\nEpoch 43/50\n - 0s - loss: 50.6076 - val_loss: 79.4954\nEpoch 44/50\n - 0s - loss: 48.2783 - val_loss: 76.0431\nEpoch 45/50\n - 0s - loss: 49.5713 - val_loss: 79.0290\nEpoch 46/50\n - 0s - loss: 46.7715 - val_loss: 83.7569\nEpoch 47/50\n - 0s - loss: 47.8867 - val_loss: 77.7803\nEpoch 48/50\n - 0s - loss: 49.0045 - val_loss: 77.6317\nEpoch 49/50\n - 0s - loss: 47.9897 - val_loss: 89.1554\nEpoch 50/50\n - 0s - loss: 51.1329 - val_loss: 98.7873\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 50.3592 - val_loss: 71.6557\nEpoch 2/50\n - 0s - loss: 46.8731 - val_loss: 88.7094\nEpoch 3/50\n - 0s - loss: 48.7192 - val_loss: 91.8644\nEpoch 4/50\n - 0s - loss: 47.7172 - val_loss: 80.7383\nEpoch 5/50\n - 0s - loss: 47.7166 - val_loss: 76.9821\nEpoch 6/50\n - 0s - loss: 48.0419 - val_loss: 92.7028\nEpoch 7/50\n - 0s - loss: 50.1129 - val_loss: 84.9815\nEpoch 8/50\n - 7s - loss: 51.6667 - val_loss: 73.7853\nEpoch 9/50\n - 4s - loss: 54.1461 - val_loss: 125.7021\nEpoch 10/50\n - 0s - loss: 52.4352 - val_loss: 75.0531\nEpoch 11/50\n - 0s - loss: 50.9361 - val_loss: 69.2396\nEpoch 12/50\n - 0s - loss: 49.2950 - val_loss: 77.4681\nEpoch 13/50\n - 0s - loss: 47.1911 - val_loss: 89.0372\nEpoch 14/50\n - 0s - loss: 46.5128 - val_loss: 73.7905\nEpoch 15/50\n - 0s - loss: 49.5267 - val_loss: 79.6352\nEpoch 16/50\n - 0s - loss: 52.2363 - val_loss: 79.3056\nEpoch 17/50\n - 0s - loss: 48.9033 - val_loss: 90.4384\nEpoch 18/50\n - 0s - loss: 47.7411 - val_loss: 75.3593\nEpoch 19/50\n - 0s - loss: 48.5929 - val_loss: 90.5371\nEpoch 20/50\n - 0s - loss: 50.4864 - val_loss: 80.9501\nEpoch 21/50\n - 0s - loss: 48.6002 - val_loss: 87.0322\nEpoch 22/50\n - 0s - loss: 47.4828 - val_loss: 69.7789\nEpoch 23/50\n - 0s - loss: 50.9639 - val_loss: 78.5155\nEpoch 24/50\n - 0s - loss: 48.1987 - val_loss: 84.4707\nEpoch 25/50\n - 0s - loss: 48.1545 - val_loss: 72.9767\nEpoch 26/50\n - 0s - loss: 49.2647 - val_loss: 67.4814\nEpoch 27/50\n - 0s - loss: 48.7494 - val_loss: 89.4961\nEpoch 28/50\n - 0s - loss: 49.6283 - val_loss: 83.7729\nEpoch 29/50\n - 0s - loss: 47.6133 - val_loss: 103.5925\nEpoch 30/50\n - 0s - loss: 47.7516 - val_loss: 71.7503\nEpoch 31/50\n - 0s - loss: 50.8898 - val_loss: 127.3484\nEpoch 32/50\n - 0s - loss: 48.8571 - val_loss: 105.1975\nEpoch 33/50\n - 0s - loss: 48.6459 - val_loss: 72.2673\nEpoch 34/50\n - 0s - loss: 47.7660 - val_loss: 77.8276\nEpoch 35/50\n - 0s - loss: 49.1108 - val_loss: 78.2852\nEpoch 36/50\n - 5s - loss: 48.4837 - val_loss: 86.9681\nEpoch 37/50\n - 0s - loss: 46.8035 - val_loss: 94.9429\nEpoch 38/50\n - 0s - loss: 49.3335 - val_loss: 99.2595\nEpoch 39/50\n - 3s - loss: 47.8508 - val_loss: 92.1042\nEpoch 40/50\n - 0s - loss: 49.5425 - val_loss: 88.6621\nEpoch 41/50\n - 1s - loss: 47.2348 - val_loss: 90.3658\nEpoch 42/50\n - 0s - loss: 48.9130 - val_loss: 83.2692\nEpoch 43/50\n - 1s - loss: 46.8904 - val_loss: 71.1104\nEpoch 44/50\n - 0s - loss: 48.8952 - val_loss: 77.9453\nEpoch 45/50\n - 0s - loss: 49.4876 - val_loss: 90.3289\nEpoch 46/50\n - 0s - loss: 47.8129 - val_loss: 88.2312\nEpoch 47/50\n - 0s - loss: 49.0578 - val_loss: 97.6175\nEpoch 48/50\n - 0s - loss: 48.8880 - val_loss: 73.7464\nEpoch 49/50\n - 0s - loss: 46.8109 - val_loss: 92.0650\nEpoch 50/50\n - 0s - loss: 49.3569 - val_loss: 79.6602\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 48.8439 - val_loss: 90.5477\nEpoch 2/50\n - 0s - loss: 48.6388 - val_loss: 88.6477\nEpoch 3/50\n - 0s - loss: 46.5394 - val_loss: 76.6571\nEpoch 4/50\n - 0s - loss: 48.2649 - val_loss: 79.4160\nEpoch 5/50\n - 0s - loss: 49.1893 - val_loss: 89.4425\nEpoch 6/50\n - 0s - loss: 47.6913 - val_loss: 67.1749\nEpoch 7/50\n - 0s - loss: 50.8023 - val_loss: 120.4208\nEpoch 8/50\n - 0s - loss: 50.2058 - val_loss: 78.2214\nEpoch 9/50\n - 0s - loss: 51.6526 - val_loss: 110.1702\nEpoch 10/50\n - 1s - loss: 47.6120 - val_loss: 82.7129\nEpoch 11/50\n - 0s - loss: 47.4513 - val_loss: 78.6228\nEpoch 12/50\n - 0s - loss: 47.9443 - val_loss: 69.6023\nEpoch 13/50\n - 5s - loss: 49.8540 - val_loss: 88.6801\nEpoch 14/50\n - 0s - loss: 48.3092 - val_loss: 114.9380\nEpoch 15/50\n - 0s - loss: 53.8850 - val_loss: 79.7758\nEpoch 16/50\n - 3s - loss: 50.3248 - val_loss: 80.9606\nEpoch 17/50\n - 0s - loss: 48.9987 - val_loss: 77.4519\nEpoch 18/50\n - 0s - loss: 51.4627 - val_loss: 83.2773\nEpoch 19/50\n - 0s - loss: 48.1867 - val_loss: 72.9685\nEpoch 20/50\n - 0s - loss: 49.6197 - val_loss: 72.6369\nEpoch 21/50\n - 0s - loss: 48.1660 - val_loss: 98.6609\nEpoch 22/50\n - 0s - loss: 48.1468 - val_loss: 100.0292\nEpoch 23/50\n - 0s - loss: 48.9247 - val_loss: 96.5723\nEpoch 24/50\n - 1s - loss: 53.0287 - val_loss: 73.8445\nEpoch 25/50\n - 0s - loss: 47.8221 - val_loss: 83.3381\nEpoch 26/50\n - 0s - loss: 46.9192 - val_loss: 102.5410\nEpoch 27/50\n - 0s - loss: 48.4713 - val_loss: 70.5697\nEpoch 28/50\n - 0s - loss: 48.2216 - val_loss: 103.1361\nEpoch 29/50\n - 0s - loss: 47.9333 - val_loss: 80.7070\nEpoch 30/50\n - 0s - loss: 53.9427 - val_loss: 98.1335\nEpoch 31/50\n - 0s - loss: 47.1452 - val_loss: 75.9241\nEpoch 32/50\n - 0s - loss: 49.1056 - val_loss: 76.1921\nEpoch 33/50\n - 0s - loss: 47.2087 - val_loss: 82.6782\nEpoch 34/50\n - 0s - loss: 47.8147 - val_loss: 86.5241\nEpoch 35/50\n - 0s - loss: 49.5210 - val_loss: 68.3058\nEpoch 36/50\n - 0s - loss: 48.1776 - val_loss: 90.2232\nEpoch 37/50\n - 0s - loss: 47.4603 - val_loss: 83.1980\nEpoch 38/50\n - 0s - loss: 49.5051 - val_loss: 110.8097\nEpoch 39/50\n - 0s - loss: 52.2441 - val_loss: 93.1142\nEpoch 40/50\n - 0s - loss: 49.2845 - val_loss: 71.1598\nEpoch 41/50\n - 10s - loss: 52.6243 - val_loss: 114.6879\nEpoch 42/50\n - 0s - loss: 50.8375 - val_loss: 80.3581\nEpoch 43/50\n - 0s - loss: 46.8857 - val_loss: 96.0318\nEpoch 44/50\n - 0s - loss: 47.6017 - val_loss: 88.6760\nEpoch 45/50\n - 0s - loss: 49.5841 - val_loss: 83.8406\nEpoch 46/50\n - 0s - loss: 47.0298 - val_loss: 70.9436\nEpoch 47/50\n - 0s - loss: 49.4337 - val_loss: 81.7464\nEpoch 48/50\n - 0s - loss: 50.1906 - val_loss: 76.6718\nEpoch 49/50\n - 0s - loss: 48.0471 - val_loss: 80.9300\nEpoch 50/50\n - 1s - loss: 47.4948 - val_loss: 109.9248\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 53.2443 - val_loss: 78.1561\nEpoch 2/50\n - 0s - loss: 50.9567 - val_loss: 101.7806\nEpoch 3/50\n - 0s - loss: 49.7815 - val_loss: 83.4909\nEpoch 4/50\n - 0s - loss: 47.5798 - val_loss: 71.5770\nEpoch 5/50\n - 0s - loss: 47.7102 - val_loss: 80.0910\nEpoch 6/50\n - 0s - loss: 51.3853 - val_loss: 80.9508\nEpoch 7/50\n - 0s - loss: 48.0124 - val_loss: 98.4276\nEpoch 8/50\n - 0s - loss: 52.2042 - val_loss: 72.9499\nEpoch 9/50\n - 0s - loss: 53.3836 - val_loss: 80.9065\nEpoch 10/50\n - 1s - loss: 47.2910 - val_loss: 72.2911\nEpoch 11/50\n - 0s - loss: 48.6391 - val_loss: 95.4313\nEpoch 12/50\n - 0s - loss: 55.1899 - val_loss: 91.0638\nEpoch 13/50\n - 0s - loss: 55.7726 - val_loss: 68.6349\nEpoch 14/50\n - 0s - loss: 48.9918 - val_loss: 99.3005\nEpoch 15/50\n - 10s - loss: 50.5734 - val_loss: 104.2285\nEpoch 16/50\n - 0s - loss: 50.4323 - val_loss: 67.0143\nEpoch 17/50\n - 0s - loss: 49.3329 - val_loss: 105.4722\nEpoch 18/50\n - 0s - loss: 52.9212 - val_loss: 104.8239\nEpoch 19/50\n - 1s - loss: 48.0421 - val_loss: 95.6595\nEpoch 20/50\n - 1s - loss: 46.9884 - val_loss: 78.0310\nEpoch 21/50\n - 0s - loss: 47.4902 - val_loss: 78.3259\nEpoch 22/50\n - 0s - loss: 52.2789 - val_loss: 93.3432\nEpoch 23/50\n - 1s - loss: 50.5362 - val_loss: 92.1890\nEpoch 24/50\n - 0s - loss: 50.2989 - val_loss: 74.9176\nEpoch 25/50\n - 1s - loss: 54.7793 - val_loss: 122.8665\nEpoch 26/50\n - 1s - loss: 48.1848 - val_loss: 90.1673\nEpoch 27/50\n - 1s - loss: 52.0274 - val_loss: 91.4638\nEpoch 28/50\n - 0s - loss: 46.7510 - val_loss: 85.9325\nEpoch 29/50\n - 0s - loss: 49.1566 - val_loss: 70.0365\nEpoch 30/50\n - 0s - loss: 48.5312 - val_loss: 88.4281\nEpoch 31/50\n - 0s - loss: 48.2019 - val_loss: 68.2666\nEpoch 32/50\n - 0s - loss: 48.5916 - val_loss: 72.6167\nEpoch 33/50\n - 0s - loss: 52.7233 - val_loss: 110.3889\nEpoch 34/50\n - 0s - loss: 49.7282 - val_loss: 76.4736\nEpoch 35/50\n - 0s - loss: 47.5740 - val_loss: 88.2694\nEpoch 36/50\n - 10s - loss: 48.1973 - val_loss: 84.6533\nEpoch 37/50\n - 1s - loss: 47.3228 - val_loss: 83.5372\nEpoch 38/50\n - 1s - loss: 48.3229 - val_loss: 84.2370\nEpoch 39/50\n - 0s - loss: 49.3355 - val_loss: 113.8950\nEpoch 40/50\n - 0s - loss: 48.6510 - val_loss: 95.3465\nEpoch 41/50\n - 1s - loss: 47.6939 - val_loss: 86.3876\nEpoch 42/50\n - 0s - loss: 47.3080 - val_loss: 84.2017\nEpoch 43/50\n - 0s - loss: 47.0728 - val_loss: 73.0472\nEpoch 44/50\n - 0s - loss: 47.7426 - val_loss: 72.4960\nEpoch 45/50\n - 0s - loss: 50.1475 - val_loss: 71.5988\nEpoch 46/50\n - 0s - loss: 51.1622 - val_loss: 82.7151\nEpoch 47/50\n - 0s - loss: 48.3964 - val_loss: 93.3976\nEpoch 48/50\n - 0s - loss: 48.3675 - val_loss: 102.1186\nEpoch 49/50\n - 0s - loss: 48.9560 - val_loss: 113.4900\nEpoch 50/50\n - 0s - loss: 47.5922 - val_loss: 91.0573\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 48.5714 - val_loss: 102.3755\nEpoch 2/50\n - 0s - loss: 50.2621 - val_loss: 79.0911\nEpoch 3/50\n - 0s - loss: 49.0472 - val_loss: 77.5144\nEpoch 4/50\n - 8s - loss: 46.7856 - val_loss: 80.9910\nEpoch 5/50\n - 0s - loss: 48.7817 - val_loss: 90.1133\nEpoch 6/50\n - 0s - loss: 47.2200 - val_loss: 106.0488\nEpoch 7/50\n - 0s - loss: 49.9320 - val_loss: 70.9725\nEpoch 8/50\n - 0s - loss: 48.6367 - val_loss: 72.0811\nEpoch 9/50\n - 0s - loss: 55.0019 - val_loss: 82.1748\nEpoch 10/50\n - 0s - loss: 47.8756 - val_loss: 102.7282\nEpoch 11/50\n - 0s - loss: 48.5361 - val_loss: 84.3291\nEpoch 12/50\n - 0s - loss: 51.0642 - val_loss: 90.5589\nEpoch 13/50\n - 0s - loss: 49.5365 - val_loss: 75.6348\nEpoch 14/50\n - 0s - loss: 47.7103 - val_loss: 82.9047\nEpoch 15/50\n - 0s - loss: 46.9408 - val_loss: 96.1734\nEpoch 16/50\n - 0s - loss: 52.1828 - val_loss: 77.4810\nEpoch 17/50\n - 0s - loss: 48.6932 - val_loss: 83.0135\nEpoch 18/50\n - 0s - loss: 48.1871 - val_loss: 72.5328\nEpoch 19/50\n - 0s - loss: 53.8359 - val_loss: 73.8252\nEpoch 20/50\n - 1s - loss: 56.7249 - val_loss: 99.7039\nEpoch 21/50\n - 0s - loss: 51.4318 - val_loss: 94.8081\nEpoch 22/50\n - 0s - loss: 47.5251 - val_loss: 110.6858\nEpoch 23/50\n - 0s - loss: 48.8779 - val_loss: 88.8797\nEpoch 24/50\n - 0s - loss: 48.9359 - val_loss: 71.8265\nEpoch 25/50\n - 1s - loss: 55.1229 - val_loss: 134.0975\nEpoch 26/50\n - 0s - loss: 48.8465 - val_loss: 70.6300\nEpoch 27/50\n - 0s - loss: 49.8280 - val_loss: 97.4129\nEpoch 28/50\n - 0s - loss: 48.1756 - val_loss: 93.2260\nEpoch 29/50\n - 8s - loss: 48.8435 - val_loss: 116.5895\nEpoch 30/50\n - 0s - loss: 47.7605 - val_loss: 106.3723\nEpoch 31/50\n - 0s - loss: 48.0958 - val_loss: 104.4342\nEpoch 32/50\n - 0s - loss: 50.8329 - val_loss: 101.1826\nEpoch 33/50\n - 0s - loss: 48.4376 - val_loss: 87.8446\nEpoch 34/50\n - 0s - loss: 47.5656 - val_loss: 77.8346\nEpoch 35/50\n - 0s - loss: 47.8564 - val_loss: 82.6602\nEpoch 36/50\n - 0s - loss: 47.2607 - val_loss: 89.7550\nEpoch 37/50\n - 1s - loss: 47.0136 - val_loss: 84.5862\nEpoch 38/50\n - 0s - loss: 50.0344 - val_loss: 74.4748\nEpoch 39/50\n - 0s - loss: 53.2171 - val_loss: 93.4579\nEpoch 40/50\n - 0s - loss: 47.4120 - val_loss: 94.6628\nEpoch 41/50\n - 0s - loss: 47.2280 - val_loss: 98.5707\nEpoch 42/50\n - 0s - loss: 47.9101 - val_loss: 107.2906\nEpoch 43/50\n - 0s - loss: 47.6263 - val_loss: 97.4634\nEpoch 44/50\n - 1s - loss: 49.5332 - val_loss: 96.6667\nEpoch 45/50\n - 0s - loss: 48.6683 - val_loss: 89.5298\nEpoch 46/50\n - 0s - loss: 47.4648 - val_loss: 106.6949\nEpoch 47/50\n - 0s - loss: 48.9484 - val_loss: 80.6611\nEpoch 48/50\n - 0s - loss: 52.4655 - val_loss: 90.5914\nEpoch 49/50\n - 0s - loss: 50.1067 - val_loss: 72.2001\nEpoch 50/50\n - 0s - loss: 49.9993 - val_loss: 84.2937\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 49.8646 - val_loss: 115.9373\nEpoch 2/50\n - 0s - loss: 54.8160 - val_loss: 79.1897\nEpoch 3/50\n - 0s - loss: 50.1111 - val_loss: 115.9684\nEpoch 4/50\n - 5s - loss: 57.1341 - val_loss: 99.2614\nEpoch 5/50\n - 0s - loss: 47.5682 - val_loss: 87.6421\nEpoch 6/50\n - 3s - loss: 47.1467 - val_loss: 93.3867\nEpoch 7/50\n - 0s - loss: 48.1719 - val_loss: 96.7061\nEpoch 8/50\n - 0s - loss: 52.1477 - val_loss: 93.8779\nEpoch 9/50\n - 0s - loss: 49.5238 - val_loss: 93.9316\nEpoch 10/50\n - 0s - loss: 49.2763 - val_loss: 90.9174\nEpoch 11/50\n - 0s - loss: 47.3996 - val_loss: 73.8899\nEpoch 12/50\n - 1s - loss: 49.1326 - val_loss: 81.5848\nEpoch 13/50\n - 0s - loss: 49.2554 - val_loss: 79.2037\nEpoch 14/50\n - 0s - loss: 48.2010 - val_loss: 98.4103\nEpoch 15/50\n - 0s - loss: 47.4125 - val_loss: 79.3966\nEpoch 16/50\n - 0s - loss: 46.8907 - val_loss: 78.8530\nEpoch 17/50\n - 0s - loss: 47.3308 - val_loss: 77.9231\nEpoch 18/50\n - 0s - loss: 50.5519 - val_loss: 95.1274\nEpoch 19/50\n - 0s - loss: 48.8166 - val_loss: 86.4887\nEpoch 20/50\n - 0s - loss: 48.4008 - val_loss: 117.8802\nEpoch 21/50\n - 0s - loss: 52.5644 - val_loss: 99.5218\nEpoch 22/50\n - 0s - loss: 47.4691 - val_loss: 90.4696\nEpoch 23/50\n - 0s - loss: 47.1707 - val_loss: 83.6487\nEpoch 24/50\n - 0s - loss: 48.0092 - val_loss: 84.1696\nEpoch 25/50\n - 8s - loss: 47.1619 - val_loss: 97.3992\nEpoch 26/50\n - 3s - loss: 48.1298 - val_loss: 77.3692\nEpoch 27/50\n - 0s - loss: 48.3593 - val_loss: 96.9037\nEpoch 28/50\n - 0s - loss: 47.2745 - val_loss: 91.9151\nEpoch 29/50\n - 0s - loss: 48.0125 - val_loss: 82.2750\nEpoch 30/50\n - 0s - loss: 47.5840 - val_loss: 75.6396\nEpoch 31/50\n - 0s - loss: 47.0169 - val_loss: 117.7518\nEpoch 32/50\n - 0s - loss: 58.1813 - val_loss: 71.2673\nEpoch 33/50\n - 0s - loss: 52.4074 - val_loss: 115.7016\nEpoch 34/50\n - 0s - loss: 51.9537 - val_loss: 77.4009\nEpoch 35/50\n - 0s - loss: 53.2037 - val_loss: 81.0878\nEpoch 36/50\n - 1s - loss: 49.5899 - val_loss: 99.2274\nEpoch 37/50\n - 0s - loss: 51.6420 - val_loss: 86.7837\nEpoch 38/50\n - 0s - loss: 47.0541 - val_loss: 78.1983\nEpoch 39/50\n - 0s - loss: 46.6669 - val_loss: 104.0603\nEpoch 40/50\n - 3s - loss: 48.8442 - val_loss: 68.5696\nEpoch 41/50\n - 1s - loss: 49.4676 - val_loss: 75.2351\nEpoch 42/50\n - 0s - loss: 47.3532 - val_loss: 111.0411\nEpoch 43/50\n - 0s - loss: 51.7694 - val_loss: 89.9041\nEpoch 44/50\n - 0s - loss: 48.5270 - val_loss: 77.1426\nEpoch 45/50\n - 7s - loss: 47.1288 - val_loss: 83.7219\nEpoch 46/50\n - 0s - loss: 49.4524 - val_loss: 73.7101\nEpoch 47/50\n - 0s - loss: 46.9934 - val_loss: 89.8415\nEpoch 48/50\n - 0s - loss: 47.6537 - val_loss: 82.5240\nEpoch 49/50\n - 0s - loss: 48.0558 - val_loss: 75.8261\nEpoch 50/50\n - 1s - loss: 48.6159 - val_loss: 79.4471\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 46.5806 - val_loss: 82.1651\nEpoch 2/50\n - 0s - loss: 49.6168 - val_loss: 73.7704\nEpoch 3/50\n - 0s - loss: 48.0447 - val_loss: 92.8268\nEpoch 4/50\n - 0s - loss: 48.3647 - val_loss: 78.4060\nEpoch 5/50\n - 0s - loss: 49.2231 - val_loss: 102.5086\nEpoch 6/50\n - 0s - loss: 57.3511 - val_loss: 66.9916\nEpoch 7/50\n - 0s - loss: 53.9346 - val_loss: 85.7887\nEpoch 8/50\n - 0s - loss: 46.5158 - val_loss: 77.0088\nEpoch 9/50\n - 0s - loss: 47.4729 - val_loss: 79.3486\nEpoch 10/50\n - 0s - loss: 49.6141 - val_loss: 72.1351\nEpoch 11/50\n - 0s - loss: 48.4602 - val_loss: 83.7248\nEpoch 12/50\n - 1s - loss: 48.8791 - val_loss: 85.3537\nEpoch 13/50\n - 8s - loss: 48.5425 - val_loss: 75.5719\nEpoch 14/50\n - 0s - loss: 50.9352 - val_loss: 70.1655\nEpoch 15/50\n - 0s - loss: 49.5473 - val_loss: 105.7900\nEpoch 16/50\n - 3s - loss: 49.6784 - val_loss: 90.3061\nEpoch 17/50\n - 0s - loss: 50.2154 - val_loss: 97.4744\nEpoch 18/50\n - 0s - loss: 47.5708 - val_loss: 89.4820\nEpoch 19/50\n - 0s - loss: 46.7503 - val_loss: 87.5036\nEpoch 20/50\n - 0s - loss: 48.6461 - val_loss: 81.3759\nEpoch 21/50\n - 0s - loss: 47.1832 - val_loss: 82.9163\nEpoch 22/50\n - 0s - loss: 47.8097 - val_loss: 82.3118\nEpoch 23/50\n - 0s - loss: 47.4504 - val_loss: 74.6693\nEpoch 24/50\n - 3s - loss: 48.7769 - val_loss: 68.0905\nEpoch 25/50\n - 0s - loss: 58.8083 - val_loss: 120.5124\nEpoch 26/50\n - 0s - loss: 52.3677 - val_loss: 124.2942\nEpoch 27/50\n - 0s - loss: 49.8511 - val_loss: 80.1134\nEpoch 28/50\n - 0s - loss: 50.1234 - val_loss: 81.6935\nEpoch 29/50\n - 0s - loss: 46.9805 - val_loss: 78.3607\nEpoch 30/50\n - 0s - loss: 47.7334 - val_loss: 88.7828\nEpoch 31/50\n - 0s - loss: 50.1536 - val_loss: 102.4459\nEpoch 32/50\n - 1s - loss: 55.0161 - val_loss: 73.4707\nEpoch 33/50\n - 8s - loss: 48.8571 - val_loss: 105.8575\nEpoch 34/50\n - 0s - loss: 47.3817 - val_loss: 96.5086\nEpoch 35/50\n - 1s - loss: 49.9563 - val_loss: 76.3463\nEpoch 36/50\n - 0s - loss: 48.6550 - val_loss: 70.5807\nEpoch 37/50\n - 0s - loss: 49.2505 - val_loss: 80.4959\nEpoch 38/50\n - 0s - loss: 48.0408 - val_loss: 96.5154\nEpoch 39/50\n - 0s - loss: 47.2811 - val_loss: 88.0973\nEpoch 40/50\n - 1s - loss: 46.7915 - val_loss: 98.6981\nEpoch 41/50\n - 3s - loss: 48.3376 - val_loss: 70.6137\nEpoch 42/50\n - 0s - loss: 50.3806 - val_loss: 71.7333\nEpoch 43/50\n - 0s - loss: 49.8025 - val_loss: 71.8077\nEpoch 44/50\n - 0s - loss: 55.2714 - val_loss: 82.1034\nEpoch 45/50\n - 0s - loss: 47.6486 - val_loss: 76.9714\nEpoch 46/50\n - 5s - loss: 50.9401 - val_loss: 86.7235\nEpoch 47/50\n - 0s - loss: 49.2413 - val_loss: 114.3332\nEpoch 48/50\n - 0s - loss: 49.4831 - val_loss: 84.4471\nEpoch 49/50\n - 3s - loss: 49.7659 - val_loss: 86.9231\nEpoch 50/50\n - 0s - loss: 47.8983 - val_loss: 94.7754\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 47.6037 - val_loss: 112.2386\nEpoch 2/50\n - 0s - loss: 50.8867 - val_loss: 91.8222\nEpoch 3/50\n - 0s - loss: 47.3450 - val_loss: 80.5964\nEpoch 4/50\n - 0s - loss: 52.4058 - val_loss: 73.1534\nEpoch 5/50\n - 0s - loss: 49.4885 - val_loss: 87.4170\nEpoch 6/50\n - 0s - loss: 51.0631 - val_loss: 96.8248\nEpoch 7/50\n - 0s - loss: 48.9309 - val_loss: 78.7708\nEpoch 8/50\n - 0s - loss: 49.0497 - val_loss: 85.0324\nEpoch 9/50\n - 0s - loss: 49.5869 - val_loss: 100.4800\nEpoch 10/50\n - 0s - loss: 49.5390 - val_loss: 84.2282\nEpoch 11/50\n - 3s - loss: 46.9872 - val_loss: 81.5115\nEpoch 12/50\n - 0s - loss: 49.2275 - val_loss: 112.7750\nEpoch 13/50\n - 0s - loss: 51.3895 - val_loss: 105.5385\nEpoch 14/50\n - 0s - loss: 54.2058 - val_loss: 82.8063\nEpoch 15/50\n - 0s - loss: 50.3712 - val_loss: 80.1858\nEpoch 16/50\n - 0s - loss: 47.5464 - val_loss: 82.0057\nEpoch 17/50\n - 0s - loss: 50.4909 - val_loss: 128.2608\nEpoch 18/50\n - 0s - loss: 50.9648 - val_loss: 89.4802\nEpoch 19/50\n - 5s - loss: 48.1853 - val_loss: 88.8559\nEpoch 20/50\n - 3s - loss: 50.0921 - val_loss: 82.4434\nEpoch 21/50\n - 0s - loss: 51.9809 - val_loss: 92.6652\nEpoch 22/50\n - 0s - loss: 46.8254 - val_loss: 89.7759\nEpoch 23/50\n - 0s - loss: 47.2697 - val_loss: 87.0776\nEpoch 24/50\n - 0s - loss: 50.0604 - val_loss: 82.6020\nEpoch 25/50\n - 0s - loss: 50.3323 - val_loss: 101.1560\nEpoch 26/50\n - 0s - loss: 51.0242 - val_loss: 81.2020\nEpoch 27/50\n - 0s - loss: 50.6639 - val_loss: 85.7667\nEpoch 28/50\n - 0s - loss: 47.5363 - val_loss: 79.3236\nEpoch 29/50\n - 0s - loss: 47.0704 - val_loss: 80.0855\nEpoch 30/50\n - 0s - loss: 47.7375 - val_loss: 146.2489\nEpoch 31/50\n - 0s - loss: 50.4951 - val_loss: 81.4861\nEpoch 32/50\n - 0s - loss: 46.4340 - val_loss: 83.2563\nEpoch 33/50\n - 0s - loss: 46.8777 - val_loss: 81.1086\nEpoch 34/50\n - 0s - loss: 48.5331 - val_loss: 73.6796\nEpoch 35/50\n - 1s - loss: 48.3920 - val_loss: 94.0383\nEpoch 36/50\n - 0s - loss: 50.0175 - val_loss: 71.8038\nEpoch 37/50\n - 0s - loss: 47.2106 - val_loss: 76.0972\nEpoch 38/50\n - 0s - loss: 48.5439 - val_loss: 79.8859\nEpoch 39/50\n - 0s - loss: 48.3653 - val_loss: 102.6739\nEpoch 40/50\n - 0s - loss: 47.6293 - val_loss: 99.0164\nEpoch 41/50\n - 0s - loss: 48.1540 - val_loss: 94.7364\nEpoch 42/50\n - 7s - loss: 49.3591 - val_loss: 77.1987\nEpoch 43/50\n - 0s - loss: 47.1763 - val_loss: 82.9883\nEpoch 44/50\n - 0s - loss: 47.6948 - val_loss: 95.9262\nEpoch 45/50\n - 3s - loss: 47.7494 - val_loss: 89.5064\nEpoch 46/50\n - 1s - loss: 50.1862 - val_loss: 119.8197\nEpoch 47/50\n - 0s - loss: 49.4454 - val_loss: 110.2191\nEpoch 48/50\n - 0s - loss: 49.7508 - val_loss: 92.6551\nEpoch 49/50\n - 0s - loss: 47.5220 - val_loss: 79.9391\nEpoch 50/50\n - 0s - loss: 47.4222 - val_loss: 77.9002\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 52.0321 - val_loss: 77.3509\nEpoch 2/50\n - 0s - loss: 58.1456 - val_loss: 120.6757\nEpoch 3/50\n - 0s - loss: 48.8863 - val_loss: 100.4094\nEpoch 4/50\n - 0s - loss: 47.4283 - val_loss: 77.3652\nEpoch 5/50\n - 0s - loss: 48.7247 - val_loss: 97.5019\nEpoch 6/50\n - 0s - loss: 47.4261 - val_loss: 81.8243\nEpoch 7/50\n - 0s - loss: 53.9084 - val_loss: 75.7319\nEpoch 8/50\n - 0s - loss: 48.0232 - val_loss: 100.2309\nEpoch 9/50\n - 0s - loss: 48.5615 - val_loss: 109.6821\nEpoch 10/50\n - 1s - loss: 48.5655 - val_loss: 85.8958\nEpoch 11/50\n - 0s - loss: 48.1384 - val_loss: 70.0913\nEpoch 12/50\n - 0s - loss: 51.4690 - val_loss: 92.2121\nEpoch 13/50\n - 0s - loss: 48.4601 - val_loss: 95.3165\nEpoch 14/50\n - 0s - loss: 46.5480 - val_loss: 88.0761\nEpoch 15/50\n - 0s - loss: 48.4026 - val_loss: 99.6113\nEpoch 16/50\n - 0s - loss: 46.8173 - val_loss: 92.5905\nEpoch 17/50\n - 9s - loss: 46.8875 - val_loss: 90.1782\nEpoch 18/50\n - 0s - loss: 46.9492 - val_loss: 87.4755\nEpoch 19/50\n - 0s - loss: 47.9410 - val_loss: 88.6355\nEpoch 20/50\n - 0s - loss: 48.5412 - val_loss: 78.4483\nEpoch 21/50\n - 0s - loss: 50.1334 - val_loss: 79.4753\nEpoch 22/50\n - 0s - loss: 49.4545 - val_loss: 95.3825\nEpoch 23/50\n - 0s - loss: 47.6191 - val_loss: 77.2296\nEpoch 24/50\n - 0s - loss: 48.2069 - val_loss: 101.7317\nEpoch 25/50\n - 0s - loss: 46.9365 - val_loss: 85.4412\nEpoch 26/50\n - 0s - loss: 47.5309 - val_loss: 78.8798\nEpoch 27/50\n - 0s - loss: 48.5923 - val_loss: 82.6877\nEpoch 28/50\n - 1s - loss: 47.7757 - val_loss: 76.4346\nEpoch 29/50\n - 0s - loss: 47.5886 - val_loss: 77.9384\nEpoch 30/50\n - 0s - loss: 51.7702 - val_loss: 93.7528\nEpoch 31/50\n - 0s - loss: 47.4821 - val_loss: 91.1089\nEpoch 32/50\n - 0s - loss: 47.6559 - val_loss: 73.7375\nEpoch 33/50\n - 0s - loss: 48.0789 - val_loss: 83.2809\nEpoch 34/50\n - 0s - loss: 49.6661 - val_loss: 109.6610\nEpoch 35/50\n - 0s - loss: 48.7246 - val_loss: 119.8574\nEpoch 36/50\n - 0s - loss: 48.7221 - val_loss: 93.4874\nEpoch 37/50\n - 0s - loss: 49.3680 - val_loss: 78.3881\nEpoch 38/50\n - 0s - loss: 48.7078 - val_loss: 111.3082\nEpoch 39/50\n - 0s - loss: 48.9600 - val_loss: 71.6796\nEpoch 40/50\n - 5s - loss: 49.3773 - val_loss: 77.7241\nEpoch 41/50\n - 0s - loss: 48.7404 - val_loss: 75.9849\nEpoch 42/50\n - 0s - loss: 50.1305 - val_loss: 99.7196\nEpoch 43/50\n - 3s - loss: 49.0036 - val_loss: 100.2306\nEpoch 44/50\n - 0s - loss: 50.7805 - val_loss: 69.3672\nEpoch 45/50\n - 0s - loss: 52.8787 - val_loss: 88.4267\nEpoch 46/50\n - 0s - loss: 50.0703 - val_loss: 113.6152\nEpoch 47/50\n - 0s - loss: 47.4628 - val_loss: 85.6186\nEpoch 48/50\n - 0s - loss: 47.0478 - val_loss: 84.3813\nEpoch 49/50\n - 0s - loss: 48.5857 - val_loss: 87.3485\nEpoch 50/50\n - 0s - loss: 47.0089 - val_loss: 90.5675\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 49.1249 - val_loss: 83.6220\nEpoch 2/50\n - 0s - loss: 49.3112 - val_loss: 116.8810\nEpoch 3/50\n - 0s - loss: 52.9299 - val_loss: 81.0272\nEpoch 4/50\n - 0s - loss: 50.8877 - val_loss: 94.2757\nEpoch 5/50\n - 0s - loss: 47.6482 - val_loss: 78.0446\nEpoch 6/50\n - 0s - loss: 46.5639 - val_loss: 71.7566\nEpoch 7/50\n - 0s - loss: 57.1660 - val_loss: 90.3969\nEpoch 8/50\n - 0s - loss: 48.9950 - val_loss: 94.7623\nEpoch 9/50\n - 0s - loss: 52.5918 - val_loss: 83.6631\nEpoch 10/50\n - 0s - loss: 47.4097 - val_loss: 92.1151\nEpoch 11/50\n - 0s - loss: 48.7734 - val_loss: 77.4066\nEpoch 12/50\n - 0s - loss: 47.7896 - val_loss: 100.9970\nEpoch 13/50\n - 0s - loss: 48.4518 - val_loss: 108.1597\nEpoch 14/50\n - 0s - loss: 47.4598 - val_loss: 103.0372\nEpoch 15/50\n - 0s - loss: 50.8304 - val_loss: 86.2037\nEpoch 16/50\n - 0s - loss: 48.3123 - val_loss: 82.7571\nEpoch 17/50\n - 0s - loss: 50.6613 - val_loss: 93.8898\nEpoch 18/50\n - 0s - loss: 49.2934 - val_loss: 87.4548\nEpoch 19/50\n - 10s - loss: 48.1371 - val_loss: 97.6488\nEpoch 20/50\n - 0s - loss: 46.6344 - val_loss: 93.5814\nEpoch 21/50\n - 0s - loss: 47.3031 - val_loss: 79.0816\nEpoch 22/50\n - 0s - loss: 54.0784 - val_loss: 108.7104\nEpoch 23/50\n - 0s - loss: 49.7616 - val_loss: 95.2359\nEpoch 24/50\n - 1s - loss: 48.0505 - val_loss: 75.0040\nEpoch 25/50\n - 0s - loss: 48.8764 - val_loss: 91.6558\nEpoch 26/50\n - 0s - loss: 48.7710 - val_loss: 127.7524\nEpoch 27/50\n - 0s - loss: 49.0565 - val_loss: 104.9522\nEpoch 28/50\n - 0s - loss: 47.6132 - val_loss: 80.4320\nEpoch 29/50\n - 0s - loss: 48.7782 - val_loss: 89.7591\nEpoch 30/50\n - 0s - loss: 49.6255 - val_loss: 69.7701\nEpoch 31/50\n - 0s - loss: 49.5394 - val_loss: 75.2758\nEpoch 32/50\n - 0s - loss: 47.8341 - val_loss: 80.0014\nEpoch 33/50\n - 0s - loss: 47.5438 - val_loss: 103.8991\nEpoch 34/50\n - 0s - loss: 49.4309 - val_loss: 70.9947\nEpoch 35/50\n - 0s - loss: 47.7707 - val_loss: 97.5791\nEpoch 36/50\n - 0s - loss: 48.9410 - val_loss: 120.8458\nEpoch 37/50\n - 0s - loss: 51.3330 - val_loss: 87.9919\nEpoch 38/50\n - 7s - loss: 46.8838 - val_loss: 95.3056\nEpoch 39/50\n - 1s - loss: 48.1234 - val_loss: 106.1467\nEpoch 40/50\n - 1s - loss: 48.3723 - val_loss: 90.0549\nEpoch 41/50\n - 3s - loss: 46.7942 - val_loss: 83.3660\nEpoch 42/50\n - 0s - loss: 46.8290 - val_loss: 115.8265\nEpoch 43/50\n - 0s - loss: 49.6885 - val_loss: 70.3135\nEpoch 44/50\n - 0s - loss: 50.1788 - val_loss: 85.3154\nEpoch 45/50\n - 0s - loss: 51.7802 - val_loss: 79.7374\nEpoch 46/50\n - 0s - loss: 48.8758 - val_loss: 98.2563\nEpoch 47/50\n - 0s - loss: 47.5203 - val_loss: 89.9959\nEpoch 48/50\n - 0s - loss: 51.2269 - val_loss: 80.0217\nEpoch 49/50\n - 0s - loss: 48.5343 - val_loss: 79.7036\nEpoch 50/50\n - 0s - loss: 48.3261 - val_loss: 108.0029\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 47.5454 - val_loss: 88.8898\nEpoch 2/50\n - 3s - loss: 50.1824 - val_loss: 122.3615\nEpoch 3/50\n - 0s - loss: 53.8086 - val_loss: 98.3761\nEpoch 4/50\n - 1s - loss: 48.8901 - val_loss: 71.2529\nEpoch 5/50\n - 5s - loss: 54.9433 - val_loss: 85.8830\nEpoch 6/50\n - 0s - loss: 50.9996 - val_loss: 90.2150\nEpoch 7/50\n - 0s - loss: 49.3631 - val_loss: 92.5838\nEpoch 8/50\n - 4s - loss: 47.9691 - val_loss: 108.3070\nEpoch 9/50\n - 0s - loss: 52.2270 - val_loss: 80.8391\nEpoch 10/50\n - 0s - loss: 47.8415 - val_loss: 82.5573\nEpoch 11/50\n - 3s - loss: 47.6670 - val_loss: 80.1131\nEpoch 12/50\n - 0s - loss: 47.1792 - val_loss: 100.5471\nEpoch 13/50\n - 0s - loss: 47.4874 - val_loss: 72.3492\nEpoch 14/50\n - 0s - loss: 47.6387 - val_loss: 76.6895\nEpoch 15/50\n - 0s - loss: 47.7368 - val_loss: 74.4024\nEpoch 16/50\n - 0s - loss: 47.2965 - val_loss: 80.0506\nEpoch 17/50\n - 0s - loss: 49.7168 - val_loss: 74.2917\nEpoch 18/50\n - 0s - loss: 48.2671 - val_loss: 80.8472\nEpoch 19/50\n - 0s - loss: 48.5496 - val_loss: 75.0411\nEpoch 20/50\n - 0s - loss: 47.7824 - val_loss: 81.4696\nEpoch 21/50\n - 0s - loss: 49.3812 - val_loss: 73.6315\nEpoch 22/50\n - 0s - loss: 47.8091 - val_loss: 85.3968\nEpoch 23/50\n - 0s - loss: 47.6280 - val_loss: 74.3709\nEpoch 24/50\n - 0s - loss: 49.8568 - val_loss: 103.8402\nEpoch 25/50\n - 5s - loss: 50.9305 - val_loss: 100.1717\nEpoch 26/50\n - 0s - loss: 48.5776 - val_loss: 85.3043\nEpoch 27/50\n - 0s - loss: 47.8640 - val_loss: 115.8589\nEpoch 28/50\n - 3s - loss: 48.3559 - val_loss: 88.4502\nEpoch 29/50\n - 0s - loss: 46.6920 - val_loss: 74.0993\nEpoch 30/50\n - 1s - loss: 47.7790 - val_loss: 76.7551\nEpoch 31/50\n - 3s - loss: 47.1949 - val_loss: 82.0379\nEpoch 32/50\n - 0s - loss: 47.6595 - val_loss: 105.1765\nEpoch 33/50\n - 1s - loss: 48.0614 - val_loss: 69.1079\nEpoch 34/50\n - 0s - loss: 50.0140 - val_loss: 85.3823\nEpoch 35/50\n - 0s - loss: 48.2796 - val_loss: 100.9119\nEpoch 36/50\n - 0s - loss: 48.0248 - val_loss: 77.2970\nEpoch 37/50\n - 0s - loss: 51.9026 - val_loss: 69.5531\nEpoch 38/50\n - 0s - loss: 52.4873 - val_loss: 115.9834\nEpoch 39/50\n - 0s - loss: 52.8439 - val_loss: 92.2611\nEpoch 40/50\n - 0s - loss: 50.3051 - val_loss: 89.9284\nEpoch 41/50\n - 0s - loss: 51.2255 - val_loss: 77.0123\nEpoch 42/50\n - 0s - loss: 48.7509 - val_loss: 68.2380\nEpoch 43/50\n - 0s - loss: 54.6330 - val_loss: 91.5984\nEpoch 44/50\n - 0s - loss: 58.3619 - val_loss: 141.7739\nEpoch 45/50\n - 0s - loss: 53.4148 - val_loss: 81.1376\nEpoch 46/50\n - 7s - loss: 51.1881 - val_loss: 83.0943\nEpoch 47/50\n - 0s - loss: 53.3394 - val_loss: 80.8116\nEpoch 48/50\n - 0s - loss: 49.6162 - val_loss: 97.6196\nEpoch 49/50\n - 0s - loss: 47.8280 - val_loss: 122.9401\nEpoch 50/50\n - 0s - loss: 52.8566 - val_loss: 99.7993\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 48.0438 - val_loss: 103.7944\nEpoch 2/50\n - 0s - loss: 46.5003 - val_loss: 77.4203\nEpoch 3/50\n - 0s - loss: 47.1060 - val_loss: 79.6878\nEpoch 4/50\n - 0s - loss: 48.1362 - val_loss: 116.2096\nEpoch 5/50\n - 0s - loss: 49.3360 - val_loss: 80.0662\nEpoch 6/50\n - 0s - loss: 47.6663 - val_loss: 78.5118\nEpoch 7/50\n - 0s - loss: 47.2472 - val_loss: 115.9641\nEpoch 8/50\n - 1s - loss: 47.4862 - val_loss: 112.2173\nEpoch 9/50\n - 0s - loss: 51.0750 - val_loss: 83.3876\nEpoch 10/50\n - 0s - loss: 48.5563 - val_loss: 81.7333\nEpoch 11/50\n - 3s - loss: 47.3957 - val_loss: 75.0891\nEpoch 12/50\n - 0s - loss: 47.3785 - val_loss: 104.1474\nEpoch 13/50\n - 0s - loss: 47.2133 - val_loss: 93.2176\nEpoch 14/50\n - 7s - loss: 46.9759 - val_loss: 77.1637\nEpoch 15/50\n - 3s - loss: 48.3998 - val_loss: 72.3112\nEpoch 16/50\n - 0s - loss: 47.6054 - val_loss: 80.1341\nEpoch 17/50\n - 0s - loss: 50.1284 - val_loss: 102.7742\nEpoch 18/50\n - 0s - loss: 47.8529 - val_loss: 89.6604\nEpoch 19/50\n - 0s - loss: 47.7566 - val_loss: 87.0982\nEpoch 20/50\n - 0s - loss: 46.6617 - val_loss: 95.3413\nEpoch 21/50\n - 3s - loss: 48.5332 - val_loss: 107.2907\nEpoch 22/50\n - 0s - loss: 46.8126 - val_loss: 81.8869\nEpoch 23/50\n - 0s - loss: 47.3392 - val_loss: 74.1142\nEpoch 24/50\n - 0s - loss: 51.5701 - val_loss: 110.8519\nEpoch 25/50\n - 9s - loss: 52.3293 - val_loss: 83.1299\nEpoch 26/50\n - 1s - loss: 49.4423 - val_loss: 84.0990\nEpoch 27/50\n - 1s - loss: 50.5085 - val_loss: 89.3453\nEpoch 28/50\n - 0s - loss: 48.5450 - val_loss: 108.1453\nEpoch 29/50\n - 4s - loss: 53.0423 - val_loss: 100.3504\nEpoch 30/50\n - 0s - loss: 51.9457 - val_loss: 72.4449\nEpoch 31/50\n - 0s - loss: 48.7627 - val_loss: 90.0362\nEpoch 32/50\n - 0s - loss: 48.8414 - val_loss: 96.6161\nEpoch 33/50\n - 1s - loss: 48.3943 - val_loss: 72.7256\nEpoch 34/50\n - 6s - loss: 60.8043 - val_loss: 73.6788\nEpoch 35/50\n - 0s - loss: 52.4539 - val_loss: 119.8878\nEpoch 36/50\n - 0s - loss: 58.6098 - val_loss: 69.5198\nEpoch 37/50\n - 8s - loss: 49.0226 - val_loss: 78.0315\nEpoch 38/50\n - 1s - loss: 48.8272 - val_loss: 68.3018\nEpoch 39/50\n - 1s - loss: 47.8793 - val_loss: 92.6316\nEpoch 40/50\n - 1s - loss: 47.9397 - val_loss: 81.8362\nEpoch 41/50\n - 0s - loss: 50.4355 - val_loss: 102.9350\nEpoch 42/50\n - 1s - loss: 49.0068 - val_loss: 84.1939\nEpoch 43/50\n - 0s - loss: 47.3526 - val_loss: 102.4154\nEpoch 44/50\n - 0s - loss: 50.1023 - val_loss: 76.5523\nEpoch 45/50\n - 1s - loss: 49.0924 - val_loss: 87.2627\nEpoch 46/50\n - 1s - loss: 56.6438 - val_loss: 109.4440\nEpoch 47/50\n - 0s - loss: 51.1886 - val_loss: 76.6091\nEpoch 48/50\n - 0s - loss: 48.4362 - val_loss: 84.3809\nEpoch 49/50\n - 1s - loss: 51.7637 - val_loss: 94.9509\nEpoch 50/50\n - 0s - loss: 49.1655 - val_loss: 81.9034\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 48.6211 - val_loss: 90.4194\nEpoch 2/50\n - 0s - loss: 46.5547 - val_loss: 81.9382\nEpoch 3/50\n - 0s - loss: 46.5842 - val_loss: 83.0965\nEpoch 4/50\n - 0s - loss: 47.6615 - val_loss: 79.6141\nEpoch 5/50\n - 6s - loss: 52.1162 - val_loss: 78.1428\nEpoch 6/50\n - 0s - loss: 49.9149 - val_loss: 74.3328\nEpoch 7/50\n - 0s - loss: 50.6338 - val_loss: 95.8966\nEpoch 8/50\n - 3s - loss: 48.7528 - val_loss: 82.1341\nEpoch 9/50\n - 0s - loss: 48.7916 - val_loss: 89.7566\nEpoch 10/50\n - 0s - loss: 47.9193 - val_loss: 89.8591\nEpoch 11/50\n - 0s - loss: 48.8649 - val_loss: 75.2062\nEpoch 12/50\n - 0s - loss: 48.7947 - val_loss: 99.7326\nEpoch 13/50\n - 0s - loss: 48.7902 - val_loss: 81.2701\nEpoch 14/50\n - 1s - loss: 49.7512 - val_loss: 128.0073\nEpoch 15/50\n - 0s - loss: 54.8986 - val_loss: 65.4990\nEpoch 16/50\n - 0s - loss: 50.3578 - val_loss: 99.0713\nEpoch 17/50\n - 0s - loss: 50.1223 - val_loss: 93.5079\nEpoch 18/50\n - 0s - loss: 52.3517 - val_loss: 83.5502\nEpoch 19/50\n - 0s - loss: 53.6030 - val_loss: 93.4739\nEpoch 20/50\n - 0s - loss: 48.2596 - val_loss: 124.3600\nEpoch 21/50\n - 0s - loss: 50.8049 - val_loss: 85.0593\nEpoch 22/50\n - 0s - loss: 48.4587 - val_loss: 83.5743\nEpoch 23/50\n - 0s - loss: 47.8654 - val_loss: 89.6846\nEpoch 24/50\n - 0s - loss: 47.2535 - val_loss: 89.6755\nEpoch 25/50\n - 0s - loss: 47.3781 - val_loss: 98.0706\nEpoch 26/50\n - 1s - loss: 47.0930 - val_loss: 98.7874\nEpoch 27/50\n - 1s - loss: 47.6440 - val_loss: 76.7448\nEpoch 28/50\n - 7s - loss: 49.7009 - val_loss: 75.4823\nEpoch 29/50\n - 3s - loss: 50.5012 - val_loss: 85.7328\nEpoch 30/50\n - 0s - loss: 47.3419 - val_loss: 71.9833\nEpoch 31/50\n - 0s - loss: 49.3682 - val_loss: 83.6388\nEpoch 32/50\n - 0s - loss: 47.5636 - val_loss: 81.4190\nEpoch 33/50\n - 0s - loss: 50.1624 - val_loss: 65.4348\nEpoch 34/50\n - 0s - loss: 49.5660 - val_loss: 79.0990\nEpoch 35/50\n - 0s - loss: 47.3998 - val_loss: 83.1574\nEpoch 36/50\n - 0s - loss: 46.7633 - val_loss: 85.6433\nEpoch 37/50\n - 0s - loss: 46.8576 - val_loss: 87.8312\nEpoch 38/50\n - 0s - loss: 47.9719 - val_loss: 87.8458\nEpoch 39/50\n - 0s - loss: 48.0059 - val_loss: 77.3974\nEpoch 40/50\n - 1s - loss: 48.5553 - val_loss: 79.1943\nEpoch 41/50\n - 0s - loss: 48.9522 - val_loss: 67.3432\nEpoch 42/50\n - 0s - loss: 61.3828 - val_loss: 74.1530\nEpoch 43/50\n - 0s - loss: 50.7155 - val_loss: 70.6901\nEpoch 44/50\n - 3s - loss: 52.0577 - val_loss: 90.3912\nEpoch 45/50\n - 0s - loss: 47.5307 - val_loss: 67.5797\nEpoch 46/50\n - 5s - loss: 47.4845 - val_loss: 87.5590\nEpoch 47/50\n - 0s - loss: 49.6846 - val_loss: 100.8863\nEpoch 48/50\n - 0s - loss: 47.5759 - val_loss: 81.5320\nEpoch 49/50\n - 3s - loss: 47.3306 - val_loss: 86.5537\nEpoch 50/50\n - 0s - loss: 47.1706 - val_loss: 107.5691\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 51.8504 - val_loss: 75.9569\nEpoch 2/50\n - 0s - loss: 47.5221 - val_loss: 104.0577\nEpoch 3/50\n - 1s - loss: 49.1653 - val_loss: 81.1351\nEpoch 4/50\n - 0s - loss: 48.5792 - val_loss: 82.8327\nEpoch 5/50\n - 0s - loss: 46.8525 - val_loss: 111.7287\nEpoch 6/50\n - 0s - loss: 47.5356 - val_loss: 163.3693\nEpoch 7/50\n - 0s - loss: 53.9706 - val_loss: 86.3298\nEpoch 8/50\n - 0s - loss: 49.2129 - val_loss: 69.4090\nEpoch 9/50\n - 0s - loss: 48.4984 - val_loss: 73.6942\nEpoch 10/50\n - 0s - loss: 49.2580 - val_loss: 102.9694\nEpoch 11/50\n - 4s - loss: 49.3187 - val_loss: 75.8914\nEpoch 12/50\n - 0s - loss: 48.5687 - val_loss: 76.5881\nEpoch 13/50\n - 0s - loss: 47.8104 - val_loss: 91.3824\nEpoch 14/50\n - 6s - loss: 50.1354 - val_loss: 113.1936\nEpoch 15/50\n - 1s - loss: 48.6413 - val_loss: 102.2572\nEpoch 16/50\n - 3s - loss: 48.0420 - val_loss: 93.8835\nEpoch 17/50\n - 0s - loss: 48.9708 - val_loss: 86.4906\nEpoch 18/50\n - 0s - loss: 48.2331 - val_loss: 106.9826\nEpoch 19/50\n - 0s - loss: 57.0604 - val_loss: 114.0192\nEpoch 20/50\n - 0s - loss: 56.2147 - val_loss: 73.5301\nEpoch 21/50\n - 0s - loss: 47.9050 - val_loss: 96.5318\nEpoch 22/50\n - 0s - loss: 47.2741 - val_loss: 84.1281\nEpoch 23/50\n - 0s - loss: 48.3963 - val_loss: 123.1743\nEpoch 24/50\n - 0s - loss: 51.5874 - val_loss: 71.5398\nEpoch 25/50\n - 0s - loss: 50.2089 - val_loss: 91.2128\nEpoch 26/50\n - 1s - loss: 46.9689 - val_loss: 90.5737\nEpoch 27/50\n - 0s - loss: 50.4978 - val_loss: 126.8081\nEpoch 28/50\n - 0s - loss: 49.7994 - val_loss: 85.8398\nEpoch 29/50\n - 0s - loss: 47.8939 - val_loss: 85.4441\nEpoch 30/50\n - 1s - loss: 49.9124 - val_loss: 92.0963\nEpoch 31/50\n - 3s - loss: 48.6478 - val_loss: 71.4158\nEpoch 32/50\n - 4s - loss: 61.5154 - val_loss: 82.9405\nEpoch 33/50\n - 2s - loss: 57.3664 - val_loss: 144.5349\nEpoch 34/50\n - 0s - loss: 52.4392 - val_loss: 75.8751\nEpoch 35/50\n - 1s - loss: 48.6391 - val_loss: 77.0240\nEpoch 36/50\n - 4s - loss: 50.6684 - val_loss: 98.2475\nEpoch 37/50\n - 1s - loss: 48.2000 - val_loss: 82.0820\nEpoch 38/50\n - 3s - loss: 47.4352 - val_loss: 76.3380\nEpoch 39/50\n - 1s - loss: 47.1014 - val_loss: 83.5236\nEpoch 40/50\n - 1s - loss: 48.3920 - val_loss: 92.8988\nEpoch 41/50\n - 1s - loss: 47.4457 - val_loss: 79.7050\nEpoch 42/50\n - 1s - loss: 46.9715 - val_loss: 91.3981\nEpoch 43/50\n - 0s - loss: 47.2755 - val_loss: 76.9331\nEpoch 44/50\n - 0s - loss: 47.7490 - val_loss: 92.2660\nEpoch 45/50\n - 0s - loss: 46.4258 - val_loss: 82.1087\nEpoch 46/50\n - 0s - loss: 47.5194 - val_loss: 89.3497\nEpoch 47/50\n - 0s - loss: 47.6612 - val_loss: 89.4172\nEpoch 48/50\n - 0s - loss: 49.7464 - val_loss: 72.3544\nEpoch 49/50\n - 6s - loss: 53.3954 - val_loss: 117.2881\nEpoch 50/50\n - 0s - loss: 50.6731 - val_loss: 67.5201\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 4s - loss: 49.8221 - val_loss: 76.2358\nEpoch 2/50\n - 0s - loss: 46.6207 - val_loss: 73.7878\nEpoch 3/50\n - 0s - loss: 47.9081 - val_loss: 73.2549\nEpoch 4/50\n - 0s - loss: 50.9382 - val_loss: 96.2822\nEpoch 5/50\n - 0s - loss: 50.7826 - val_loss: 95.5992\nEpoch 6/50\n - 1s - loss: 46.7951 - val_loss: 77.9477\nEpoch 7/50\n - 1s - loss: 47.0699 - val_loss: 92.2457\nEpoch 8/50\n - 0s - loss: 49.8488 - val_loss: 89.4102\nEpoch 9/50\n - 0s - loss: 46.9741 - val_loss: 83.5852\nEpoch 10/50\n - 0s - loss: 46.9378 - val_loss: 81.7880\nEpoch 11/50\n - 0s - loss: 51.5361 - val_loss: 113.8473\nEpoch 12/50\n - 0s - loss: 56.4122 - val_loss: 77.0756\nEpoch 13/50\n - 0s - loss: 47.5926 - val_loss: 87.2711\nEpoch 14/50\n - 0s - loss: 47.3623 - val_loss: 111.0964\nEpoch 15/50\n - 0s - loss: 49.2376 - val_loss: 88.8128\nEpoch 16/50\n - 1s - loss: 49.4424 - val_loss: 75.5667\nEpoch 17/50\n - 0s - loss: 47.7140 - val_loss: 97.8426\nEpoch 18/50\n - 0s - loss: 48.9986 - val_loss: 82.4761\nEpoch 19/50\n - 0s - loss: 47.5735 - val_loss: 75.3640\nEpoch 20/50\n - 0s - loss: 47.7081 - val_loss: 86.1004\nEpoch 21/50\n - 0s - loss: 48.3913 - val_loss: 78.3487\nEpoch 22/50\n - 0s - loss: 48.3450 - val_loss: 84.5282\nEpoch 23/50\n - 0s - loss: 46.7074 - val_loss: 90.0269\nEpoch 24/50\n - 5s - loss: 51.5741 - val_loss: 79.7686\nEpoch 25/50\n - 1s - loss: 47.3552 - val_loss: 101.5646\nEpoch 26/50\n - 5s - loss: 47.9058 - val_loss: 110.2102\nEpoch 27/50\n - 0s - loss: 47.4166 - val_loss: 81.8036\nEpoch 28/50\n - 0s - loss: 46.8808 - val_loss: 92.6980\nEpoch 29/50\n - 0s - loss: 47.9381 - val_loss: 87.4584\nEpoch 30/50\n - 0s - loss: 49.4883 - val_loss: 73.7977\nEpoch 31/50\n - 0s - loss: 49.0300 - val_loss: 102.0505\nEpoch 32/50\n - 0s - loss: 48.0806 - val_loss: 80.7468\nEpoch 33/50\n - 1s - loss: 47.9931 - val_loss: 101.0426\nEpoch 34/50\n - 0s - loss: 47.5257 - val_loss: 80.8788\nEpoch 35/50\n - 0s - loss: 48.8654 - val_loss: 76.6291\nEpoch 36/50\n - 0s - loss: 49.0358 - val_loss: 105.8734\nEpoch 37/50\n - 0s - loss: 48.7314 - val_loss: 107.8911\nEpoch 38/50\n - 0s - loss: 52.4203 - val_loss: 87.8125\nEpoch 39/50\n - 0s - loss: 50.7878 - val_loss: 83.3153\nEpoch 40/50\n - 0s - loss: 46.8817 - val_loss: 86.1051\nEpoch 41/50\n - 0s - loss: 52.1940 - val_loss: 93.3839\nEpoch 42/50\n - 0s - loss: 54.0629 - val_loss: 95.4005\nEpoch 43/50\n - 0s - loss: 48.2469 - val_loss: 80.9142\nEpoch 44/50\n - 1s - loss: 49.4170 - val_loss: 98.2924\nEpoch 45/50\n - 1s - loss: 49.1181 - val_loss: 86.3987\nEpoch 46/50\n - 0s - loss: 48.3810 - val_loss: 79.8461\nEpoch 47/50\n - 0s - loss: 49.6772 - val_loss: 75.7673\nEpoch 48/50\n - 0s - loss: 63.7158 - val_loss: 71.1115\nEpoch 49/50\n - 9s - loss: 49.0555 - val_loss: 73.9809\nEpoch 50/50\n - 0s - loss: 46.8204 - val_loss: 87.4605\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 51.6176 - val_loss: 95.0663\nEpoch 2/50\n - 0s - loss: 47.0523 - val_loss: 89.0286\nEpoch 3/50\n - 0s - loss: 46.7236 - val_loss: 83.6196\nEpoch 4/50\n - 0s - loss: 47.4316 - val_loss: 86.0758\nEpoch 5/50\n - 0s - loss: 49.2327 - val_loss: 74.7347\nEpoch 6/50\n - 0s - loss: 47.3765 - val_loss: 81.0297\nEpoch 7/50\n - 0s - loss: 46.9800 - val_loss: 83.2972\nEpoch 8/50\n - 0s - loss: 47.9553 - val_loss: 87.7994\nEpoch 9/50\n - 1s - loss: 48.0904 - val_loss: 86.3699\nEpoch 10/50\n - 0s - loss: 50.8614 - val_loss: 99.0625\nEpoch 11/50\n - 0s - loss: 49.1484 - val_loss: 95.5506\nEpoch 12/50\n - 0s - loss: 52.8824 - val_loss: 132.0564\nEpoch 13/50\n - 0s - loss: 52.9077 - val_loss: 100.0795\nEpoch 14/50\n - 0s - loss: 48.2829 - val_loss: 107.8001\nEpoch 15/50\n - 0s - loss: 48.3744 - val_loss: 70.3700\nEpoch 16/50\n - 0s - loss: 47.6044 - val_loss: 90.6671\nEpoch 17/50\n - 0s - loss: 46.6091 - val_loss: 81.2328\nEpoch 18/50\n - 0s - loss: 47.9903 - val_loss: 86.4433\nEpoch 19/50\n - 0s - loss: 47.3199 - val_loss: 75.8502\nEpoch 20/50\n - 0s - loss: 50.5050 - val_loss: 119.9686\nEpoch 21/50\n - 1s - loss: 55.2392 - val_loss: 123.2295\nEpoch 22/50\n - 0s - loss: 51.5766 - val_loss: 101.9384\nEpoch 23/50\n - 0s - loss: 47.4371 - val_loss: 78.6487\nEpoch 24/50\n - 9s - loss: 50.2599 - val_loss: 70.1674\nEpoch 25/50\n - 1s - loss: 48.6112 - val_loss: 83.8157\nEpoch 26/50\n - 1s - loss: 46.3309 - val_loss: 91.1682\nEpoch 27/50\n - 0s - loss: 51.3651 - val_loss: 76.9133\nEpoch 28/50\n - 1s - loss: 52.3875 - val_loss: 71.0641\nEpoch 29/50\n - 0s - loss: 52.9675 - val_loss: 71.5782\nEpoch 30/50\n - 1s - loss: 54.0663 - val_loss: 85.7793\nEpoch 31/50\n - 1s - loss: 49.8914 - val_loss: 70.1039\nEpoch 32/50\n - 0s - loss: 54.7242 - val_loss: 79.0463\nEpoch 33/50\n - 1s - loss: 47.4417 - val_loss: 104.4727\nEpoch 34/50\n - 1s - loss: 47.5195 - val_loss: 78.4291\nEpoch 35/50\n - 0s - loss: 52.3542 - val_loss: 80.5224\nEpoch 36/50\n - 0s - loss: 47.0334 - val_loss: 78.4185\nEpoch 37/50\n - 0s - loss: 48.0020 - val_loss: 108.7039\nEpoch 38/50\n - 0s - loss: 47.5552 - val_loss: 91.1709\nEpoch 39/50\n - 0s - loss: 46.5492 - val_loss: 98.3642\nEpoch 40/50\n - 0s - loss: 47.4416 - val_loss: 93.8298\nEpoch 41/50\n - 0s - loss: 47.5823 - val_loss: 74.5874\nEpoch 42/50\n - 1s - loss: 51.3327 - val_loss: 78.2669\nEpoch 43/50\n - 0s - loss: 47.4925 - val_loss: 92.8974\nEpoch 44/50\n - 10s - loss: 46.9051 - val_loss: 85.0035\nEpoch 45/50\n - 1s - loss: 48.0975 - val_loss: 113.1999\nEpoch 46/50\n - 0s - loss: 47.4299 - val_loss: 87.0034\nEpoch 47/50\n - 1s - loss: 47.4400 - val_loss: 93.8333\nEpoch 48/50\n - 0s - loss: 47.6099 - val_loss: 83.9964\nEpoch 49/50\n - 0s - loss: 46.8470 - val_loss: 90.4069\nEpoch 50/50\n - 1s - loss: 49.3271 - val_loss: 85.8079\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 51.8000 - val_loss: 83.3427\nEpoch 2/50\n - 0s - loss: 47.6005 - val_loss: 73.5589\nEpoch 3/50\n - 0s - loss: 47.8393 - val_loss: 73.7870\nEpoch 4/50\n - 0s - loss: 47.7205 - val_loss: 91.3834\nEpoch 5/50\n - 0s - loss: 47.7114 - val_loss: 86.0827\nEpoch 6/50\n - 0s - loss: 49.1005 - val_loss: 80.1218\nEpoch 7/50\n - 0s - loss: 47.8255 - val_loss: 94.9000\nEpoch 8/50\n - 0s - loss: 48.7221 - val_loss: 87.1573\nEpoch 9/50\n - 0s - loss: 49.4231 - val_loss: 91.4447\nEpoch 10/50\n - 0s - loss: 46.2153 - val_loss: 97.6531\nEpoch 11/50\n - 0s - loss: 49.5653 - val_loss: 78.0944\nEpoch 12/50\n - 0s - loss: 49.0541 - val_loss: 97.8184\nEpoch 13/50\n - 5s - loss: 47.3516 - val_loss: 92.6524\nEpoch 14/50\n - 4s - loss: 47.6761 - val_loss: 93.1433\nEpoch 15/50\n - 0s - loss: 47.7179 - val_loss: 92.0683\nEpoch 16/50\n - 1s - loss: 46.6822 - val_loss: 75.0436\nEpoch 17/50\n - 0s - loss: 49.6571 - val_loss: 97.1448\nEpoch 18/50\n - 0s - loss: 47.4819 - val_loss: 77.6624\nEpoch 19/50\n - 0s - loss: 48.0600 - val_loss: 86.8481\nEpoch 20/50\n - 0s - loss: 46.6363 - val_loss: 96.3664\nEpoch 21/50\n - 0s - loss: 47.6156 - val_loss: 125.9807\nEpoch 22/50\n - 0s - loss: 49.3275 - val_loss: 82.7545\nEpoch 23/50\n - 0s - loss: 48.5932 - val_loss: 83.7417\nEpoch 24/50\n - 0s - loss: 49.0855 - val_loss: 69.7195\nEpoch 25/50\n - 1s - loss: 52.5604 - val_loss: 114.5239\nEpoch 26/50\n - 0s - loss: 49.4240 - val_loss: 113.1039\nEpoch 27/50\n - 0s - loss: 52.2182 - val_loss: 88.4460\nEpoch 28/50\n - 0s - loss: 51.6648 - val_loss: 73.9306\nEpoch 29/50\n - 0s - loss: 50.1370 - val_loss: 86.4326\nEpoch 30/50\n - 0s - loss: 49.4384 - val_loss: 112.8432\nEpoch 31/50\n - 0s - loss: 51.3886 - val_loss: 78.9490\nEpoch 32/50\n - 0s - loss: 48.1696 - val_loss: 69.1942\nEpoch 33/50\n - 5s - loss: 47.8997 - val_loss: 99.6926\nEpoch 34/50\n - 0s - loss: 49.2418 - val_loss: 79.9586\nEpoch 35/50\n - 0s - loss: 48.7021 - val_loss: 71.6058\nEpoch 36/50\n - 5s - loss: 52.4428 - val_loss: 69.3312\nEpoch 37/50\n - 0s - loss: 49.8673 - val_loss: 78.3446\nEpoch 38/50\n - 0s - loss: 55.3427 - val_loss: 121.7254\nEpoch 39/50\n - 0s - loss: 50.2181 - val_loss: 91.4012\nEpoch 40/50\n - 0s - loss: 49.8030 - val_loss: 83.5000\nEpoch 41/50\n - 0s - loss: 48.4850 - val_loss: 75.3472\nEpoch 42/50\n - 0s - loss: 46.8338 - val_loss: 124.7339\nEpoch 43/50\n - 0s - loss: 52.4428 - val_loss: 77.9275\nEpoch 44/50\n - 0s - loss: 50.7769 - val_loss: 90.7633\nEpoch 45/50\n - 0s - loss: 48.3489 - val_loss: 93.0354\nEpoch 46/50\n - 0s - loss: 46.4085 - val_loss: 82.6818\nEpoch 47/50\n - 0s - loss: 48.3116 - val_loss: 87.6374\nEpoch 48/50\n - 1s - loss: 50.4729 - val_loss: 89.0384\nEpoch 49/50\n - 0s - loss: 47.1750 - val_loss: 96.0489\nEpoch 50/50\n - 0s - loss: 47.6837 - val_loss: 73.1910\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 47.5713 - val_loss: 82.9782\nEpoch 2/50\n - 0s - loss: 49.6284 - val_loss: 93.1727\nEpoch 3/50\n - 0s - loss: 47.3501 - val_loss: 82.2077\nEpoch 4/50\n - 0s - loss: 47.0400 - val_loss: 79.2548\nEpoch 5/50\n - 0s - loss: 48.6026 - val_loss: 76.1752\nEpoch 6/50\n - 8s - loss: 48.1054 - val_loss: 70.1500\nEpoch 7/50\n - 0s - loss: 52.3260 - val_loss: 101.0628\nEpoch 8/50\n - 0s - loss: 47.6919 - val_loss: 88.8802\nEpoch 9/50\n - 0s - loss: 48.6258 - val_loss: 71.2392\nEpoch 10/50\n - 0s - loss: 49.2546 - val_loss: 114.8799\nEpoch 11/50\n - 0s - loss: 49.4680 - val_loss: 87.6594\nEpoch 12/50\n - 0s - loss: 49.3047 - val_loss: 85.4988\nEpoch 13/50\n - 0s - loss: 46.6135 - val_loss: 78.8517\nEpoch 14/50\n - 0s - loss: 47.3971 - val_loss: 79.7489\nEpoch 15/50\n - 3s - loss: 51.3569 - val_loss: 95.1559\nEpoch 16/50\n - 0s - loss: 48.4083 - val_loss: 88.6561\nEpoch 17/50\n - 0s - loss: 49.1369 - val_loss: 80.7535\nEpoch 18/50\n - 0s - loss: 49.3265 - val_loss: 134.8915\nEpoch 19/50\n - 0s - loss: 54.9499 - val_loss: 89.4154\nEpoch 20/50\n - 5s - loss: 48.7325 - val_loss: 80.5633\nEpoch 21/50\n - 0s - loss: 50.0686 - val_loss: 83.2892\nEpoch 22/50\n - 3s - loss: 48.0679 - val_loss: 92.5555\nEpoch 23/50\n - 0s - loss: 47.8877 - val_loss: 74.1217\nEpoch 24/50\n - 0s - loss: 47.4529 - val_loss: 83.5621\nEpoch 25/50\n - 0s - loss: 50.3092 - val_loss: 121.4824\nEpoch 26/50\n - 0s - loss: 48.4578 - val_loss: 99.1507\nEpoch 27/50\n - 0s - loss: 49.7238 - val_loss: 112.6937\nEpoch 28/50\n - 0s - loss: 53.0132 - val_loss: 75.5844\nEpoch 29/50\n - 0s - loss: 55.1515 - val_loss: 71.9298\nEpoch 30/50\n - 0s - loss: 53.0818 - val_loss: 117.5338\nEpoch 31/50\n - 0s - loss: 49.5900 - val_loss: 95.6099\nEpoch 32/50\n - 0s - loss: 49.8853 - val_loss: 82.8357\nEpoch 33/50\n - 3s - loss: 49.6860 - val_loss: 102.5910\nEpoch 34/50\n - 0s - loss: 49.5320 - val_loss: 96.3238\nEpoch 35/50\n - 5s - loss: 48.7821 - val_loss: 86.8892\nEpoch 36/50\n - 1s - loss: 50.1434 - val_loss: 82.7685\nEpoch 37/50\n - 0s - loss: 48.9477 - val_loss: 93.0258\nEpoch 38/50\n - 3s - loss: 50.6993 - val_loss: 113.7613\nEpoch 39/50\n - 0s - loss: 47.6521 - val_loss: 82.3422\nEpoch 40/50\n - 0s - loss: 48.9349 - val_loss: 107.5043\nEpoch 41/50\n - 0s - loss: 49.0828 - val_loss: 92.6107\nEpoch 42/50\n - 0s - loss: 48.6556 - val_loss: 80.5718\nEpoch 43/50\n - 0s - loss: 48.7706 - val_loss: 88.1657\nEpoch 44/50\n - 0s - loss: 48.9016 - val_loss: 93.7825\nEpoch 45/50\n - 0s - loss: 52.8856 - val_loss: 71.6172\nEpoch 46/50\n - 0s - loss: 47.9720 - val_loss: 74.7004\nEpoch 47/50\n - 0s - loss: 49.3616 - val_loss: 100.5507\nEpoch 48/50\n - 0s - loss: 50.4690 - val_loss: 86.8065\nEpoch 49/50\n - 3s - loss: 46.9715 - val_loss: 71.4766\nEpoch 50/50\n - 0s - loss: 53.5644 - val_loss: 76.7759\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 48.7688 - val_loss: 84.9351\nEpoch 2/50\n - 0s - loss: 47.0667 - val_loss: 85.8037\nEpoch 3/50\n - 0s - loss: 50.1415 - val_loss: 73.6764\nEpoch 4/50\n - 0s - loss: 54.6929 - val_loss: 99.3387\nEpoch 5/50\n - 0s - loss: 47.4241 - val_loss: 124.0790\nEpoch 6/50\n - 0s - loss: 51.9188 - val_loss: 98.1449\nEpoch 7/50\n - 5s - loss: 48.2336 - val_loss: 72.5397\nEpoch 8/50\n - 0s - loss: 47.2676 - val_loss: 88.9493\nEpoch 9/50\n - 0s - loss: 46.8440 - val_loss: 81.1012\nEpoch 10/50\n - 0s - loss: 48.3282 - val_loss: 91.7100\nEpoch 11/50\n - 3s - loss: 48.0971 - val_loss: 66.3499\nEpoch 12/50\n - 0s - loss: 54.0195 - val_loss: 76.9338\nEpoch 13/50\n - 0s - loss: 51.3846 - val_loss: 97.1152\nEpoch 14/50\n - 0s - loss: 49.4957 - val_loss: 80.2250\nEpoch 15/50\n - 3s - loss: 47.0810 - val_loss: 105.8679\nEpoch 16/50\n - 0s - loss: 48.0132 - val_loss: 102.6140\nEpoch 17/50\n - 0s - loss: 47.3351 - val_loss: 78.4724\nEpoch 18/50\n - 0s - loss: 47.5790 - val_loss: 101.9615\nEpoch 19/50\n - 0s - loss: 50.6202 - val_loss: 80.9679\nEpoch 20/50\n - 8s - loss: 47.8047 - val_loss: 94.2992\nEpoch 21/50\n - 0s - loss: 47.4923 - val_loss: 95.8331\nEpoch 22/50\n - 1s - loss: 47.7371 - val_loss: 96.8004\nEpoch 23/50\n - 0s - loss: 46.5071 - val_loss: 95.5185\nEpoch 24/50\n - 0s - loss: 46.8356 - val_loss: 84.8280\nEpoch 25/50\n - 0s - loss: 47.2985 - val_loss: 82.5572\nEpoch 26/50\n - 0s - loss: 49.0162 - val_loss: 73.6171\nEpoch 27/50\n - 0s - loss: 48.3207 - val_loss: 102.6846\nEpoch 28/50\n - 0s - loss: 47.8210 - val_loss: 99.0296\nEpoch 29/50\n - 0s - loss: 48.7060 - val_loss: 66.1107\nEpoch 30/50\n - 0s - loss: 52.1671 - val_loss: 106.1243\nEpoch 31/50\n - 0s - loss: 54.6827 - val_loss: 97.8607\nEpoch 32/50\n - 0s - loss: 49.5047 - val_loss: 86.0517\nEpoch 33/50\n - 1s - loss: 48.1669 - val_loss: 77.5340\nEpoch 34/50\n - 1s - loss: 47.2428 - val_loss: 84.1839\nEpoch 35/50\n - 0s - loss: 47.6578 - val_loss: 70.7980\nEpoch 36/50\n - 0s - loss: 49.1044 - val_loss: 81.4082\nEpoch 37/50\n - 0s - loss: 46.8449 - val_loss: 95.0214\nEpoch 38/50\n - 0s - loss: 47.5973 - val_loss: 91.6537\nEpoch 39/50\n - 0s - loss: 48.2186 - val_loss: 109.0196\nEpoch 40/50\n - 0s - loss: 48.0043 - val_loss: 87.9449\nEpoch 41/50\n - 8s - loss: 53.0481 - val_loss: 121.7145\nEpoch 42/50\n - 0s - loss: 50.7896 - val_loss: 90.8611\nEpoch 43/50\n - 3s - loss: 46.5716 - val_loss: 78.5888\nEpoch 44/50\n - 0s - loss: 50.6700 - val_loss: 88.9379\nEpoch 45/50\n - 0s - loss: 48.8863 - val_loss: 105.5184\nEpoch 46/50\n - 0s - loss: 47.2728 - val_loss: 75.9342\nEpoch 47/50\n - 0s - loss: 47.8687 - val_loss: 89.2363\nEpoch 48/50\n - 0s - loss: 47.9803 - val_loss: 79.8847\nEpoch 49/50\n - 0s - loss: 47.7304 - val_loss: 88.3082\nEpoch 50/50\n - 1s - loss: 49.8910 - val_loss: 78.3912\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 46.9379 - val_loss: 121.0569\nEpoch 2/50\n - 0s - loss: 50.4499 - val_loss: 96.9239\nEpoch 3/50\n - 0s - loss: 50.1526 - val_loss: 81.5821\nEpoch 4/50\n - 0s - loss: 48.9909 - val_loss: 86.9441\nEpoch 5/50\n - 0s - loss: 48.0743 - val_loss: 83.4568\nEpoch 6/50\n - 1s - loss: 48.1840 - val_loss: 75.6672\nEpoch 7/50\n - 0s - loss: 47.2781 - val_loss: 72.9263\nEpoch 8/50\n - 0s - loss: 50.7910 - val_loss: 68.7865\nEpoch 9/50\n - 0s - loss: 49.4960 - val_loss: 95.6052\nEpoch 10/50\n - 0s - loss: 48.9464 - val_loss: 82.3551\nEpoch 11/50\n - 6s - loss: 47.7071 - val_loss: 76.8742\nEpoch 12/50\n - 0s - loss: 51.0588 - val_loss: 91.3517\nEpoch 13/50\n - 0s - loss: 52.1841 - val_loss: 84.6963\nEpoch 14/50\n - 0s - loss: 47.3964 - val_loss: 97.1749\nEpoch 15/50\n - 3s - loss: 48.2136 - val_loss: 82.9161\nEpoch 16/50\n - 0s - loss: 46.6812 - val_loss: 75.7206\nEpoch 17/50\n - 0s - loss: 47.3929 - val_loss: 80.5080\nEpoch 18/50\n - 0s - loss: 50.0006 - val_loss: 87.2273\nEpoch 19/50\n - 0s - loss: 47.1570 - val_loss: 76.4719\nEpoch 20/50\n - 0s - loss: 47.4912 - val_loss: 75.0218\nEpoch 21/50\n - 0s - loss: 52.8169 - val_loss: 74.9535\nEpoch 22/50\n - 1s - loss: 55.7863 - val_loss: 101.5235\nEpoch 23/50\n - 1s - loss: 48.1782 - val_loss: 108.0569\nEpoch 24/50\n - 0s - loss: 49.2227 - val_loss: 83.3120\nEpoch 25/50\n - 0s - loss: 47.2427 - val_loss: 91.0055\nEpoch 26/50\n - 0s - loss: 48.9148 - val_loss: 100.5074\nEpoch 27/50\n - 0s - loss: 47.7200 - val_loss: 77.2594\nEpoch 28/50\n - 0s - loss: 48.5569 - val_loss: 75.9873\nEpoch 29/50\n - 0s - loss: 52.2487 - val_loss: 101.6996\nEpoch 30/50\n - 0s - loss: 48.3257 - val_loss: 85.2452\nEpoch 31/50\n - 0s - loss: 48.1518 - val_loss: 97.8699\nEpoch 32/50\n - 0s - loss: 47.6063 - val_loss: 85.0314\nEpoch 33/50\n - 0s - loss: 47.1353 - val_loss: 76.5523\nEpoch 34/50\n - 0s - loss: 49.5237 - val_loss: 124.2039\nEpoch 35/50\n - 0s - loss: 49.2983 - val_loss: 72.8778\nEpoch 36/50\n - 0s - loss: 50.2611 - val_loss: 75.1125\nEpoch 37/50\n - 0s - loss: 47.1650 - val_loss: 75.3222\nEpoch 38/50\n - 1s - loss: 49.1494 - val_loss: 93.7185\nEpoch 39/50\n - 0s - loss: 51.0046 - val_loss: 120.0182\nEpoch 40/50\n - 7s - loss: 50.2539 - val_loss: 83.6367\nEpoch 41/50\n - 0s - loss: 51.6703 - val_loss: 72.2771\nEpoch 42/50\n - 0s - loss: 53.4724 - val_loss: 85.0495\nEpoch 43/50\n - 1s - loss: 49.6062 - val_loss: 76.0008\nEpoch 44/50\n - 1s - loss: 47.9776 - val_loss: 114.7695\nEpoch 45/50\n - 0s - loss: 51.5111 - val_loss: 69.9870\nEpoch 46/50\n - 0s - loss: 48.4463 - val_loss: 76.0643\nEpoch 47/50\n - 0s - loss: 52.9885 - val_loss: 86.7278\nEpoch 48/50\n - 0s - loss: 52.7472 - val_loss: 111.5283\nEpoch 49/50\n - 0s - loss: 49.3307 - val_loss: 92.7142\nEpoch 50/50\n - 0s - loss: 47.6357 - val_loss: 92.3583\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 47.8980 - val_loss: 86.6647\nEpoch 2/50\n - 0s - loss: 51.3471 - val_loss: 72.9011\nEpoch 3/50\n - 0s - loss: 48.3913 - val_loss: 72.3707\nEpoch 4/50\n - 0s - loss: 48.8922 - val_loss: 74.3146\nEpoch 5/50\n - 0s - loss: 46.9237 - val_loss: 78.0776\nEpoch 6/50\n - 1s - loss: 48.3945 - val_loss: 71.5021\nEpoch 7/50\n - 0s - loss: 51.3634 - val_loss: 102.1797\nEpoch 8/50\n - 0s - loss: 47.4098 - val_loss: 86.2051\nEpoch 9/50\n - 0s - loss: 47.2405 - val_loss: 82.4821\nEpoch 10/50\n - 0s - loss: 51.7757 - val_loss: 117.7542\nEpoch 11/50\n - 0s - loss: 53.6144 - val_loss: 73.3474\nEpoch 12/50\n - 0s - loss: 49.0221 - val_loss: 82.9366\nEpoch 13/50\n - 0s - loss: 47.8202 - val_loss: 88.9027\nEpoch 14/50\n - 5s - loss: 50.2275 - val_loss: 107.7625\nEpoch 15/50\n - 0s - loss: 51.0091 - val_loss: 84.5018\nEpoch 16/50\n - 0s - loss: 47.5519 - val_loss: 91.1045\nEpoch 17/50\n - 3s - loss: 49.4801 - val_loss: 90.7221\nEpoch 18/50\n - 0s - loss: 48.0336 - val_loss: 90.0744\nEpoch 19/50\n - 0s - loss: 48.2311 - val_loss: 97.4172\nEpoch 20/50\n - 0s - loss: 47.8262 - val_loss: 87.4590\nEpoch 21/50\n - 0s - loss: 48.3231 - val_loss: 91.6124\nEpoch 22/50\n - 0s - loss: 49.4489 - val_loss: 72.6900\nEpoch 23/50\n - 0s - loss: 46.6535 - val_loss: 98.3911\nEpoch 24/50\n - 1s - loss: 47.3165 - val_loss: 79.4797\nEpoch 25/50\n - 0s - loss: 49.2488 - val_loss: 84.0272\nEpoch 26/50\n - 0s - loss: 49.5629 - val_loss: 83.1197\nEpoch 27/50\n - 0s - loss: 48.0739 - val_loss: 75.8558\nEpoch 28/50\n - 0s - loss: 46.7641 - val_loss: 84.2572\nEpoch 29/50\n - 0s - loss: 51.5043 - val_loss: 116.3256\nEpoch 30/50\n - 0s - loss: 49.6531 - val_loss: 68.6746\nEpoch 31/50\n - 0s - loss: 51.4496 - val_loss: 100.9238\nEpoch 32/50\n - 0s - loss: 52.2461 - val_loss: 109.0089\nEpoch 33/50\n - 0s - loss: 50.1159 - val_loss: 74.9212\nEpoch 34/50\n - 0s - loss: 49.5194 - val_loss: 71.6634\nEpoch 35/50\n - 0s - loss: 53.9911 - val_loss: 112.5027\nEpoch 36/50\n - 1s - loss: 52.3804 - val_loss: 81.1275\nEpoch 37/50\n - 0s - loss: 48.6230 - val_loss: 85.3161\nEpoch 38/50\n - 0s - loss: 46.8315 - val_loss: 100.4324\nEpoch 39/50\n - 0s - loss: 48.1928 - val_loss: 91.8416\nEpoch 40/50\n - 0s - loss: 50.3021 - val_loss: 75.5535\nEpoch 41/50\n - 0s - loss: 48.3006 - val_loss: 71.9153\nEpoch 42/50\n - 6s - loss: 50.2611 - val_loss: 93.4591\nEpoch 43/50\n - 0s - loss: 46.6593 - val_loss: 89.8169\nEpoch 44/50\n - 0s - loss: 47.6651 - val_loss: 75.8258\nEpoch 45/50\n - 3s - loss: 50.4330 - val_loss: 86.6518\nEpoch 46/50\n - 0s - loss: 52.6328 - val_loss: 95.5367\nEpoch 47/50\n - 0s - loss: 50.0438 - val_loss: 78.3311\nEpoch 48/50\n - 0s - loss: 48.9075 - val_loss: 104.4255\nEpoch 49/50\n - 0s - loss: 49.6056 - val_loss: 92.1580\nEpoch 50/50\n - 0s - loss: 46.9066 - val_loss: 96.2651\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 47.8106 - val_loss: 93.0424\nEpoch 2/50\n - 0s - loss: 47.5877 - val_loss: 90.6800\nEpoch 3/50\n - 0s - loss: 47.1982 - val_loss: 83.7776\nEpoch 4/50\n - 1s - loss: 47.3992 - val_loss: 93.0497\nEpoch 5/50\n - 0s - loss: 51.8753 - val_loss: 98.4640\nEpoch 6/50\n - 0s - loss: 48.8774 - val_loss: 91.4876\nEpoch 7/50\n - 0s - loss: 48.1029 - val_loss: 76.4140\nEpoch 8/50\n - 0s - loss: 52.4769 - val_loss: 128.2833\nEpoch 9/50\n - 0s - loss: 52.1462 - val_loss: 86.9747\nEpoch 10/50\n - 0s - loss: 47.2896 - val_loss: 83.4514\nEpoch 11/50\n - 0s - loss: 46.4585 - val_loss: 99.1564\nEpoch 12/50\n - 0s - loss: 49.2773 - val_loss: 71.1241\nEpoch 13/50\n - 0s - loss: 48.1292 - val_loss: 95.5153\nEpoch 14/50\n - 0s - loss: 50.5015 - val_loss: 81.6217\nEpoch 15/50\n - 0s - loss: 48.9137 - val_loss: 75.4878\nEpoch 16/50\n - 0s - loss: 48.1742 - val_loss: 91.4098\nEpoch 17/50\n - 0s - loss: 49.1290 - val_loss: 95.7127\nEpoch 18/50\n - 10s - loss: 46.8923 - val_loss: 83.6487\nEpoch 19/50\n - 0s - loss: 47.8786 - val_loss: 81.3108\nEpoch 20/50\n - 0s - loss: 50.3739 - val_loss: 118.7525\nEpoch 21/50\n - 0s - loss: 47.8568 - val_loss: 103.3142\nEpoch 22/50\n - 0s - loss: 47.6522 - val_loss: 101.7465\nEpoch 23/50\n - 0s - loss: 48.6018 - val_loss: 110.0590\nEpoch 24/50\n - 0s - loss: 51.1193 - val_loss: 79.2947\nEpoch 25/50\n - 0s - loss: 51.0897 - val_loss: 103.8881\nEpoch 26/50\n - 0s - loss: 48.7789 - val_loss: 84.9941\nEpoch 27/50\n - 0s - loss: 48.2806 - val_loss: 87.7276\nEpoch 28/50\n - 0s - loss: 48.7619 - val_loss: 97.2023\nEpoch 29/50\n - 0s - loss: 47.2463 - val_loss: 97.5104\nEpoch 30/50\n - 1s - loss: 48.2044 - val_loss: 104.3667\nEpoch 31/50\n - 0s - loss: 49.0172 - val_loss: 74.0920\nEpoch 32/50\n - 0s - loss: 54.8128 - val_loss: 139.3617\nEpoch 33/50\n - 0s - loss: 56.7937 - val_loss: 105.1049\nEpoch 34/50\n - 0s - loss: 50.0166 - val_loss: 103.1213\nEpoch 35/50\n - 0s - loss: 50.1160 - val_loss: 73.1063\nEpoch 36/50\n - 0s - loss: 52.6018 - val_loss: 72.9575\nEpoch 37/50\n - 0s - loss: 48.0442 - val_loss: 83.0812\nEpoch 38/50\n - 0s - loss: 46.8993 - val_loss: 104.9583\nEpoch 39/50\n - 0s - loss: 47.3757 - val_loss: 97.5873\nEpoch 40/50\n - 0s - loss: 47.3203 - val_loss: 116.1220\nEpoch 41/50\n - 6s - loss: 48.8895 - val_loss: 107.8777\nEpoch 42/50\n - 1s - loss: 48.6426 - val_loss: 76.9170\nEpoch 43/50\n - 3s - loss: 47.1306 - val_loss: 71.8635\nEpoch 44/50\n - 0s - loss: 55.6480 - val_loss: 81.9295\nEpoch 45/50\n - 0s - loss: 49.3533 - val_loss: 70.7600\nEpoch 46/50\n - 0s - loss: 48.9264 - val_loss: 75.2385\nEpoch 47/50\n - 0s - loss: 46.5016 - val_loss: 95.9197\nEpoch 48/50\n - 0s - loss: 48.1724 - val_loss: 95.6701\nEpoch 49/50\n - 1s - loss: 47.3680 - val_loss: 88.2825\nEpoch 50/50\n - 0s - loss: 46.9659 - val_loss: 88.3776\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 48.5932 - val_loss: 76.9143\nEpoch 2/50\n - 1s - loss: 50.0088 - val_loss: 91.9346\nEpoch 3/50\n - 0s - loss: 47.1872 - val_loss: 101.2990\nEpoch 4/50\n - 0s - loss: 48.0120 - val_loss: 86.1355\nEpoch 5/50\n - 0s - loss: 47.0967 - val_loss: 96.5295\nEpoch 6/50\n - 0s - loss: 48.8902 - val_loss: 84.0609\nEpoch 7/50\n - 0s - loss: 48.3013 - val_loss: 83.3782\nEpoch 8/50\n - 1s - loss: 48.7252 - val_loss: 86.5541\nEpoch 9/50\n - 0s - loss: 49.5084 - val_loss: 81.4545\nEpoch 10/50\n - 0s - loss: 47.4438 - val_loss: 98.2440\nEpoch 11/50\n - 0s - loss: 49.6557 - val_loss: 107.2903\nEpoch 12/50\n - 0s - loss: 50.4277 - val_loss: 83.5742\nEpoch 13/50\n - 0s - loss: 53.2326 - val_loss: 73.1482\nEpoch 14/50\n - 0s - loss: 47.6602 - val_loss: 75.1468\nEpoch 15/50\n - 0s - loss: 48.9652 - val_loss: 97.3988\nEpoch 16/50\n - 6s - loss: 48.9089 - val_loss: 87.8981\nEpoch 17/50\n - 0s - loss: 46.9873 - val_loss: 99.1216\nEpoch 18/50\n - 0s - loss: 48.3420 - val_loss: 94.9425\nEpoch 19/50\n - 3s - loss: 46.4502 - val_loss: 106.4907\nEpoch 20/50\n - 0s - loss: 47.7604 - val_loss: 90.9585\nEpoch 21/50\n - 0s - loss: 47.1184 - val_loss: 91.8124\nEpoch 22/50\n - 0s - loss: 46.8731 - val_loss: 90.5709\nEpoch 23/50\n - 0s - loss: 49.7953 - val_loss: 88.1680\nEpoch 24/50\n - 1s - loss: 48.0026 - val_loss: 75.5527\nEpoch 25/50\n - 0s - loss: 46.9866 - val_loss: 75.9522\nEpoch 26/50\n - 0s - loss: 48.8230 - val_loss: 91.2526\nEpoch 27/50\n - 0s - loss: 49.0777 - val_loss: 98.3730\nEpoch 28/50\n - 0s - loss: 46.7974 - val_loss: 76.9690\nEpoch 29/50\n - 0s - loss: 50.3718 - val_loss: 77.1305\nEpoch 30/50\n - 0s - loss: 55.2064 - val_loss: 96.2826\nEpoch 31/50\n - 0s - loss: 51.9270 - val_loss: 75.3512\nEpoch 32/50\n - 3s - loss: 47.7111 - val_loss: 67.8804\nEpoch 33/50\n - 0s - loss: 50.7211 - val_loss: 90.9696\nEpoch 34/50\n - 0s - loss: 48.8240 - val_loss: 93.7841\nEpoch 35/50\n - 0s - loss: 51.1712 - val_loss: 91.3899\nEpoch 36/50\n - 0s - loss: 47.3511 - val_loss: 88.7370\nEpoch 37/50\n - 5s - loss: 48.8708 - val_loss: 87.2138\nEpoch 38/50\n - 0s - loss: 47.6535 - val_loss: 85.7438\nEpoch 39/50\n - 0s - loss: 49.9505 - val_loss: 104.9122\nEpoch 40/50\n - 3s - loss: 48.4117 - val_loss: 119.9143\nEpoch 41/50\n - 0s - loss: 51.1364 - val_loss: 91.7274\nEpoch 42/50\n - 0s - loss: 52.5061 - val_loss: 68.9360\nEpoch 43/50\n - 0s - loss: 52.1128 - val_loss: 80.3094\nEpoch 44/50\n - 0s - loss: 47.8253 - val_loss: 75.7300\nEpoch 45/50\n - 0s - loss: 47.9041 - val_loss: 74.3077\nEpoch 46/50\n - 0s - loss: 47.2154 - val_loss: 83.0300\nEpoch 47/50\n - 0s - loss: 47.0200 - val_loss: 74.4222\nEpoch 48/50\n - 1s - loss: 49.4201 - val_loss: 95.9583\nEpoch 49/50\n - 0s - loss: 46.9155 - val_loss: 95.8002\nEpoch 50/50\n - 3s - loss: 47.2912 - val_loss: 89.3436\n"
                }
            ],
            "source": "from sklearn.model_selection import train_test_split\nscores = []\n# fit the model\nfor i in range(50):\n    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.30, random_state=24)\n    model.fit(predictors, target, validation_split=0.3, epochs=50, verbose=2)\n    ll = model.evaluate(X_test, y_test, verbose=0)\n    scores.append(ll)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[1328.0946206892193, 268.7856226059997, 117.01818773584458, 111.30066629057949, 114.25883557958511, 103.94896270850715, 104.74052399570502, 101.7625293423057, 110.42682748936527, 118.01732284891567, 112.69932990706855, 113.33850317402565, 100.62952331740493, 100.36020850517988, 102.74327200979091, 112.99284809692777, 91.08897992476676, 69.22199992454553, 62.851260435233996, 64.43072420879476, 60.057208533425936, 65.99974041540646, 62.25322671538418, 62.403544750028445, 55.40468293717764, 60.07210246953378, 57.261622234455594, 61.42236247880559, 52.5736944945499, 67.2373785370762, 55.81201301500635, 53.820993232109785, 52.908091375357124, 57.55864114360131, 53.319314666550525, 55.761288824976454, 63.66425398792649, 62.867150241888844, 53.376219801918204, 68.48452358801389, 54.739839201992, 54.48102864632715, 55.51491798474951, 52.55554388101819, 52.06527530954108, 52.71947634798809, 56.22108917483234, 59.714558894580236, 54.86779664172324, 55.38097082835571]\nMean loss is 102.784586\nWith standard deviation 180.473735\n"
                }
            ],
            "source": "import statistics as st\nprint(scores)\nprint(\"Mean loss is %f\" % st.mean(scores))\nprint(\"With standard deviation %f\" % st.stdev(scores))"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}